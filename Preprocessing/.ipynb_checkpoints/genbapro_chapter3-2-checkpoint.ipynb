{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "classified-farming",
   "metadata": {},
   "source": [
    "# 現場のプロが伝える前処理技術 Chapter3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-argument",
   "metadata": {},
   "source": [
    "# Chapter3-1 自然言語データの処理の基本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-escape",
   "metadata": {},
   "source": [
    "### 自然言語処理の作業手順\n",
    "\n",
    "テキスト読み込み  \n",
    "↓  \n",
    "クレイジング  \n",
    "↓　　　　　← データオーグメンテーション  \n",
    "形態素解析  \n",
    "↓　　　　　← データオーグメンテーション  \n",
    "ベクトル化  \n",
    "↓  \n",
    "学習/推定  \n",
    "↓  \n",
    "可視化/評価  → 改善ループ  \n",
    "↓  \n",
    "レポート作成/報告  → フィードバック"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polish-interpretation",
   "metadata": {},
   "source": [
    "## Chapter3-2 テキスト読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-match",
   "metadata": {},
   "source": [
    "#### 3-2-1 一覧データの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satellite-linux",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.11 s, sys: 676 ms, total: 2.79 s\n",
      "Wall time: 2.84 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>作品id</th>\n",
       "      <th>作品名</th>\n",
       "      <th>作品名読み</th>\n",
       "      <th>ソート用読み</th>\n",
       "      <th>副題</th>\n",
       "      <th>副題読み</th>\n",
       "      <th>原題</th>\n",
       "      <th>初出</th>\n",
       "      <th>分類番号</th>\n",
       "      <th>文字遣い種別</th>\n",
       "      <th>...</th>\n",
       "      <th>テキストファイル最終更新日</th>\n",
       "      <th>テキストファイル符号化方式</th>\n",
       "      <th>テキストファイル文字集合</th>\n",
       "      <th>テキストファイル修正回数</th>\n",
       "      <th>XHTML/HTMLファイルURL</th>\n",
       "      <th>XHTML/HTMLファイル最終更新日</th>\n",
       "      <th>XHTML/HTMLファイル符号化方式</th>\n",
       "      <th>XHTML/HTMLファイル文字集合</th>\n",
       "      <th>XHTML/HTMLファイル修正回数</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000002</td>\n",
       "      <td>三十三の死</td>\n",
       "      <td>さんじゅうさんのし</td>\n",
       "      <td>さんしゆうさんのし</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NDC 913</td>\n",
       "      <td>旧字旧仮名</td>\n",
       "      <td>...</td>\n",
       "      <td>2005-12-28</td>\n",
       "      <td>ShiftJIS</td>\n",
       "      <td>JIS X 0208</td>\n",
       "      <td>2</td>\n",
       "      <td>http://www.aozora.gr.jp/cards/000012/files/2_2...</td>\n",
       "      <td>2005-12-28</td>\n",
       "      <td>ShiftJIS</td>\n",
       "      <td>JIS X 0208</td>\n",
       "      <td>0</td>\n",
       "      <td>2_20959.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000005</td>\n",
       "      <td>あいびき</td>\n",
       "      <td>あいびき</td>\n",
       "      <td>あいひき</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NDC 983</td>\n",
       "      <td>新字新仮名</td>\n",
       "      <td>...</td>\n",
       "      <td>2006-01-06</td>\n",
       "      <td>ShiftJIS</td>\n",
       "      <td>JIS X 0208</td>\n",
       "      <td>3</td>\n",
       "      <td>http://www.aozora.gr.jp/cards/000005/files/5_2...</td>\n",
       "      <td>2006-01-06</td>\n",
       "      <td>ShiftJIS</td>\n",
       "      <td>JIS X 0208</td>\n",
       "      <td>0</td>\n",
       "      <td>5_21310.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     作品id    作品名      作品名読み     ソート用読み   副題 副題読み   原題   初出     分類番号 文字遣い種別  \\\n",
       "0  000002  三十三の死  さんじゅうさんのし  さんしゆうさんのし  NaN  NaN  NaN  NaN  NDC 913  旧字旧仮名   \n",
       "1  000005   あいびき       あいびき       あいひき  NaN  NaN  NaN  NaN  NDC 983  新字新仮名   \n",
       "\n",
       "   ... テキストファイル最終更新日 テキストファイル符号化方式 テキストファイル文字集合 テキストファイル修正回数  \\\n",
       "0  ...    2005-12-28      ShiftJIS   JIS X 0208            2   \n",
       "1  ...    2006-01-06      ShiftJIS   JIS X 0208            3   \n",
       "\n",
       "                                   XHTML/HTMLファイルURL XHTML/HTMLファイル最終更新日  \\\n",
       "0  http://www.aozora.gr.jp/cards/000012/files/2_2...          2005-12-28   \n",
       "1  http://www.aozora.gr.jp/cards/000005/files/5_2...          2006-01-06   \n",
       "\n",
       "  XHTML/HTMLファイル符号化方式 XHTML/HTMLファイル文字集合 XHTML/HTMLファイル修正回数          file  \n",
       "0            ShiftJIS         JIS X 0208                  0  2_20959.html  \n",
       "1            ShiftJIS         JIS X 0208                  0  5_21310.html  \n",
       "\n",
       "[2 rows x 56 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "# 青空文庫の一覧データのURLにアクセスし、データフレームとして保持\n",
    "aozora_list_url = \"http://aozora-word.hahasoha.net/aozora_word_list_utf8.csv.gz\"\n",
    "df = pandas.read_csv(aozora_list_url, header=0, encoding=\"UTF-8\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-prize",
   "metadata": {},
   "source": [
    "#### 3-2-2 一覧データの理解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "incorporated-secret",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 再現性を担保するため、乱数のシードを明示的に指定\n",
    "seed = 1\n",
    "rs = numpy.random.RandomState(seed)\n",
    "\n",
    "# ランダムサンプリング\n",
    "sample_idx = rs.randint(0, len(df))\n",
    "sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "devoted-economy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "作品id                                                              000246\n",
       "作品名                                                                  畜犬談\n",
       "作品名読み                                                             ちくけんだん\n",
       "ソート用読み                                                            ちくけんたん\n",
       "副題                                                           ―伊馬鵜平君に与える―\n",
       "副題読み                                                      ―いまうへいくんにあたえる―\n",
       "原題                                                                   NaN\n",
       "初出                                                    「文学者」1939（昭和14）年8月\n",
       "分類番号                                                             NDC 913\n",
       "文字遣い種別                                                             新字新仮名\n",
       "作品著作権フラグ                                                              なし\n",
       "公開日                                                           1999-04-12\n",
       "最終更新日                                                         2012-10-31\n",
       "図書カードurl               http://www.aozora.gr.jp/cards/000035/card246.html\n",
       "人物id                                                                  35\n",
       "姓                                                                     太宰\n",
       "名                                                                      治\n",
       "姓読み                                                                  だざい\n",
       "名読み                                                                  おさむ\n",
       "姓読みソート用                                                              たさい\n",
       "名読みソート用                                                              おさむ\n",
       "姓ローマ字                                                              Dazai\n",
       "名ローマ字                                                              Osamu\n",
       "役割フラグ                                                                 著者\n",
       "生年月日                                                          1909-06-19\n",
       "没年月日                                                          1948-06-13\n",
       "人物著作権フラグ                                                              なし\n",
       "底本名1                                                       日本文学全集70　太宰治集\n",
       "底本出版社名1                                                              集英社\n",
       "底本初版発行年1                                                   1972（昭和47）年3月\n",
       "入力に使用した版1                                                            NaN\n",
       "校正に使用した版1                                                            NaN\n",
       "底本の親本名1                                                              NaN\n",
       "底本の親本出版社名1                                                           NaN\n",
       "底本の親本初版発行年1                                                          NaN\n",
       "底本名2                                                                 NaN\n",
       "底本出版社名2                                                              NaN\n",
       "底本初版発行年2                                                             NaN\n",
       "入力に使用した版2                                                            NaN\n",
       "校正に使用した版2                                                            NaN\n",
       "底本の親本名2                                                              NaN\n",
       "底本の親本出版社名2                                                           NaN\n",
       "底本の親本初版発行年2                                                          NaN\n",
       "入力者                                                                   網迫\n",
       "校正者                                                                 田尻幹二\n",
       "テキストファイルurl            http://www.aozora.gr.jp/cards/000035/files/246...\n",
       "テキストファイル最終更新日                                                 2009-03-06\n",
       "テキストファイル符号化方式                                                   ShiftJIS\n",
       "テキストファイル文字集合                                                  JIS X 0208\n",
       "テキストファイル修正回数                                                           2\n",
       "XHTML/HTMLファイルURL      http://www.aozora.gr.jp/cards/000035/files/246...\n",
       "XHTML/HTMLファイル最終更新日                                           2009-03-06\n",
       "XHTML/HTMLファイル符号化方式                                             ShiftJIS\n",
       "XHTML/HTMLファイル文字集合                                            JIS X 0208\n",
       "XHTML/HTMLファイル修正回数                                                     0\n",
       "file                                                      246_34649.html\n",
       "Name: 235, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[sample_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "wicked-reflection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "テキストファイルurl            http://www.aozora.gr.jp/cards/000035/files/246...\n",
       "XHTML/HTMLファイルURL      http://www.aozora.gr.jp/cards/000035/files/246...\n",
       "XHTML/HTMLファイル符号化方式                                             ShiftJIS\n",
       "分類番号                                                             NDC 913\n",
       "Name: 235, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 情報量が多すぎるので表示される情報を絞る。\n",
    "# 分類番号は文章分類で使用する。\n",
    "df.iloc[sample_idx][[\"テキストファイルurl\", \"XHTML/HTMLファイルURL\", \"XHTML/HTMLファイル符号化方式\", \"分類番号\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-armenia",
   "metadata": {},
   "source": [
    "#### HTMLのテキスト取得手順\n",
    "- URLにリクエストして、レスポンスを取得。\n",
    "- HTTPコードが200以上の時は、例外処理。\n",
    "- HTMLオブジェクトを取得\n",
    "- HTMLオブジェクトからテキストコンテンツを取得し表示\n",
    "  -  最初の100文字\n",
    "  -  区切り線\n",
    "  -  最後の100文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "diverse-panama",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('http://www.aozora.gr.jp/cards/000035/files/246_ruby_1636.zip',\n",
       " 'http://www.aozora.gr.jp/cards/000035/files/246_34649.html',\n",
       " 'ShiftJIS')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_zip_url = df.iloc[sample_idx][\"テキストファイルurl\"]    # zip\n",
    "sample_html_url = df.iloc[sample_idx][\"XHTML/HTMLファイルURL\"]    # html\n",
    "sample_encoding = df.iloc[sample_idx][\"XHTML/HTMLファイル符号化方式\"]    # encoding\n",
    "sample_zip_url, sample_html_url, sample_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sweet-administration",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lxml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ca23eab864eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lxml'"
     ]
    }
   ],
   "source": [
    "import lxml.html\n",
    "import requests\n",
    "\n",
    "html = None\n",
    "\n",
    "# 指定されたURL にリクエストし、レスポンスを取得\n",
    "response = requests.get(sample_html_url)\n",
    "\n",
    "# HTTPコードが200以外の場合は、例外処理\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f\"HTTP Error. status code: {response.status_code}\")\n",
    "\n",
    "# HTMLオブジェクトを取得\n",
    "html = lxml.html.fromstring(response.content)\n",
    "\n",
    "\n",
    "# HTMLオブジェクトから、テキストコンテンツを取得し表示\n",
    "# - 最初の150文字\n",
    "print(html.text_content()[:150])\n",
    "\n",
    "# - 区切り線を表示\n",
    "print(\"\\n\", \"~ \" * 50, sep=\"\")\n",
    "print(\"~ \" * 50, \"\\n\")\n",
    "\n",
    "# - 最後の150文字\n",
    "print(html.text_content()[-150:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-stake",
   "metadata": {},
   "source": [
    "#### 3-2-4 テキストデータの取得(ZIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "athletic-finance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "畜犬談\n",
      "―伊馬鵜平君に与える―\n",
      "太宰治\n",
      "\n",
      "-------------------------------------------------------\n",
      "【テキスト中に現れる記号について】\n",
      "\n",
      "《》：ルビ\n",
      "（例）喰《く》いつかれる\n",
      "\n",
      "｜：ルビの付く文字列の始まりを特定する記号\n",
      "（例）十匹｜這《は》っている\n",
      "-------------------------------------------------------\n",
      "\n",
      "　私は、犬については自信がある。いつの日か、かならず喰《く》いつかれるであろうという自信である。私は、きっと噛《か》まれるにちがいない。自信があるのである。よくぞ、きょうまで喰いつ\n",
      "\n",
      "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ \n",
      "~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~  \n",
      "\n",
      "かい？」\n",
      "「ええ」家内は、浮かぬ顔をしていた。\n",
      "「ポチにやれ、二つあるなら、二つやれ。おまえも我慢しろ。皮膚病なんてのは、すぐなおるよ」\n",
      "「ええ」家内は、やはり浮かぬ顔をしていた。\n",
      "\n",
      "\n",
      "\n",
      "底本：「日本文学全集70　太宰治集」集英社\n",
      "　　　1972（昭和47）年3月初版\n",
      "初出：「文学者」\n",
      "　　　1939（昭和14）年8月\n",
      "入力：網迫\n",
      "校正：田尻幹二\n",
      "1999年4月12日公開\n",
      "2009年3月6日修正\n",
      "青空文庫作成ファイル：\n",
      "このファイルは、インターネットの図書館、青空文庫（http://www.aozora.gr.jp/）で作られました。入力、校正、制作にあたったのは、ボランティアの皆さんです。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "import zipfile\n",
    "\n",
    "# URLからダウンロード\n",
    "response = requests.get(sample_zip_url)\n",
    "\n",
    "# ダウンロードしたzipデータをファイルとして展開（解答）\n",
    "z = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "z.extractall(\"./data\")\n",
    "\n",
    "# ローカルのテキストファイルパスを変数として保持\n",
    "txt_file = \"./data/\" + z.infolist()[0].filename\n",
    "\n",
    "# エンコーディングを指定して、テキストファイルを読み込み\n",
    "with open(txt_file, \"r\", encoding=sample_encoding) as f:\n",
    "    # ファイル全体を読み込み内容を保持\n",
    "    content = f.read()\n",
    "\n",
    "    # 最初の300文字を表示\n",
    "    print(content[:300])\n",
    "\n",
    "    # 区切り線を表示\n",
    "    print(\"\\n\", \"~ \" * 50, sep=\"\")\n",
    "    print(\"~ \" * 50, \"\\n\")\n",
    "\n",
    "    # 最後の100文字を表示\n",
    "    print(content[-300:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-framing",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3-2-5 エンコーディング\n",
    "- LINUX系OSのデフォルトはUTF-8\n",
    "- windowsはShirt-JIS、またはその拡張版のCPS932\n",
    "\n",
    "google colabはUTF-8をデフォルトとしている。  \n",
    "自然言語でもUTF-8が多い。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "referenced-crawford",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-8-b1c27d6ca8fc>\", line 4, in <module>\n",
      "    lines = f.readlines()\n",
      "  File \"/opt/conda/lib/python3.8/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position 0: invalid start byte\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "try:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "except UnicodeDecodeError as e:\n",
    "    stack_trace = traceback.format_exc(chain=e)\n",
    "    print(stack_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "previous-enemy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UTF-8'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "published-front",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ShiftJIS'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一覧からの取得した文字コードを確認\n",
    "sample_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interested-noise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-11-22eb386eb1ee>\", line 4, in <module>\n",
      "    lines = f.readlines()\n",
      "  File \"/opt/conda/lib/python3.8/codecs.py\", line 322, in decode\n",
      "    (result, consumed) = self._buffer_decode(data, self.errors, final)\n",
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0x92 in position 0: invalid start byte\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UTF-8 を明示的に指定\n",
    "try:\n",
    "    with open(txt_file, \"r\", encoding=\"UTF-8\") as f:\n",
    "        lines = f.readlines()\n",
    "except UnicodeDecodeError as e:\n",
    "    stack_trace = traceback.format_exc(chain=e)\n",
    "    print(stack_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "circular-shanghai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift-JIS を明示的に指定\n",
    "with open(txt_file, \"r\", encoding=\"Shift-JIS\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "close-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CP9328 を明示的に指定\n",
    "with open(txt_file, \"r\", encoding=\"CP932\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-assurance",
   "metadata": {},
   "source": [
    "#### 3-2-6-1 TSV(タブ区切り)形式\n",
    "csvではカンマなどを含む区切り位置が多少面倒な事がある、TSVでは値に入る事がほとんど無い区切り文字を使用することで、  \n",
    "区切り位置が入っているか確認して加工する必要がなくなります。  \n",
    "\n",
    "Excelをコピーしてテキストファイルに入れるとTSV型式になるので比較的よく見る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dominant-consumption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer\t\t\t   preprocess_knock_Python.ipynb\n",
      "data\t\t\t   preprocess_knock_R.ipynb\n",
      "Entity_Relationship.ipynb  preprocess_knock_SQL.ipynb\n",
      "genbapro_chapter3.ipynb\n",
      "-----\n",
      "04-21-1991\t9:09\t58\t100\n",
      "04-21-1991\t9:09\t33\t009\n",
      "04-21-1991\t9:09\t34\t013\n",
      "04-21-1991\t17:08\t62\t119\n",
      "04-21-1991\t17:08\t33\t007\n",
      "-----\n",
      "answer\t       diabetes-data.tar.Z\t  preprocess_knock_Python.ipynb\n",
      "data\t       Entity_Relationship.ipynb  preprocess_knock_R.ipynb\n",
      "Diabetes-Data  genbapro_chapter3.ipynb\t  preprocess_knock_SQL.ipynb\n"
     ]
    }
   ],
   "source": [
    "!rm -rf diabetes-data* Diabetes-Data\n",
    "!ls\n",
    "!echo \"-----\"\n",
    "!curl -sSLO https://archive.ics.uci.edu/ml/machine-learning-databases/diabetes/diabetes-data.tar.Z\n",
    "!tar xzf diabetes-data.tar.Z\n",
    "!head -n 5 Diabetes-Data/data-01\n",
    "!echo \"-----\"\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unable-vietnam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>code</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>04-21-1991</td>\n",
       "      <td>9:09</td>\n",
       "      <td>58</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>04-21-1991</td>\n",
       "      <td>9:09</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>04-21-1991</td>\n",
       "      <td>9:09</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  time  code  value\n",
       "0  04-21-1991  9:09    58    100\n",
       "1  04-21-1991  9:09    33      9\n",
       "2  04-21-1991  9:09    34     13"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ダウンロードしたデータを読み込む\n",
    "df = pandas.read_csv(\"Diabetes-Data/data-01\", sep=\"\\t\", header=None)\n",
    "\n",
    "# データの可読性を上げるため、カラムを設定\n",
    "df.columns = [\"date\", \"time\", \"code\", \"value\"]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "psychological-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用したファイルを削除\n",
    "!rm -rf diabetes-data.*\n",
    "!rm -rf Diabetes-Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-export",
   "metadata": {},
   "source": [
    "#### 3-2-6-2 Excel 形式\n",
    "\n",
    "CSV型式やTSV型式に変換することで読み込めます。  \n",
    "pandasはそのままExcelから読み込めるようになっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "differential-merchandise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in /opt/conda/lib/python3.8/site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xlrd -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "public-enlargement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>年度</th>\n",
       "      <th>西暦</th>\n",
       "      <th>総数</th>\n",
       "      <th>男</th>\n",
       "      <th>女</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H23</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>182828.0</td>\n",
       "      <td>89874.0</td>\n",
       "      <td>92954.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H24</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>183154.0</td>\n",
       "      <td>89846.0</td>\n",
       "      <td>93308.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H25</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>183951.0</td>\n",
       "      <td>89946.0</td>\n",
       "      <td>94005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H26</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>184292.0</td>\n",
       "      <td>89942.0</td>\n",
       "      <td>94350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H27</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>185287.0</td>\n",
       "      <td>90338.0</td>\n",
       "      <td>94949.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H28</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>191032.0</td>\n",
       "      <td>93115.0</td>\n",
       "      <td>97917.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H29</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>193677.0</td>\n",
       "      <td>94205.0</td>\n",
       "      <td>99472.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>資料　選挙管理委員会事務局</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              年度      西暦        総数        男        女\n",
       "0            H23  2011.0  182828.0  89874.0  92954.0\n",
       "1            H24  2012.0  183154.0  89846.0  93308.0\n",
       "2            H25  2013.0  183951.0  89946.0  94005.0\n",
       "3            H26  2014.0  184292.0  89942.0  94350.0\n",
       "4            H27  2015.0  185287.0  90338.0  94949.0\n",
       "5            H28  2016.0  191032.0  93115.0  97917.0\n",
       "6            H29  2017.0  193677.0  94205.0  99472.0\n",
       "7  資料　選挙管理委員会事務局     NaN       NaN      NaN      NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls_url = \"http://www.city.chofu.tokyo.jp/www/contents/1557709709559/simple/86.xls\"\n",
    "pandas.read_excel(xls_url, header=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-circus",
   "metadata": {},
   "source": [
    "#### 3-2-63 JSON形式\n",
    "Webアプリケーションの業務利用など多く使われており、近年増えている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dietary-tension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n",
      "Collecting jq\n",
      "  Downloading jq-1.1.2-cp38-cp38-manylinux2010_x86_64.whl (582 kB)\n",
      "Installing collected packages: jq\n",
      "Successfully installed jq-1.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "apt update -y\n",
    "apt install -y autoconf automake build-essential libtool python-dev\n",
    "apt install -y jq  # コマンドライン\n",
    "pip install jq  # python モジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "wireless-damage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/bash: line 7: jq: command not found\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'# Wikipedia API \\xe3\\x81\\xb8\\xe3\\x82\\xa2\\xe3\\x82\\xaf\\xe3\\x82\\xbb\\xe3\\x82\\xb9\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe3\\x80\\x81\\xe5\\x8f\\x96\\xe5\\xbe\\x97\\xe3\\x81\\x97\\xe3\\x81\\x9f\\xe5\\x86\\x85\\xe5\\xae\\xb9(JSON\\xe5\\xbd\\xa2\\xe5\\xbc\\x8f\\xe3\\x81\\xae\\xe3\\x83\\x87\\xe3\\x83\\xbc\\xe3\\x82\\xbf)\\xe3\\x82\\x92 `wikipedia.json` \\xe3\\x81\\xab\\xe4\\xbf\\x9d\\xe5\\xad\\x98\\ncurl -sS -XPOST \\\\\\n    \"https://ja.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exlimit=1\" \\\\\\n    --data-urlencode \"titles=\\xe8\\x87\\xaa\\xe7\\x84\\xb6\\xe8\\xa8\\x80\\xe8\\xaa\\x9e\" -o wikipedia.json\\n\\n# \\xe4\\xbf\\x9d\\xe5\\xad\\x98\\xe3\\x81\\x97\\xe3\\x81\\x9f\\xe5\\x86\\x85\\xe5\\xae\\xb9(JSON\\xe5\\xbd\\xa2\\xe5\\xbc\\x8f)\\xe3\\x82\\x92\\xe6\\x95\\xb4\\xe5\\xbd\\xa2\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe8\\xa1\\xa8\\xe7\\xa4\\xba\\ncat wikipedia.json | jq .\\n'' returned non-zero exit status 127.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1761361d7f8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'# Wikipedia API へアクセスして、取得した内容(JSON形式のデータ)を `wikipedia.json` に保存\\ncurl -sS -XPOST \\\\\\n    \"https://ja.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exlimit=1\" \\\\\\n    --data-urlencode \"titles=自然言語\" -o wikipedia.json\\n\\n# 保存した内容(JSON形式)を整形して表示\\ncat wikipedia.json | jq .\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2389\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2391\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2392\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'# Wikipedia API \\xe3\\x81\\xb8\\xe3\\x82\\xa2\\xe3\\x82\\xaf\\xe3\\x82\\xbb\\xe3\\x82\\xb9\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe3\\x80\\x81\\xe5\\x8f\\x96\\xe5\\xbe\\x97\\xe3\\x81\\x97\\xe3\\x81\\x9f\\xe5\\x86\\x85\\xe5\\xae\\xb9(JSON\\xe5\\xbd\\xa2\\xe5\\xbc\\x8f\\xe3\\x81\\xae\\xe3\\x83\\x87\\xe3\\x83\\xbc\\xe3\\x82\\xbf)\\xe3\\x82\\x92 `wikipedia.json` \\xe3\\x81\\xab\\xe4\\xbf\\x9d\\xe5\\xad\\x98\\ncurl -sS -XPOST \\\\\\n    \"https://ja.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exlimit=1\" \\\\\\n    --data-urlencode \"titles=\\xe8\\x87\\xaa\\xe7\\x84\\xb6\\xe8\\xa8\\x80\\xe8\\xaa\\x9e\" -o wikipedia.json\\n\\n# \\xe4\\xbf\\x9d\\xe5\\xad\\x98\\xe3\\x81\\x97\\xe3\\x81\\x9f\\xe5\\x86\\x85\\xe5\\xae\\xb9(JSON\\xe5\\xbd\\xa2\\xe5\\xbc\\x8f)\\xe3\\x82\\x92\\xe6\\x95\\xb4\\xe5\\xbd\\xa2\\xe3\\x81\\x97\\xe3\\x81\\xa6\\xe8\\xa1\\xa8\\xe7\\xa4\\xba\\ncat wikipedia.json | jq .\\n'' returned non-zero exit status 127."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Wikipedia API へアクセスして、取得した内容(JSON形式のデータ)を `wikipedia.json` に保存\n",
    "curl -sS -XPOST \\\n",
    "    \"https://ja.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exlimit=1\" \\\n",
    "    --data-urlencode \"titles=自然言語\" -o wikipedia.json\n",
    "\n",
    "# 保存した内容(JSON形式)を整形して表示\n",
    "cat wikipedia.json | jq ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cordless-immune",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batchcomplete': '', 'warnings': {'extracts': {'*': 'HTML may be malformed and/or unbalanced and may omit inline images. Use at your own risk. Known problems are listed at https://www.mediawiki.org/wiki/Special:MyLanguage/Extension:TextExtracts#Caveats.'}}, 'query': {'pages': {'68': {'pageid': 68, 'ns': 0, 'title': '自然言語', 'extract': '<p><b>自然言語</b>（しぜんげんご、英: <span lang=\"en\">natural language</span>）とは、言語学や論理学、計算機科学の専門用語で、「英語」・「中国語」・「日本語」といった「○○語」の総称。つまり普通の「言語」のこと。人間が意思疎通のために日常的に用いる言語であり、文化的背景を持っておのずから発展してきた言語。\\n</p><p>対義語は「人工言語」「形式言語」、すなわちプログラミング言語や論理式など。\\n</p>\\n<h2><span id=\".E6.A6.82.E8.A6.81\"></span><span id=\"概要\">概要</span></h2>\\n<p>人間がお互いにコミュニケーションを行うための自然発生的な言語である。形式言語との対比では、その構文や意味が明確に揺るぎなく定められ利用者に厳格な規則の遵守を強いる（ことが多い）形式言語に対し、話者集団の社会的文脈に沿った曖昧な規則が存在していると考えられるものが自然言語である。自然言語には、規則が曖昧であるがゆえに、話者による規則の解釈の自由度が残されており、話者が直面した状況に応じて規則の解釈を変化させることで、状況を共有する他の話者とのコミュニケーションを継続する事が可能となっている。\\n</p><p>人間のコミュニケーションを目的として設計された形式言語、といったようなものも存在するが（ログランなど）あまり多くない。人工言語という分類は多義的であり、形式言語のことを指している場合もあれば、エスペラントなど「人為発生的な自然言語」といったほうが良い場合もある。\\n</p><p>また、文法,単語の用法に曖昧さを含み、使用する単語,単語の順序を入れ替える等が可能であり、感情で文章を制御しやすいため、多様な情景表現が可能となっている。しかし、文法、単語の用法が曖昧であるため、「言語仕様」のように明確に固定することは難しい。各自然言語自体も他言語との統合が起きる事により変化し続けており、自然言語の文法その他あらゆる面が言語学によって研究が続けられている。また、統計的手法を利用する<span title=\"リンク先の項目はまだ不十分なため、加筆や他言語版からの追加翻訳が望まれます。\">計量言語学</span>や、情報処理の対象として自然言語を扱う自然言語処理は、コンピュータの能力の向上にあわせ、またコンピュータのより便利な利用のために（例えばワードプロセッサや、音声入力による情報探索など）、さかんに研究され実地にも応用されるようになった。\\n</p>\\n<h2><span id=\".E6.B3.A8\"></span><span id=\"注\">注</span></h2>\\n\\n<h2><span id=\".E9.96.A2.E9.80.A3.E9.A0.85.E7.9B.AE\"></span><span id=\"関連項目\">関連項目</span></h2>\\n<ul><li>自然言語処理</li>\\n<li>曖昧</li>\\n<li>文脈</li>\\n<li>修辞技法</li>\\n<li>モンタギュー文法</li>\\n<li>生成文法</li>\\n<li>依存文法</li>\\n<li>構文解析</li>\\n<li>形式文法</li>\\n<li>句構造規則</li>\\n<li>日常言語学派</li>\\n<li>認知言語学</li></ul><!-- \\nNewPP limit report\\nParsed by mw1376\\nCached time: 20210409234210\\nCache expiry: 2592000\\nDynamic content: false\\nComplications: []\\nCPU time usage: 0.199 seconds\\nReal time usage: 0.274 seconds\\nPreprocessor visited node count: 555/1000000\\nPost‐expand include size: 10666/2097152 bytes\\nTemplate argument size: 979/2097152 bytes\\nHighest expansion depth: 22/40\\nExpensive parser function count: 2/500\\nUnstrip recursion depth: 0/20\\nUnstrip post‐expand size: 2/5000000 bytes\\nLua time usage: 0.076/10.000 seconds\\nLua memory usage: 1746246/52428800 bytes\\nNumber of Wikibase entities loaded: 1/400\\n--><!--\\nTransclusion expansion time report (%,ms,calls,template)\\n100.00%  243.081      1 -total\\n 65.89%  160.170      1 Template:出典の明記\\n 52.28%  127.086      1 Template:Ambox\\n 22.38%   54.392      1 Template:Find_sources_mainspace\\n 16.86%   40.994      1 Template:Normdaten\\n 11.66%   28.338      1 Template:DMC\\n  8.57%   20.824      1 Template:仮リンク\\n  8.44%   20.523      1 Template:DMC/core\\n  5.49%   13.340      2 Template:出典の明記/dateHandler\\n  4.82%   11.728      1 Template:Lang-en-short\\n-->'}}}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from jq import jq\n",
    "\n",
    "# JSON形式のファイルを読み込む\n",
    "with open(\"wikipedia.json\", \"r\") as f:\n",
    "    jsn = json.load(f)\n",
    "\n",
    "# JSONオブジェクトを表示\n",
    "print(jsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "visible-samba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"<p><b>\\u81ea\\u7136\\u8a00\\u8a9e</b>\\uff08\\u3057\\u305c\\u3093\\u3052\\u3093\\u3054\\u3001\\u82f1: <span lang=\\\"en\\\">natural language</span>\\uff09\\u3068\\u306f\\u3001\\u8a00\\u8a9e\\u5b66\\u3084\\u8ad6\\u7406\\u5b66\\u3001\\u8a08\\u7b97\\u6a5f\\u79d1\\u5b66\\u306e\\u5c02\\u9580\\u7528\\u8a9e\\u3067\\u3001\\u300c\\u82f1\\u8a9e\\u300d\\u30fb\\u300c\\u4e2d\\u56fd\\u8a9e\\u300d\\u30fb\\u300c\\u65e5\\u672c\\u8a9e\\u300d\\u3068\\u3044\\u3063\\u305f\\u300c\\u25cb\\u25cb\\u8a9e\\u300d\\u306e\\u7dcf\\u79f0\\u3002\\u3064\\u307e\\u308a\\u666e\\u901a\\u306e\\u300c\\u8a00\\u8a9e\\u300d\\u306e\\u3053\\u3068\\u3002\\u4eba\\u9593\\u304c\\u610f\\u601d\\u758e\\u901a\\u306e\\u305f\\u3081\\u306b\\u65e5\\u5e38\\u7684\\u306b\\u7528\\u3044\\u308b\\u8a00\\u8a9e\\u3067\\u3042\\u308a\\u3001\\u6587\\u5316\\u7684\\u80cc\\u666f\\u3092\\u6301\\u3063\\u3066\\u304a\\u306e\\u305a\\u304b\\u3089\\u767a\\u5c55\\u3057\\u3066\\u304d\\u305f\\u8a00\\u8a9e\\u3002\\n</p><p>\\u5bfe\\u7fa9\\u8a9e\\u306f\\u300c\\u4eba\\u5de5\\u8a00\\u8a9e\\u300d\\u300c\\u5f62\\u5f0f\\u8a00\\u8a9e\\u300d\\u3001\\u3059\\u306a\\u308f\\u3061\\u30d7\\u30ed\\u30b0\\u30e9\\u30df\\u30f3\\u30b0\\u8a00\\u8a9e\\u3084\\u8ad6\\u7406\\u5f0f\\u306a\\u3069\\u3002\\n</p>\\n<h2><span id=\\\".E6.A6.82.E8.A6.81\\\"></span><span id=\\\"\\u6982\\u8981\\\">\\u6982\\u8981</span></h2>\\n<p>\\u4eba\\u9593\\u304c\\u304a\\u4e92\\u3044\\u306b\\u30b3\\u30df\\u30e5\\u30cb\\u30b1\\u30fc\\u30b7\\u30e7\\u30f3\\u3092\\u884c\\u3046\\u305f\\u3081\\u306e\\u81ea\\u7136\\u767a\\u751f\\u7684\\u306a\\u8a00\\u8a9e\\u3067\\u3042\\u308b\\u3002\\u5f62\\u5f0f\\u8a00\\u8a9e\\u3068\\u306e\\u5bfe\\u6bd4\\u3067\\u306f\\u3001\\u305d\\u306e\\u69cb\\u6587\\u3084\\u610f\\u5473\\u304c\\u660e\\u78ba\\u306b\\u63fa\\u308b\\u304e\\u306a\\u304f\\u5b9a\\u3081\\u3089\\u308c\\u5229\\u7528\\u8005\\u306b\\u53b3\\u683c\\u306a\\u898f\\u5247\\u306e\\u9075\\u5b88\\u3092\\u5f37\\u3044\\u308b\\uff08\\u3053\\u3068\\u304c\\u591a\\u3044\\uff09\\u5f62\\u5f0f\\u8a00\\u8a9e\\u306b\\u5bfe\\u3057\\u3001\\u8a71\\u8005\\u96c6\\u56e3\\u306e\\u793e\\u4f1a\\u7684\\u6587\\u8108\\u306b\\u6cbf\\u3063\\u305f\\u66d6\\u6627\\u306a\\u898f\\u5247\\u304c\\u5b58\\u5728\\u3057\\u3066\\u3044\\u308b\\u3068\\u8003\\u3048\\u3089\\u308c\\u308b\\u3082\\u306e\\u304c\\u81ea\\u7136\\u8a00\\u8a9e\\u3067\\u3042\\u308b\\u3002\\u81ea\\u7136\\u8a00\\u8a9e\\u306b\\u306f\\u3001\\u898f\\u5247\\u304c\\u66d6\\u6627\\u3067\\u3042\\u308b\\u304c\\u3086\\u3048\\u306b\\u3001\\u8a71\\u8005\\u306b\\u3088\\u308b\\u898f\\u5247\\u306e\\u89e3\\u91c8\\u306e\\u81ea\\u7531\\u5ea6\\u304c\\u6b8b\\u3055\\u308c\\u3066\\u304a\\u308a\\u3001\\u8a71\\u8005\\u304c\\u76f4\\u9762\\u3057\\u305f\\u72b6\\u6cc1\\u306b\\u5fdc\\u3058\\u3066\\u898f\\u5247\\u306e\\u89e3\\u91c8\\u3092\\u5909\\u5316\\u3055\\u305b\\u308b\\u3053\\u3068\\u3067\\u3001\\u72b6\\u6cc1\\u3092\\u5171\\u6709\\u3059\\u308b\\u4ed6\\u306e\\u8a71\\u8005\\u3068\\u306e\\u30b3\\u30df\\u30e5\\u30cb\\u30b1\\u30fc\\u30b7\\u30e7\\u30f3\\u3092\\u7d99\\u7d9a\\u3059\\u308b\\u4e8b\\u304c\\u53ef\\u80fd\\u3068\\u306a\\u3063\\u3066\\u3044\\u308b\\u3002\\n</p><p>\\u4eba\\u9593\\u306e\\u30b3\\u30df\\u30e5\\u30cb\\u30b1\\u30fc\\u30b7\\u30e7\\u30f3\\u3092\\u76ee\\u7684\\u3068\\u3057\\u3066\\u8a2d\\u8a08\\u3055\\u308c\\u305f\\u5f62\\u5f0f\\u8a00\\u8a9e\\u3001\\u3068\\u3044\\u3063\\u305f\\u3088\\u3046\\u306a\\u3082\\u306e\\u3082\\u5b58\\u5728\\u3059\\u308b\\u304c\\uff08\\u30ed\\u30b0\\u30e9\\u30f3\\u306a\\u3069\\uff09\\u3042\\u307e\\u308a\\u591a\\u304f\\u306a\\u3044\\u3002\\u4eba\\u5de5\\u8a00\\u8a9e\\u3068\\u3044\\u3046\\u5206\\u985e\\u306f\\u591a\\u7fa9\\u7684\\u3067\\u3042\\u308a\\u3001\\u5f62\\u5f0f\\u8a00\\u8a9e\\u306e\\u3053\\u3068\\u3092\\u6307\\u3057\\u3066\\u3044\\u308b\\u5834\\u5408\\u3082\\u3042\\u308c\\u3070\\u3001\\u30a8\\u30b9\\u30da\\u30e9\\u30f3\\u30c8\\u306a\\u3069\\u300c\\u4eba\\u70ba\\u767a\\u751f\\u7684\\u306a\\u81ea\\u7136\\u8a00\\u8a9e\\u300d\\u3068\\u3044\\u3063\\u305f\\u307b\\u3046\\u304c\\u826f\\u3044\\u5834\\u5408\\u3082\\u3042\\u308b\\u3002\\n</p><p>\\u307e\\u305f\\u3001\\u6587\\u6cd5,\\u5358\\u8a9e\\u306e\\u7528\\u6cd5\\u306b\\u66d6\\u6627\\u3055\\u3092\\u542b\\u307f\\u3001\\u4f7f\\u7528\\u3059\\u308b\\u5358\\u8a9e,\\u5358\\u8a9e\\u306e\\u9806\\u5e8f\\u3092\\u5165\\u308c\\u66ff\\u3048\\u308b\\u7b49\\u304c\\u53ef\\u80fd\\u3067\\u3042\\u308a\\u3001\\u611f\\u60c5\\u3067\\u6587\\u7ae0\\u3092\\u5236\\u5fa1\\u3057\\u3084\\u3059\\u3044\\u305f\\u3081\\u3001\\u591a\\u69d8\\u306a\\u60c5\\u666f\\u8868\\u73fe\\u304c\\u53ef\\u80fd\\u3068\\u306a\\u3063\\u3066\\u3044\\u308b\\u3002\\u3057\\u304b\\u3057\\u3001\\u6587\\u6cd5\\u3001\\u5358\\u8a9e\\u306e\\u7528\\u6cd5\\u304c\\u66d6\\u6627\\u3067\\u3042\\u308b\\u305f\\u3081\\u3001\\u300c\\u8a00\\u8a9e\\u4ed5\\u69d8\\u300d\\u306e\\u3088\\u3046\\u306b\\u660e\\u78ba\\u306b\\u56fa\\u5b9a\\u3059\\u308b\\u3053\\u3068\\u306f\\u96e3\\u3057\\u3044\\u3002\\u5404\\u81ea\\u7136\\u8a00\\u8a9e\\u81ea\\u4f53\\u3082\\u4ed6\\u8a00\\u8a9e\\u3068\\u306e\\u7d71\\u5408\\u304c\\u8d77\\u304d\\u308b\\u4e8b\\u306b\\u3088\\u308a\\u5909\\u5316\\u3057\\u7d9a\\u3051\\u3066\\u304a\\u308a\\u3001\\u81ea\\u7136\\u8a00\\u8a9e\\u306e\\u6587\\u6cd5\\u305d\\u306e\\u4ed6\\u3042\\u3089\\u3086\\u308b\\u9762\\u304c\\u8a00\\u8a9e\\u5b66\\u306b\\u3088\\u3063\\u3066\\u7814\\u7a76\\u304c\\u7d9a\\u3051\\u3089\\u308c\\u3066\\u3044\\u308b\\u3002\\u307e\\u305f\\u3001\\u7d71\\u8a08\\u7684\\u624b\\u6cd5\\u3092\\u5229\\u7528\\u3059\\u308b<span title=\\\"\\u30ea\\u30f3\\u30af\\u5148\\u306e\\u9805\\u76ee\\u306f\\u307e\\u3060\\u4e0d\\u5341\\u5206\\u306a\\u305f\\u3081\\u3001\\u52a0\\u7b46\\u3084\\u4ed6\\u8a00\\u8a9e\\u7248\\u304b\\u3089\\u306e\\u8ffd\\u52a0\\u7ffb\\u8a33\\u304c\\u671b\\u307e\\u308c\\u307e\\u3059\\u3002\\\">\\u8a08\\u91cf\\u8a00\\u8a9e\\u5b66</span>\\u3084\\u3001\\u60c5\\u5831\\u51e6\\u7406\\u306e\\u5bfe\\u8c61\\u3068\\u3057\\u3066\\u81ea\\u7136\\u8a00\\u8a9e\\u3092\\u6271\\u3046\\u81ea\\u7136\\u8a00\\u8a9e\\u51e6\\u7406\\u306f\\u3001\\u30b3\\u30f3\\u30d4\\u30e5\\u30fc\\u30bf\\u306e\\u80fd\\u529b\\u306e\\u5411\\u4e0a\\u306b\\u3042\\u308f\\u305b\\u3001\\u307e\\u305f\\u30b3\\u30f3\\u30d4\\u30e5\\u30fc\\u30bf\\u306e\\u3088\\u308a\\u4fbf\\u5229\\u306a\\u5229\\u7528\\u306e\\u305f\\u3081\\u306b\\uff08\\u4f8b\\u3048\\u3070\\u30ef\\u30fc\\u30c9\\u30d7\\u30ed\\u30bb\\u30c3\\u30b5\\u3084\\u3001\\u97f3\\u58f0\\u5165\\u529b\\u306b\\u3088\\u308b\\u60c5\\u5831\\u63a2\\u7d22\\u306a\\u3069\\uff09\\u3001\\u3055\\u304b\\u3093\\u306b\\u7814\\u7a76\\u3055\\u308c\\u5b9f\\u5730\\u306b\\u3082\\u5fdc\\u7528\\u3055\\u308c\\u308b\\u3088\\u3046\\u306b\\u306a\\u3063\\u305f\\u3002\\n</p>\\n<h2><span id=\\\".E6.B3.A8\\\"></span><span id=\\\"\\u6ce8\\\">\\u6ce8</span></h2>\\n\\n<h2><span id=\\\".E9.96.A2.E9.80.A3.E9.A0.85.E7.9B.AE\\\"></span><span id=\\\"\\u95a2\\u9023\\u9805\\u76ee\\\">\\u95a2\\u9023\\u9805\\u76ee</span></h2>\\n<ul><li>\\u81ea\\u7136\\u8a00\\u8a9e\\u51e6\\u7406</li>\\n<li>\\u66d6\\u6627</li>\\n<li>\\u6587\\u8108</li>\\n<li>\\u4fee\\u8f9e\\u6280\\u6cd5</li>\\n<li>\\u30e2\\u30f3\\u30bf\\u30ae\\u30e5\\u30fc\\u6587\\u6cd5</li>\\n<li>\\u751f\\u6210\\u6587\\u6cd5</li>\\n<li>\\u4f9d\\u5b58\\u6587\\u6cd5</li>\\n<li>\\u69cb\\u6587\\u89e3\\u6790</li>\\n<li>\\u5f62\\u5f0f\\u6587\\u6cd5</li>\\n<li>\\u53e5\\u69cb\\u9020\\u898f\\u5247</li>\\n<li>\\u65e5\\u5e38\\u8a00\\u8a9e\\u5b66\\u6d3e</li>\\n<li>\\u8a8d\\u77e5\\u8a00\\u8a9e\\u5b66</li></ul><!-- \\nNewPP limit report\\nParsed by mw1376\\nCached time: 20210409234210\\nCache expiry: 2592000\\nDynamic content: false\\nComplications: []\\nCPU time usage: 0.199 seconds\\nReal time usage: 0.274 seconds\\nPreprocessor visited node count: 555/1000000\\nPost\\u2010expand include size: 10666/2097152 bytes\\nTemplate argument size: 979/2097152 bytes\\nHighest expansion depth: 22/40\\nExpensive parser function count: 2/500\\nUnstrip recursion depth: 0/20\\nUnstrip post\\u2010expand size: 2/5000000 bytes\\nLua time usage: 0.076/10.000 seconds\\nLua memory usage: 1746246/52428800 bytes\\nNumber of Wikibase entities loaded: 1/400\\n--><!--\\nTransclusion expansion time report (%,ms,calls,template)\\n100.00%  243.081      1 -total\\n 65.89%  160.170      1 Template:\\u51fa\\u5178\\u306e\\u660e\\u8a18\\n 52.28%  127.086      1 Template:Ambox\\n 22.38%   54.392      1 Template:Find_sources_mainspace\\n 16.86%   40.994      1 Template:Normdaten\\n 11.66%   28.338      1 Template:DMC\\n  8.57%   20.824      1 Template:\\u4eee\\u30ea\\u30f3\\u30af\\n  8.44%   20.523      1 Template:DMC/core\\n  5.49%   13.340      2 Template:\\u51fa\\u5178\\u306e\\u660e\\u8a18/dateHandler\\n  4.82%   11.728      1 Template:Lang-en-short\\n-->\"\n"
     ]
    }
   ],
   "source": [
    "# Wikipedia ページの本文をテキストとして抽出\n",
    "text = jq('.query.pages.\"68\".extract').transform(jsn, text_output=True)\n",
    "\n",
    "# 抽出した本文を表示\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "global-timber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\u81ea\\u7136\\u8a00\\u8a9e\\uff08\\u3057\\u305c\\u3093\\u3052\\u3093\\u3054\\u3001\\u82f1: natural language\\uff09\\u3068\\u306f\\u3001\\u8a00\\u8a9e\\u5b66\\u3084\\u8ad ...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# HTMLタグを削除\n",
    "formatted = re.sub(r\"</?\\w+(\\s+[^>]+)?>\", \"\", text)\n",
    "\n",
    "# `\\n` という文字列を、改行コードに変更\n",
    "formatted = formatted.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "# 整形後のテキストを表示\n",
    "print(formatted[:150], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cooked-ukraine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/bash: line 4: jq: command not found\n",
      "curl: (23) Failure writing output to destination\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -sS \\\n",
    "    \"https://ja.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&exlimit=1&explaintext=true\" \\\n",
    "    --data-urlencode \"titles=自然言語\" \\\n",
    "| jq '.query.pages.\"68\".extract' \\\n",
    "| cut -c-100    # 先頭100バイトだけ表示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "alert-panic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/bash: line 6: jq: command not found\n",
      "curl: (23) Failure writing output to destination\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "curl -sS -XPOST \\\n",
    "    \"https://ja.wikipedia.org/w/api.php?format=json&action=query&prop=extracts\" \\\n",
    "    --data \"exlimit=1\" \\\n",
    "    --data \"explaintext=true\" \\\n",
    "    --data-urlencode \"titles=自然言語\" \\\n",
    "| jq '.query.pages.\"68\".extract' \\\n",
    "| cut -c-100    # 先頭100バイトだけ表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-oregon",
   "metadata": {},
   "source": [
    "# Chapter3-3 データクレンジング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-entrepreneur",
   "metadata": {},
   "source": [
    "形態素解析するために不要なものを取り除く処理。  \n",
    "\n",
    "クレンジング対象  \n",
    "- ヘッダ、フッタ情報 (タイトルや作者などのメタ情報、文末の文章説明)\n",
    "- 文章の始まり、終わり、改行や会話、段落の先頭の全角スペース（\\u3000)\n",
    "- ふりがなやルビ\n",
    "\n",
    "処理後に綺麗な処理ができているか確認し、イメージ通りに出来るまで繰り返し分集合を作る。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-tactics",
   "metadata": {},
   "source": [
    "## 3-3-1 テキスト文書の不要文字の削除\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "assigned-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(txt_file, \"r\", encoding=sample_encoding) as f:\n",
    "    # ファイル全体を読み込み、内容を保持\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "severe-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "畜犬談\n",
      " ―伊馬鵜平君に与える―\n",
      " 太宰治\n",
      " \n",
      " -------------------------------------------------------\n",
      " 【テキスト中に現れる記号について】\n",
      " \n",
      " 《》：ルビ\n",
      " （例）喰《く》いつかれる\n",
      " \n",
      " ｜：ルビの付く文字列の始まりを特定する記号\n",
      " （例）十匹｜這《は》っている\n",
      " -------------------------------------------------------\n",
      " \n",
      " 　私は、犬については自信がある。いつの日か、かならず喰《く》いつかれるであろうという自信である。私は、きっと噛《か》まれるにちがいない。自信があるのである。よくぞ、きょうまで喰いつかれもせず無事に過してきたものだと不思議な気さえしているのである。諸君、犬は猛獣である。馬を斃《たお》し、たまさかには獅子《しし》と戦ってさえこれを征服するとかいうではないか。さもありなんと私はひとり淋しく首肯《しゅこう》しているのだ。あの犬の、鋭い牙《きば》を見るがよい。ただものではない。いまは、あのように街路で無心のふうを装い、とるに足らぬもののごとくみずから卑下して、芥箱《ごみばこ》を覗《のぞ》きまわったりなどしてみせているが、もともと馬を斃すほどの猛獣である。いつなんどき、怒り狂い、その本性を暴露するか、わかったものではない。犬はかならず鎖に固くしばりつけておくべきである。少しの油断もあってはならぬ。世の多くの飼い主は、みずから恐ろしき猛獣を養い、これに日々わずかの残飯《ざんぱん》を与えているという理由だけにて、まったくこの猛獣に心をゆるし、エスやエスやなど、気楽に呼んで、さながら家族の一員のごとく身辺に近づかしめ、三歳のわが愛子をして、その猛獣の耳をぐいと引っぱらせて大笑いしている図にいたっては、戦慄《せんりつ》、眼を蓋《おお》わざるを得ないのである。不意に、わんといって喰いついたら、どうする気だろう。気をつけなければならぬ。飼い主でさえ、噛みつかれぬとは保証できがたい猛獣を、（飼い主だから、絶対に喰いつかれぬということは愚かな気のいい迷信にすぎない。あの恐ろしい牙のある以上、かならず噛む。けっして噛まないということは、科学的に証明できるはずはないのである）その猛獣を、放し飼いにして、往来をうろうろ徘徊《はいかい》させておくとは、どんなものであろうか。昨年の晩秋、私の友人が、ついにこれの被害を受けた。いたましい犠牲者である。友人の話によると、友人は何もせず横丁を懐手《ふところで》してぶらぶら歩いていると、犬が道路上にちゃんと坐っていた。友人は、やはり何もせず、その犬の傍を通った。犬はその時、いやな横目を使ったという。何事もなく通りすぎた、とたん、わんといって右の脚《あし》に喰いついたという。災難である。一瞬のことである。友人は、呆然自失《ぼうぜんじしつ》したという。ややあって、くやし涙が沸いて出た。さもありなん、と私は、やはり淋しく首肯している。そうなってしまったら、ほんとうに、どうしようも、ないではないか。友人は、痛む脚をひきずって病院へ行き手当を受けた。それから二十一日間、病院へ通ったのである。三週間である。脚の傷がなおっても、体内に恐水病といういまわしい病気の毒が、あるいは注入されてあるかもしれぬという懸念《けねん》から、その防毒の注射をしてもらわなければならぬのである。飼い主に談判するなど、その友人の弱気をもってしては、とてもできぬことである。じっと堪《こら》えて、おのれの不運に溜息《ためいき》ついているだけなのである。しかも、注射代などけっして安いものではなく、そのような余分の貯《たくわ》えは失礼ながら友人にあるはずもなく、いずれは苦しい算段をしたにちがいないので、とにかくこれは、ひどい災難である。大災難である。また、うっかり注射でも怠《おこた》ろうものなら、恐水病といって、発熱悩乱の苦しみあって、果ては貌《かお》が犬に似てきて、四つ這《ば》いになり、ただわんわんと吠ゆるばかりだという、そんな凄惨《せいさん》な病気になるかもしれないということなのである。注射を受けながらの、友人の憂慮、不安は、どんなだったろう。友人は苦労人で、ちゃんとできた人であるから、醜くとり乱すこともなく、三七、二十一日病院に通い、注射を受けて、いまは元気に立ち働いているが、もしこれが私だったら、その犬、生かしておかないだろう。私は、人の三倍も四倍も復讐心《ふくしゅうしん》の強い男なのであるから、また、そうなると人の五倍も六倍も残忍性を発揮してしまう男なのであるから、たちどころにその犬の頭蓋骨《ずがいこつ》を、めちゃめちゃに粉砕《ふんさい》し、眼玉をくり抜き、ぐしゃぐしゃに噛んで、べっと吐き捨て、それでも足りずに近所近辺の飼い犬ことごとく毒殺してしまうであろう。こちらが何もせぬのに、突然わんといって噛みつくとはなんという無礼、狂暴の仕草《しぐさ》であろう。いかに畜生といえども許しがたい。畜生ふびんのゆえをもって、人はこれを甘やかしているからいけないのだ。容赦《ようしゃ》なく酷刑《こっけい》に処すべきである。昨秋、友人の遭難を聞いて、私の畜犬に対する日ごろの憎悪は、その極点に達した。青い焔《ほのお》が燃え上るほどの、思いつめたる憎悪である。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(*lines[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "interesting-console",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「ポチにやれ、二つあるなら、二つやれ。おまえも我慢しろ。皮膚病なんてのは、すぐなおるよ」\n",
      " 「ええ」家内は、やはり浮かぬ顔をしていた。\n",
      " \n",
      " \n",
      " \n",
      " 底本：「日本文学全集70　太宰治集」集英社\n",
      " 　　　1972（昭和47）年3月初版\n",
      " 初出：「文学者」\n",
      " 　　　1939（昭和14）年8月\n",
      " 入力：網迫\n",
      " 校正：田尻幹二\n",
      " 1999年4月12日公開\n",
      " 2009年3月6日修正\n",
      " 青空文庫作成ファイル：\n",
      " このファイルは、インターネットの図書館、青空文庫（http://www.aozora.gr.jp/）で作られました。入力、校正、制作にあたったのは、ボランティアの皆さんです。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(*lines[-15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "opposed-break",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 70\n"
     ]
    }
   ],
   "source": [
    "# 文章の開始行、末尾行インデックス番号の取得\n",
    "import re\n",
    "idx_start, idx_end = -1, -1\n",
    "for idx, line in enumerate(lines):\n",
    "    # 本文の開始行のインデックス（番号）を特定する\n",
    "    if re.search(r\"^---\", line):\n",
    "        idx_start = idx + 1\n",
    "\n",
    "    # 本文の末尾行のインデックス（番号）を特定する\n",
    "    if re.search(r\"^底本：\", line):\n",
    "        idx_end = idx - 1\n",
    "\n",
    "print(idx_start, idx_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "challenging-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 開始行と末尾行間のインデックス毎にfor文内の処理を行う。\n",
    "prepped_lines = []\n",
    "for line in lines[idx_start:idx_end]:\n",
    "    # 行の前後の空白コード(半角スペース、改行コードなど))を削除\n",
    "    line = line.strip()\n",
    "\n",
    "    # ルビを削除\n",
    "    line = re.sub(r\"｜\", \"\", line)\n",
    "    line = re.sub(r\"《.*?》\", \"\", line)\n",
    "\n",
    "    # 入力者の注釈を削除\n",
    "    line = re.sub(r\"※?［＃.*?］\", \"\", line)\n",
    "\n",
    "    # Unicode の全角スペースを削除\n",
    "    line = re.sub(r\"\\u3000+\", \" \", line)\n",
    "\n",
    "    # 変換後の行が空行の場合は、スキップ\n",
    "    if line == \"\":\n",
    "        continue\n",
    "    prepped_lines.append(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-principle",
   "metadata": {},
   "source": [
    "まとめ\n",
    "- 削除の指定には正規表現を使用する。\n",
    "- 最短一致(.*?)にしないと、最初から最後の間まで全部削除される可能性があるため。\n",
    "- 全角スペースは半角スペースに変換し、何か文字列があれば処理済みの行の配列に保持している。\n",
    "\n",
    "慣れないうちは、変換結果を見ながらPDCA細かく回していく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "shared-fight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_line_idx: 47\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['「ポチ、食え」私はポチを見たくなかった。ぼんやりそこに立ったまま、「ポチ、食え」足もとで、ぺちゃぺちゃ食べている音がする。一分たたぬうちに死ぬはずだ。',\n",
       " '私は猫背になって、のろのろ歩いた。霧が深い。ほんのちかくの山が、ぼんやり黒く見えるだけだ。南アルプス連峰も、富士山も、何も見えない。朝露で、下駄がびしょぬれである。私はいっそうひどい猫背になって、のろのろ帰途についた。橋を渡り、中学校のまえまで来て、振り向くとポチが、ちゃんといた。面目なげに、首を垂れ、私の視線をそっとそらした。',\n",
       " '私も、もう大人である。いたずらな感傷はなかった。すぐ事態を察知した。薬品が効かなかったのだ。うなずいて、もうすでに私は、白紙還元である。家へ帰って、',\n",
       " '「だめだよ。薬が効かないのだ。ゆるしてやろうよ。あいつには、罪がなかったんだぜ。芸術家は、もともと弱い者の味方だったはずなんだ」私は、途中で考えてきたことをそのまま言ってみた。「弱者の友なんだ。芸術家にとって、これが出発で、また最高の目的なんだ。こんな単純なこと、僕は忘れていた。僕だけじゃない。みんなが、忘れているんだ。僕は、ポチを東京へ連れてゆこうと思うよ。友がもしポチの恰好を笑ったら、ぶん殴ってやる。卵あるかい？」',\n",
       " '「ええ」家内は、浮かぬ顔をしていた。',\n",
       " '「ポチにやれ、二つあるなら、二つやれ。おまえも我慢しろ。皮膚病なんてのは、すぐなおるよ」',\n",
       " '「ええ」家内は、やはり浮かぬ顔をしていた。']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最初の10行を表示\n",
    "# prepped_lines[:10]\n",
    "# 最後の10行を表示\n",
    "# prepped_lines[-10:]\n",
    "\n",
    "# 再現性を担保するため、乱数のシードを明示的に指定\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# ランダムに選んだ行から10行を表示\n",
    "sample_line_idx = numpy.random.randint(0, len(prepped_lines))\n",
    "print(\"sample_line_idx:\", sample_line_idx)\n",
    "prepped_lines[sample_line_idx:sample_line_idx+10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-violin",
   "metadata": {},
   "source": [
    "## 3-3-2 HTML文書から本文のみ取得／抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "tight-singapore",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'html' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-87ed08d20ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# XPathを利用して、本文のみを抽出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//div[@class='main_text']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 最初の100文字を表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'html' is not defined"
     ]
    }
   ],
   "source": [
    "# XPathを利用して、本文のみを抽出\n",
    "main_text = html.xpath(\"//div[@class='main_text']\")\n",
    "content = main_text[0].text_content()\n",
    "\n",
    "# 最初の100文字を表示\n",
    "print(content[:100])\n",
    "\n",
    "# 区切り線を表示\n",
    "print(\"\\n\", \"~ \" * 50, sep=\"\")\n",
    "print(\"~ \" * 50, \"\\n\")\n",
    "\n",
    "# 最後の100文字を表示\n",
    "print(content[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-layer",
   "metadata": {},
   "source": [
    "### 3-3-2-1 行分割"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-active",
   "metadata": {},
   "source": [
    "文字列型を行単位に分割したリスト型へ変換する処理。  \n",
    "一般的に行末端記号は、CRLF(\\r\\n)である文章と、LF(\\n)である文章がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "loving-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行末記号をLF(\\n)に統一 ... CRLF(\\r\\n) を LF(\\n)に変換\n",
    "content = content.replace(\"\\r\\n\", \"\\n\")\n",
    "\n",
    "# 行単位に分割\n",
    "lines = content.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "current-accessory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\u3000\\u3000\\u30001972（昭和47）年3月初版',\n",
       " '初出：「文学者」',\n",
       " '\\u3000\\u3000\\u30001939（昭和14）年8月',\n",
       " '入力：網迫',\n",
       " '校正：田尻幹二',\n",
       " '1999年4月12日公開',\n",
       " '2009年3月6日修正',\n",
       " '青空文庫作成ファイル：',\n",
       " 'このファイルは、インターネットの図書館、青空文庫（http://www.aozora.gr.jp/）で作られました。入力、校正、制作にあたったのは、ボランティアの皆さんです。',\n",
       " '']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 最初の10行を確認\n",
    "# lines[:10]\n",
    "# 最後の10行を確認\n",
    "lines[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-knock",
   "metadata": {},
   "source": [
    "### 3-3-2-2 不要文字列の削除・変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "female-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "prepped_lines = []\n",
    "for line in lines:\n",
    "    # 行の先頭、末尾の空白・改行コードを削除\n",
    "    line = line.strip()\n",
    "\n",
    "    # Unicode の全角スペースを削除\n",
    "    line = re.sub(r\"\\u3000\", \" \", line)\n",
    "\n",
    "    # ふりがなを削除\n",
    "    line = re.sub(r\"（.*?）\", \"\", line)\n",
    "\n",
    "    # 上記変換により、空行になったらスキップ\n",
    "    if line == \"\":\n",
    "        continue\n",
    "\n",
    "    # 変換後の line を処理済行リストとして保持\n",
    "    prepped_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "facial-found",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['底本：「日本文学全集70 太宰治集」集英社',\n",
       " '1972年3月初版',\n",
       " '初出：「文学者」',\n",
       " '1939年8月',\n",
       " '入力：網迫',\n",
       " '校正：田尻幹二',\n",
       " '1999年4月12日公開',\n",
       " '2009年3月6日修正',\n",
       " '青空文庫作成ファイル：',\n",
       " 'このファイルは、インターネットの図書館、青空文庫で作られました。入力、校正、制作にあたったのは、ボランティアの皆さんです。']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 作成した処理済行リストの最初の10行を確認\n",
    "# prepped_lines[:10]\n",
    "# 作成した処理済行リストの最後の10行を確認\n",
    "prepped_lines[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-nigeria",
   "metadata": {},
   "source": [
    "# Chapter3-4 形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-rainbow",
   "metadata": {},
   "source": [
    "## 3-4-1 MeCab\n",
    "\n",
    "\n",
    "昨今の一番メジャーな形態素解析ツールです。  \n",
    "C++で実装されているため、高速に動作し、言語、辞書、コーパスに依存しない凡庸的な設計が大きな特徴です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cooked-contribution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!apt upgrade -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "senior-springer",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to lock directory /var/lib/apt/lists/\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# MeCab をインストール\n",
    "!apt update\n",
    "!apt install -y libmecab-dev mecab mecab-ipadic-utf8 mecab-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "clean-state",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat 10 Apr 2021 07:10:01 AM UTC\n",
      "answer\n",
      "data\n",
      "Entity_Relationship.ipynb\n",
      "genbapro_chapter3.ipynb\n",
      "preprocess_knock_Python.ipynb\n",
      "preprocess_knock_R.ipynb\n",
      "preprocess_knock_SQL.ipynb\n",
      "wikipedia.json\n",
      "Requirement already satisfied: unidic-lite in /opt/conda/lib/python3.8/site-packages (1.0.8)\n",
      "Sat 10 Apr 2021 07:10:16 AM UTC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'mecab-ipadic-neologd'...\n",
      "error: RPC failed; curl 56 GnuTLS recv error (-54): Error in the pull function.\n",
      "fatal: the remote end hung up unexpectedly\n",
      "fatal: early EOF\n",
      "fatal: index-pack failed\n",
      "/usr/bin/bash: line 14: cd: mecab-ipadic-neologd: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# NEologd 辞書をインストール\n",
    "\n",
    "## 開始時刻を表示\n",
    "date\n",
    "\n",
    "## 古い辞書があれば削除\n",
    "rm -rf mecab-ipadic-neologd\n",
    "\n",
    "## 辞書のgit リポジトリをclone\n",
    "git clone https://github.com/neologd/mecab-ipadic-neologd.git\n",
    "ls\n",
    "\n",
    "## 辞書をインストール\n",
    "cd mecab-ipadic-neologd && bin/install-mecab-ipadic-neologd -n -a -y 2M\n",
    "\n",
    "## [issue](https://github.com/SamuraiT/mecab-python3#common-issues) への対応\n",
    "pip install unidic-lite\n",
    "\n",
    "## 修了時刻を表示\n",
    "date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "facial-receptor",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MeCab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MeCab'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import MeCab\n",
    "\n",
    "# NElogd 辞書をシステム辞書として設定し、Tagger オブジェクトを生成\n",
    "dirdic = \"/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\"\n",
    "tokenizer = MeCab.Tagger(f\"-O chasen -d {dirdic}\")\n",
    "\n",
    "# 再現性を担保するため、乱数のシードを明示的に指定\n",
    "numpy.random.seed(12345)\n",
    "\n",
    "# 処理済行リストからランダムに1行を選択し保持\n",
    "idx = numpy.random.randint(0, len(prepped_lines))\n",
    "line = prepped_lines[idx]\n",
    "\n",
    "# 1行をパース\n",
    "node = tokenizer.parseToNode(line)\n",
    "\n",
    "# パース後のノード（トークン＝単語）単位でループ処理\n",
    "parsed_mecab = []\n",
    "while node:\n",
    "    # 文書表現上の単語(表層形)を保持\n",
    "    word = node.surface\n",
    "\n",
    "    # 解析結果の特徴情報を保持\n",
    "    feature = node.feature.split(\",\")\n",
    "\n",
    "    # 原形を保持\n",
    "    base = feature[-3]\n",
    "\n",
    "\n",
    "    # 品詞を保持\n",
    "    pos = feature[0]\n",
    "\n",
    "    # 単語を保持\n",
    "    parsed_mecab.append(word)\n",
    "\n",
    "    # 単語(表層形)、原形と品詞を表示\n",
    "    print(f\"{word}\\t{base}\\t{pos}\")\n",
    "\n",
    "    # ノードを次に更新\n",
    "    node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "different-juvenile",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 1行をパースしてパース済テキストを取得\n",
    "parsed = tokenizer.parse(line)\n",
    "\n",
    "# パース済テキストを改行コード(各単語の情報単位))で分割\n",
    "splitted = [l.split(\"\\t\") for l in parsed.split(\"\\n\")]\n",
    "\n",
    "# 分割した単語情報単位でループ処理\n",
    "for s in splitted:\n",
    "    # 文書表現上の単語を保持\n",
    "    word = s[0]\n",
    "\n",
    "    base = \"\"\n",
    "    # 単語の原形を保持\n",
    "    if len(s) > 2:\n",
    "        base = s[2]\n",
    "\n",
    "    pos = \"\"\n",
    "    if len(s) > 3:\n",
    "        # 品詞を保持\n",
    "        pos = s[3].split(\"-\")[0]\n",
    "\n",
    "    # 単語と品詞を表示\n",
    "    print(f\"{word}\\t{base}\\t{pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-wednesday",
   "metadata": {},
   "source": [
    "## 3-4-2 Janome\n",
    "Mecabに次いで有名なツールで、pipだけで利用できるため使い易い。  \n",
    "Pure Pythonで実装されており辞書を内包しているライブラリです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "stone-yemen",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'janome'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'janome'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import janome.tokenizer\n",
    "import janome.analyzer\n",
    "import janome.charfilter\n",
    "import janome.tokenfilter\n",
    "\n",
    "\n",
    "# Tokenizer オブジェクトを生成\n",
    "tokenizer = janome.tokenizer.Tokenizer()\n",
    "\n",
    "# 品詞フィルタを生成 (除外する品詞リストを指定)\n",
    "stop_poses = [\"BOS/EOS\", \"助詞\", \"助動詞\", \"接続詞\", \"記号\", \"補助記号\", \"未知語\"]\n",
    "token_filters = [janome.tokenfilter.POSStopFilter(stop_poses)]\n",
    "\n",
    "# Unicode で正規化するフィルタを生成\n",
    "char_filters = [janome.charfilter.UnicodeNormalizeCharFilter()]\n",
    "\n",
    "# Analyzer オブジェクトを生成\n",
    "aly = janome.analyzer.Analyzer(char_filters, tokenizer, token_filters)\n",
    "\n",
    "# トークン（区切った単語）毎にループ処理\n",
    "parsed_janome = []\n",
    "for token in aly.analyze(line):\n",
    "    # 単語を保持\n",
    "    parsed_janome.append(token.surface)\n",
    "\n",
    "    # 単語(表層形)、原形と品詞を表示\n",
    "    print(token.surface, token.base_form, token.part_of_speech.split(\",\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-fifth",
   "metadata": {},
   "source": [
    "## 3-4-3 SudachiPy\n",
    "単語の揺らぎを正規化する機能を持ち、Small,Core,Fullの３種類の辞書を持っているpythonライブラリです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "signal-level",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./SudachiDict_full-20190718.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement SudachaiPy\n",
      "ERROR: No matching distribution found for SudachaiPy\n",
      "--2021-04-10 07:13:34--  https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_full-20190718.tar.gz\n",
      "Resolving object-storage.tyo2.conoha.io (object-storage.tyo2.conoha.io)... 157.7.224.17\n",
      "Connecting to object-storage.tyo2.conoha.io (object-storage.tyo2.conoha.io)|157.7.224.17|:443... connected.\n",
      "HTTP request sent, awaiting response... 401 Unauthorized\n",
      "\n",
      "Username/Password Authentication Failed.\n",
      "WARNING: Requirement 'SudachiDict_full-20190718.tar.gz' looks like a filename, but the file does not exist\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/jovyan/work/SudachiDict_full-20190718.tar.gz'\n",
      "\n",
      "/usr/bin/bash: line 10: sudachipy: command not found\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'\\n# SudachiPy \\xe3\\x82\\x92\\xe3\\x82\\xa4\\xe3\\x83\\xb3\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x83\\xbc\\xe3\\x83\\xab\\npip install SudachaiPy\\n\\n# \\xe8\\xbe\\x9e\\xe6\\x9b\\xb8\\xe3\\x82\\x92\\xe3\\x82\\xa4\\xe3\\x83\\xb3\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x83\\xbc\\xe3\\x83\\xab\\nwget https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_full-20190718.tar.gz\\npip install SudachiDict_full-20190718.tar.gz\\n\\n# SudachiPy \\xe3\\x81\\xabFull\\xe3\\x81\\xae\\xe8\\xbe\\x9e\\xe6\\x9b\\xb8\\xe3\\x82\\x92\\xe3\\x83\\xaa\\xe3\\x83\\xb3\\xe3\\x82\\xaf\\nsudachipy link -t full\\n'' returned non-zero exit status 127.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-2bee4258534c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n# SudachiPy をインストール\\npip install SudachaiPy\\n\\n# 辞書をインストール\\nwget https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_full-20190718.tar.gz\\npip install SudachiDict_full-20190718.tar.gz\\n\\n# SudachiPy にFullの辞書をリンク\\nsudachipy link -t full\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2389\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2391\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2392\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-103>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\n# SudachiPy \\xe3\\x82\\x92\\xe3\\x82\\xa4\\xe3\\x83\\xb3\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x83\\xbc\\xe3\\x83\\xab\\npip install SudachaiPy\\n\\n# \\xe8\\xbe\\x9e\\xe6\\x9b\\xb8\\xe3\\x82\\x92\\xe3\\x82\\xa4\\xe3\\x83\\xb3\\xe3\\x82\\xb9\\xe3\\x83\\x88\\xe3\\x83\\xbc\\xe3\\x83\\xab\\nwget https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_full-20190718.tar.gz\\npip install SudachiDict_full-20190718.tar.gz\\n\\n# SudachiPy \\xe3\\x81\\xabFull\\xe3\\x81\\xae\\xe8\\xbe\\x9e\\xe6\\x9b\\xb8\\xe3\\x82\\x92\\xe3\\x83\\xaa\\xe3\\x83\\xb3\\xe3\\x82\\xaf\\nsudachipy link -t full\\n'' returned non-zero exit status 127."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# SudachiPy をインストール\n",
    "pip install SudachaiPy\n",
    "\n",
    "# 辞書をインストール\n",
    "wget https://object-storage.tyo2.conoha.io/v1/nc_2520839e1f9641b08211a5c85243124a/sudachi/SudachiDict_full-20190718.tar.gz\n",
    "pip install SudachiDict_full-20190718.tar.gz\n",
    "\n",
    "# SudachiPy にFullの辞書をリンク\n",
    "sudachipy link -t full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-border",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディスクを有効活用するため、不要データを削除\n",
    "!rm -rf SudachiDict_full-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import sudachipy.dictionary\n",
    "import sudachipy.tokenizer\n",
    "\n",
    "\n",
    "# SudachiPyの辞書オブジェクトを生成\n",
    "sdct = sudachipy.dictionary.Dictionary().create()\n",
    "\n",
    "# 単語(NEologd 単位)の分割モードを生成\n",
    "# # mode = sudachipy.tokenizer.Tokenizer.SplitMode.A        # 短単位 (like UniDic)\n",
    "# # mode = sudachipy.tokenizer.Tokenizer.SplitMode.B        # 中単位\n",
    "mode = sudachipy.tokenizer.Tokenizer.SplitMode.C        # NE単位 (like neolog-d)\n",
    "\n",
    "# 分割されたトークン(単語)単位でループ処理\n",
    "parsed_sudachi = []\n",
    "for tkn in sdct.tokenize(line, mode):\n",
    "    # 単語(表層形)を保持\n",
    "    word = tkn.surface()\n",
    "\n",
    "    # 原形を保持\n",
    "    base = tkn.dictionary_form()\n",
    "\n",
    "    # 品詞を保持\n",
    "    pos = tkn.part_of_speech()[0]\n",
    "\n",
    "    # 正規化形を保持\n",
    "    nrm = tkn.normalized_form()\n",
    "\n",
    "    # 単語を保持\n",
    "    parsed_sudachi.append(word)\n",
    "\n",
    "    # 単語(表層形)、原形と品詞を表示\n",
    "    print(f\"{word}\\t{base}\\t{pos}\\t{nrm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"シュミレーション\" を形態素解析し、正規化\n",
    "tkn = sdct.tokenize(\"シュミレーション\", mode)[0]\n",
    "word = tkn.surface()\n",
    "nrm = tkn.normalized_form()\n",
    "print(word, \"->\", nrm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moved-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"シュミレート\" を形態素解析し、正規化\n",
    "tkn = sdct.tokenize(\"シュミレート\", mode)[0]\n",
    "word = tkn.surface()\n",
    "nrm = tkn.normalized_form()\n",
    "print(word, \"->\", nrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-anaheim",
   "metadata": {},
   "source": [
    "## 3-4-4 nagisa¶\n",
    "リカレントニューラルネットワークでモデル化実装されている、pipだけでインストール可能。  \n",
    "他の辞書と同様に辞書が実装できる、また絵文字やURLに対しても解析が出来る特徴を持ちます。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "continent-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nagisa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nagisa'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nagisa\n",
    "\n",
    "\n",
    "# 1行をパース\n",
    "parsed = nagisa.tagging(line)\n",
    "\n",
    "# 分割した単語、品詞単位でループ処理\n",
    "parsed_nagisa = []\n",
    "for word, pos in zip(parsed.words, parsed.postags):\n",
    "    # 単語を保持\n",
    "    parsed_nagisa.append(word)\n",
    "\n",
    "    # 単語と品詞を表示\n",
    "    print(f\"{word}\\t{pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "elect-trinity",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nagisa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b9c825d47ffb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 品詞フィルタを生成 (除外する品詞リストを指定) ... Janome と同じリスト\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstop_poses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"BOS/EOS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"助詞\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"助動詞\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"接続詞\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"記号\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"補助記号\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"未知語\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnagisa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_postags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_poses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 分割した単語、品詞単位でループ処理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nagisa' is not defined"
     ]
    }
   ],
   "source": [
    "# 品詞フィルタを生成 (除外する品詞リストを指定) ... Janome と同じリスト\n",
    "stop_poses = [\"BOS/EOS\", \"助詞\", \"助動詞\", \"接続詞\", \"記号\", \"補助記号\", \"未知語\"]\n",
    "parsed = nagisa.filter(line, filter_postags=stop_poses)\n",
    "\n",
    "# 分割した単語、品詞単位でループ処理\n",
    "for word, pos in zip(parsed.words, parsed.postags):\n",
    "    # 単語と品詞を表示\n",
    "    print(f\"{word}\\t{pos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "prospective-newport",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nagisa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-90bb12044b5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# URL, 絵文字を含むテキストをパース\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnagisa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 分割した単語、品詞単位でループ処理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nagisa' is not defined"
     ]
    }
   ],
   "source": [
    "# URL, 絵文字を含むテキストを用意\n",
    "text = 'https://www.google.co.jp/で検索中・・・（-o-;）'\n",
    "\n",
    "# URL, 絵文字を含むテキストをパース\n",
    "parsed = nagisa.tagging(text)\n",
    "\n",
    "# 分割した単語、品詞単位でループ処理\n",
    "for word, pos in zip(parsed.words, parsed.postags):\n",
    "    # 単語と品詞を表示\n",
    "    print(f\"{word}\\t{pos}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-orange",
   "metadata": {},
   "source": [
    "## 3-4-5 Sentence Piece\n",
    "単語ではなく、部分単語(サブワード、ワブトークン)単位で分割できるツール。  \n",
    "ニューラルネットワークの教師なし学習でモデル化されている。  \n",
    "学習済みモデルを内包するnagisaとは異なり、与えれた文書集合（コーパス）から学習する事ができます。(学習させないといけない)  \n",
    "昨今注目されているELMo、BEATもサブワードによる分割を利用しているので注目されつつあります。  \n",
    "但し、単語単位では無いので単語が与える影響を考える際は、工夫する必要があります。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "funky-questionnaire",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lxml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-a3191563eafb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lxml'"
     ]
    }
   ],
   "source": [
    "import lxml.html\n",
    "import warnings\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "# 文書(行リスト)とラベルを保持するクラス\n",
    "class DataRecord(object):\n",
    "    def __init__(self, doc, label):\n",
    "        self.doc = doc\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "# 青空文庫の文書集合を扱うクラス\n",
    "class Aozoraset(object):\n",
    "    url = \"http://aozora-word.hahasoha.net/aozora_word_list_utf8.csv.gz\"\n",
    "\n",
    "    # 初期化処理\n",
    "    def __init__(self, seed=777, n_docs=10):\n",
    "        self.seed = seed\n",
    "        self.n_docs = n_docs\n",
    "        self.rs = numpy.random.RandomState(seed)\n",
    "        self.list_df = None\n",
    "        self.dataset = None\n",
    "        self.labelset = None\n",
    "        \n",
    "    # 青空文庫を読み込む処理\n",
    "    def load(self):\n",
    "        # 一覧を読み込み、コンテンツを読み込む\n",
    "        self._load_list()._load_contents()\n",
    "        \n",
    "    # 青空文庫の一覧を読み込みする処理\n",
    "    def _load_list(self):\n",
    "        # 青空文庫の一覧URLから一覧表を読み込み\n",
    "        self.list_df = ( pandas.read_csv(self.url, header=0, encoding=\"UTF-8\")\n",
    "                        .drop_duplicates([\"作品id\"]) )\n",
    "\n",
    "        return self\n",
    "        \n",
    "    # 一覧に対応する文書コンテンツ(集合)を読み込みする処理\n",
    "    def _load_contents(self):\n",
    "        # `n_docs` の数だけ、文書を取得\n",
    "        dataset = []\n",
    "        for idx, rec in tqdm(self.list_df.iloc[:self.n_docs].iterrows(), total=self.n_docs):\n",
    "            try:\n",
    "                # URL、ラベル情報を取得\n",
    "                url = rec[\"XHTML/HTMLファイルURL\"]\n",
    "                label = rec[\"分類番号\"]\n",
    "\n",
    "                # コンテンツを取得\n",
    "                content = self._load_content(url)\n",
    "                lines = content.split(\"\\n\")\n",
    "\n",
    "                # クレンジング\n",
    "                lines = self._clean_lines(lines)\n",
    "\n",
    "                # 行リスト、label をまとめてインスタンス化し、リストに保持\n",
    "                drec = DataRecord(lines, label)\n",
    "                dataset.append(drec)\n",
    "            except Exception as e:\n",
    "                # 例外発生時は、警告表示し当該処理をスキップ(処理を継続)\n",
    "                warnings.warn(f\"Couldn't get the html via http[{e}], url: {url}\")\n",
    "                continue\n",
    "        self.dataset = numpy.array(dataset)\n",
    "        return self\n",
    "\n",
    "    # 指定されたURLのコンテンツ(本文)を読み込みする処理\n",
    "    def _load_content(self, url):\n",
    "        # 指定されたURL にリクエストし、レスポンスを取得\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # HTTPコードが200以外の場合は、例外処理\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"response code: {response.status_code}\")\n",
    "\n",
    "        # HTMLオブジェクトを取得\n",
    "        html = lxml.html.fromstring(response.content)\n",
    "\n",
    "        # XPathを利用して、コンテンツを取得\n",
    "        main_text = html.xpath(\"//div[@class='main_text']\")\n",
    "        content = main_text[0].text_content()\n",
    "        return content\n",
    "\n",
    "    # 与えられた文書(行リスト)のクレンジング処理\n",
    "    @staticmethod\n",
    "    def _clean_lines(lines):\n",
    "        prepped_lines = []\n",
    "        for line in lines:\n",
    "            # 行の先頭、末尾の空白・改行コードを削除\n",
    "            line = line.strip()\n",
    "        \n",
    "            # Unicode の全角スペースを削除\n",
    "            line = re.sub(r\"\\u3000\", \" \", line)\n",
    "        \n",
    "            # ふりがなを削除\n",
    "            line = re.sub(r\"（.*?）\", \"\", line)\n",
    "        \n",
    "            # 上記変換により、空行になったらスキップ\n",
    "            if line == \"\":\n",
    "                continue\n",
    "        \n",
    "            # 変換後の line を処理済行リストとして保持\n",
    "            prepped_lines.append(line)\n",
    "        return prepped_lines        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "superb-clock",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf aoset.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "right-eugene",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Aozoraset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Aozoraset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "import joblib\n",
    "import pathlib\n",
    "\n",
    "\n",
    "# データセットインスタンスを生成\n",
    "aoset = Aozoraset()\n",
    "\n",
    "# データセットを保存するファイルパスを保持\n",
    "aoset_file = \"aoset.gz\"\n",
    "p = pathlib.Path(aoset_file)\n",
    "\n",
    "# 過去に保存済のデータセットがあれば保存済みデータセットを読み込み\n",
    "if p.exists():\n",
    "    aoset = joblib.load(aoset_file)\n",
    "    assert isinstance(aoset, Aozoraset)\n",
    "\n",
    "# 過去に保存済のデータセットがなければ、青空文庫のサイトから読み込み\n",
    "else:\n",
    "    # 青空文庫データセットの読み込み\n",
    "    aoset.load()\n",
    "\n",
    "    # 正常に読み込めたら、ローカルに保存しておく\n",
    "    joblib.dump(aoset, aoset_file, compress=(\"gzip\", 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "official-hobby",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aoset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aoset' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 青空文庫データセットを文書単位で、行リストをすべて同じファイル(`sp.txt`)に出力\n",
    "with open(\"sp.txt\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    for drec in aoset.dataset:\n",
    "        for _line in drec.doc:\n",
    "            f.write(_line + \"\\n\")\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "# Sentence Piece の学習\n",
    "spm.SentencePieceTrainer.train('--input=sp.txt --model_prefix=m --vocab_size=5000')\n",
    "\n",
    "# Sentence Piece の学習済モデル(`m.model`)を読み込み\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(\"m.model\")\n",
    "\n",
    "# 1行を分割したピースのリストを取得し表示\n",
    "parsed_spieces = sp.encode_as_pieces(line)\n",
    "print(parsed_spieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ethical-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディスクの有効活用のため、不要ファイルを削除\n",
    "!rm -f sp.txt m.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-creation",
   "metadata": {},
   "source": [
    "# Chapter3-5 ベクトル化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-bicycle",
   "metadata": {},
   "source": [
    "単語、サブワードに分割した結果を数値処理できるように各単語や各文章をベクトルに変換します。  \n",
    "\n",
    "ベクトル化では基本的に疎なベクトル（ほとんどが0をとるベクトル）を作成し、そのあと何らかの変換を行い密なベクトルへ変換します。  \n",
    "この密なベクトルを特徴抽出、埋め込みとも言います。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-symbol",
   "metadata": {},
   "source": [
    "## 3-5-1 単語のベクトル化\n",
    "基本的なOne-Hotベクトルと、One-Hotの応用で利用も多いword2vecがある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "inclusive-analysis",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'MeCab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-4a5d916d372f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mMeCab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Transer クラス\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTranser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'MeCab'"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "# Transer クラス\n",
    "class Transer(object):\n",
    "    def transform(self, X, **kwargs):\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        return self\n",
    "\n",
    "\n",
    "# JpTokenizer クラス\n",
    "class JpTokenizer(Transer):\n",
    "    stop_poses = [\"BOS/EOS\", \"助詞\", \"助動詞\", \"接続詞\", \"記号\", \"補助記号\", \"未知語\"]\n",
    "\n",
    "    def transform(self, X, **kwargs):\n",
    "        # 文書(行リスト)単位でループ\n",
    "        docs = []\n",
    "        for lines in X:\n",
    "            doc = []\n",
    "            # 行単位でループ\n",
    "            for line in lines:\n",
    "                sentence = self.tokenize(line)\n",
    "                doc.extend(sentence)\n",
    "            docs.append(doc)\n",
    "        return docs\n",
    "\n",
    "    # 行単位のトークナイズ処理\n",
    "    def tokenize(self, line: str) -> list:\n",
    "        # return line.split(\" \") for example\n",
    "        raise NotImplementedError(\"tokenize()\")\n",
    "\n",
    "\n",
    "# JpTokenizerMeCab クラス\n",
    "class JpTokenizerMeCab(JpTokenizer):\n",
    "    # 初期処理\n",
    "    def __init__(self):\n",
    "        self.dicdir = (\"/usr/lib/x86_64-linux-gnu/mecab/dic\"\n",
    "                       \"/mecab-ipadic-neologd\")\n",
    "        self.taggerstr = f\"-O chasen -d {self.dicdir}\"\n",
    "        self.tokenizer = MeCab.Tagger(self.taggerstr)\n",
    "\n",
    "    # 行単位のトークナイズ処理(の実装)\n",
    "    def tokenize(self, line: str) -> list:\n",
    "        # 行文字列を受け取りトークナイズを行う\n",
    "        sentence = []\n",
    "        parsed = self.tokenizer.parse(line)\n",
    "        splitted = [l.split(\"\\t\") for l in parsed.split(\"\\n\")]\n",
    "        for s in splitted:\n",
    "            if len(s) == 1:     # may be \"EOS\"\n",
    "                break\n",
    "            surface, yomi, base, features = s[:4]\n",
    "            word = surface      # surface form\n",
    "            # word = base         # original form\n",
    "            pos = features.split(\"-\")[0]\n",
    "            if pos not in self.stop_poses:\n",
    "                sentence.append(word)\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dental-colors",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JpTokenizerMeCab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f3a7b4cb92db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# MeCabのトークナイザを生成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJpTokenizerMeCab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 青空文庫のデータセットから文書のみを抽出\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdrec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maoset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'JpTokenizerMeCab' is not defined"
     ]
    }
   ],
   "source": [
    "# MeCabのトークナイザを生成\n",
    "tokener = JpTokenizerMeCab()\n",
    "\n",
    "# 青空文庫のデータセットから文書のみを抽出\n",
    "X = [drec.doc for drec in aoset.dataset]\n",
    "\n",
    "# 文書集合に対して形態素解析を実行\n",
    "parsed_docs = tokener.transform(X)\n",
    "\n",
    "# 解析後の文書を先頭から5つを表示\n",
    "for idx in range(5):\n",
    "    print(f\"{idx}:\", *parsed_docs[idx][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-saturday",
   "metadata": {},
   "source": [
    "### 3-5-1-1 One-Hot\n",
    "One-Hotベクトルとは、特定のK番目の１要素が1で残りの要素全てが0であるようなベクトルを指します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "handed-principle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn による One-Hot ベクトル化\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "X = toydata = [['Male'], ['Female'], ['Female']]\n",
    "onehot.fit(X)\n",
    "onehot.transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "covered-footage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Female</th>\n",
       "      <th>Male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Female  Male\n",
       "0       0     1\n",
       "1       1     0\n",
       "2       1     0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas による One-Hot ベクトル化\n",
    "pandas.get_dummies(['Male', 'Female', 'Female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "secondary-croatia",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-1f6eae9a918f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# OneHotEncoder に入力するために変換\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed_docs\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"words:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# One-Hot ベクトルに変換\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_docs' is not defined"
     ]
    }
   ],
   "source": [
    "# OneHotEncoder に入力するために変換\n",
    "words = [[word] for doc in parsed_docs for word in doc]\n",
    "print(\"words:\", words[:10], \"...\")\n",
    "\n",
    "# One-Hot ベクトルに変換\n",
    "onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "X = words\n",
    "onehot.fit(X)\n",
    "vectors_onehot = onehot.transform(X).toarray().astype(numpy.int)\n",
    "\n",
    "print(\"vectors_onehot:\", vectors_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "imposed-stable",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors_onehot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-c74f1092e25a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 各ベクトル（各行）の要素の最小値が0であることを確認\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min==0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 各ベクトル（各行）の要素の最大値が1であることを確認\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max==1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectors_onehot' is not defined"
     ]
    }
   ],
   "source": [
    "# 各ベクトル（各行）の要素の最小値が0であることを確認\n",
    "print(\"min==0\", all(vectors_onehot.min(axis=1) == 0))\n",
    "\n",
    "# 各ベクトル（各行）の要素の最大値が1であることを確認\n",
    "print(\"max==1\", all(vectors_onehot.max(axis=1) == 1))\n",
    "\n",
    "# 各ベクトル（各行）の要素の合計値が1であることを確認\n",
    "print(\"sum==1\", all(vectors_onehot.sum(axis=1) == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "level-night",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors_onehot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-b7330c79b4b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# One-Hot ベクトルの型を表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"One-Hot:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors_onehot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 文書を単語分割(リスト化)した単語の数(重複あり)を表示\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"words:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectors_onehot' is not defined"
     ]
    }
   ],
   "source": [
    "# One-Hot ベクトルの型を表示\n",
    "print(\"One-Hot:\", vectors_onehot.shape)\n",
    "\n",
    "# 文書を単語分割(リスト化)した単語の数(重複あり)を表示\n",
    "print(\"words:\", len(words))\n",
    "\n",
    "# 文書に含まれる単語（語彙）の数(重複なし)を表示\n",
    "vocab = set([w[0] for w in words])\n",
    "print(\"vocab:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "brown-wonder",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-2c4c6f2027ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 文書の中の単語インデックスをランダムに取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ランダムに取得したインデックスに対応する単語ベクトルの次元(語彙)を特定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdim_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectors_onehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "# 文書の中の単語インデックスをランダムに取得\n",
    "idx = numpy.random.randint(0, len(words))\n",
    "\n",
    "# ランダムに取得したインデックスに対応する単語ベクトルの次元(語彙)を特定\n",
    "dim_idx = vectors_onehot[idx].argmax()\n",
    "\n",
    "# One-Hot ベクトルを作成\n",
    "v = numpy.zeros_like(vectors_onehot[idx], dtype=numpy.int)\n",
    "v[dim_idx] = 1\n",
    "assert numpy.all(v == vectors_onehot[idx])\n",
    "\n",
    "# One-Hot ベクトルから単語に変換(復元を試みる)\n",
    "w = onehot.inverse_transform([v]).ravel()\n",
    "\n",
    "# One-Hot ベクトルに変換して単語に逆変換できていることを確認\n",
    "print(words[idx], \"->\", v, f\"(k={dim_idx}/{vectors_onehot.shape[1]})\", \"->\", w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-ladder",
   "metadata": {},
   "source": [
    "### 3-5-1-2 Word2vec\n",
    "One-Hotベクトルを前後の文脈を使ってベクトル表現を得る方法です。  \n",
    "CBoWとSkip-gramという２つのモデルが提案され、gensimライブラリではパラメータで指定できます。\n",
    "\n",
    "- CBowは、対象単語の周辺単語から対象単語を推定できるようにエンコードします。\n",
    "- Skip-gramは対象の単語から周辺の単語を推定できるようにエンコードします。\n",
    "\n",
    "周辺の単語をめぐる長さの事をW(ウィンドウサイズ)と言います。\n",
    "\n",
    "Tips 局所表現と分散表現  \n",
    "One-Hotによるベクトルなどの疎なベクトル表現は局所表現で、word2vecなどの密なベクトル表現は分散表現によるベクトル表現です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "deluxe-finder",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-1533f89cebf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 簡易ハッシュ関数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 簡易ハッシュ関数\n",
    "def _hash(s):\n",
    "    n = len(s)\n",
    "    h = 0\n",
    "    for idx, c in enumerate(s):\n",
    "        h += ord(c) * math.factorial(n-idx)\n",
    "    return h\n",
    "\n",
    "vector_size = 128\n",
    "corpus = parsed_docs\n",
    "model = Word2Vec(corpus, size=vector_size, min_count=1, window=5, workers=1, seed=333, hashfxn=_hash)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "reasonable-mercy",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-61cdb528bdf7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# corpus の中からランダムに文書インデックスを選択\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdidx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 選んだ文書からランダムに単語インデックスを選択\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "# 再現性を担保するため、乱数のシードを明示的に指定\n",
    "numpy.random.seed(123)\n",
    "\n",
    "# corpus の中からランダムに文書インデックスを選択\n",
    "didx = numpy.random.randint(0, len(corpus))\n",
    "\n",
    "# 選んだ文書からランダムに単語インデックスを選択\n",
    "widx = numpy.random.randint(0, len(corpus[didx]))\n",
    "\n",
    "# ランダムに選んだインデックスに対応する単語を保持し、表示\n",
    "w = parsed_docs[didx][widx]\n",
    "print(\"word:\", w)\n",
    "\n",
    "# 選択した単語をベクトルに変換\n",
    "v = model.wv[w]\n",
    "\n",
    "# ベクトルを表示\n",
    "print(f\"vector[{w}]:\", v)\n",
    "\n",
    "# ベクトルの型を表示\n",
    "print(f\"vector[{w}].shape:\", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "canadian-boutique",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-9ea407b59d94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ベクトルにノイズを付与\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mvv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'v' is not defined"
     ]
    }
   ],
   "source": [
    "# 再現性を担保するため、乱数のシードを明示的に指定\n",
    "numpy.random.seed(123)\n",
    "\n",
    "# ベクトルにノイズを付与\n",
    "noise = numpy.random.normal(loc=0.0, scale=0.001, size=v.shape)\n",
    "vv = v + noise\n",
    "\n",
    "# 元ベクトルに一番近いベクトルを表示\n",
    "print(\"original word:\", model.wv.similar_by_vector(v, topn=1)[0])\n",
    "\n",
    "# 元ベクトルに近いベクトル、トップ4 を表示\n",
    "print(\"neighbours for v:\", model.wv.similar_by_vector(v, topn=4))\n",
    "\n",
    "# ノイズを付与した元ベクトルに一番近いベクトルを表示\n",
    "print(\"nearest neighbour for v + noise:\", model.wv.similar_by_vector(vv, topn=1))\n",
    "\n",
    "# ノイズを付与した元ベクトルに近いベクトル、トップ3 を表示\n",
    "print(\"neighbours for v + noise:\", model.wv.similar_by_vector(vv, topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aging-pharmacy",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-9c0f006e2bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"similar_by_word:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"similar_by_vector:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"word:\", w)\n",
    "print(\"similar_by_word:\", model.wv.similar_by_word(w, topn=3))\n",
    "print(\"similar_by_vector:\", model.wv.similar_by_vector(v, topn=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "golden-wrapping",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-e32bc901d2dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 類似単語を取得\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msimilars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 類似単語を取得\n",
    "similars = model.wv.similar_by_vector(v, topn=5)\n",
    "assert numpy.all(v == model.wv[w])\n",
    "\n",
    "# 元の単語(wとその単語に近い単語群でループ\n",
    "for simvec in similars[1:]:\n",
    "    w2, s2 = simvec\n",
    "    print(\"-\" * 50)\n",
    "    print(\"smilar_by_vector:\", w2, s2)\n",
    "\n",
    "    # cosine類似度を計算し表示\n",
    "    v2 = model.wv[w2]\n",
    "    cs = cosine_similarity([v], [v2])\n",
    "    print(\"cosine_similarity:\", w2, cs)\n",
    "    print(\"allclose:\", numpy.allclose(s2, cs[0][0]))\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-truck",
   "metadata": {},
   "source": [
    "## 3-5-2 文書のベクトル化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-canadian",
   "metadata": {},
   "source": [
    "### 3-5-2-1 BoW　(Bag of Words)\n",
    "One-Hotベクトルを拡張した文書のベクトル化手法。  \n",
    "文章内の単語の数をベクトルの要素数としてベクトル化する手法です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "catholic-welding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors_bow: [[2 1 0 0 1 1 0 2 1 1 1 0 0 2 0 0 1 1 2 0 0]\n",
      " [0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      " [2 1 1 0 0 0 0 2 0 0 1 1 0 2 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 恒等関数を用意\n",
    "# # CountVectorizer などで、英語としてトークナイズ処理をさせないようにするため\n",
    "def tokenize_idently(sentence: list):\n",
    "    return sentence\n",
    "\n",
    "# トイデータを用意\n",
    "toydocs = [\n",
    "    ['今日', 'は', '、', '晴れ', 'です', 'が', '、', \n",
    "        '明日', 'は', '晴れ', 'そう', 'に', 'ない', 'です', 'ね', '。'],\n",
    "    ['晴れ', 'たら', 'いい', 'のに', '。'],\n",
    "    ['今日', 'は', '、', 'あいにく', 'の', '雨', 'です', 'ね', '。',\n",
    "        '明日', 'は', '、', '晴れ', 'らしい', 'です', 'よ', '！'],\n",
    "]\n",
    "corpus = toydocs\n",
    "\n",
    "# BoW ベクトルに変換\n",
    "bow = CountVectorizer(lowercase=False, tokenizer=tokenize_idently)\n",
    "vectors_bow = bow.fit_transform(corpus).toarray()\n",
    "\n",
    "# BoW ベクトルを表示\n",
    "print(\"vectors_bow:\", vectors_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "neutral-employer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'今日': 16,\n",
       " 'は': 13,\n",
       " '、': 0,\n",
       " '晴れ': 18,\n",
       " 'です': 7,\n",
       " 'が': 4,\n",
       " '明日': 17,\n",
       " 'そう': 5,\n",
       " 'に': 9,\n",
       " 'ない': 8,\n",
       " 'ね': 10,\n",
       " '。': 1,\n",
       " 'たら': 6,\n",
       " 'いい': 3,\n",
       " 'のに': 12,\n",
       " 'あいにく': 2,\n",
       " 'の': 11,\n",
       " '雨': 19,\n",
       " 'らしい': 15,\n",
       " 'よ': 14,\n",
       " '！': 20}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "compliant-lawyer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['、', '。', 'が', 'そう', 'です', 'ない', 'に', 'ね', 'は', '今日', '明日', '晴れ'],\n",
       "       dtype='<U4'),\n",
       " array(['。', 'いい', 'たら', 'のに', '晴れ'], dtype='<U4'),\n",
       " array(['、', '。', 'あいにく', 'です', 'ね', 'の', 'は', 'よ', 'らしい', '今日', '明日',\n",
       "        '晴れ', '雨', '！'], dtype='<U4')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.inverse_transform(vectors_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "alone-muslim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 0 0 1 1 0 2 1 1 1 0 0 2 0 0 1 1 2 0 0]\n",
      " [0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      " [2 1 1 0 0 0 0 2 0 0 1 1 0 2 1 1 1 1 1 1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-73-4c40960ff7ca>:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  vectors_onehot = onehot.transform(X).toarray().astype(numpy.int)\n",
      "<ipython-input-73-4c40960ff7ca>:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  vectors_onehot = onehot.transform(X).toarray().astype(numpy.int)\n",
      "<ipython-input-73-4c40960ff7ca>:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  vectors_onehot = onehot.transform(X).toarray().astype(numpy.int)\n"
     ]
    }
   ],
   "source": [
    "# OneHotEncoder へ入力するために、corpus を変換\n",
    "corpus = toydocs\n",
    "words = [[w] for s in corpus for w in s]\n",
    "\n",
    "# corpus で学習\n",
    "onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "X = words\n",
    "onehot.fit(X)\n",
    "\n",
    "# corpus の文書単位でループ\n",
    "sum_onehots = []\n",
    "for doc in corpus:\n",
    "    # corpus を文書単位でOne-Hotベクトルに変換\n",
    "    X = [[w] for w in doc]\n",
    "    vectors_onehot = onehot.transform(X).toarray().astype(numpy.int)\n",
    "\n",
    "    # 文書単位で、One-Hot ベクトルの合計ベクトルを算出\n",
    "    sum_onehot = vectors_onehot.sum(axis=0)\n",
    "    sum_onehots.append(sum_onehot)\n",
    "sum_onehots = numpy.array(sum_onehots)\n",
    "\n",
    "# 作成した文書毎のベクトルを表示\n",
    "print(sum_onehots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "statistical-fleece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 0 0 1 1 0 2 1 1 1 0 0 2 0 0 1 1 2 0 0]\n",
      " [0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0]\n",
      " [2 1 1 0 0 0 0 2 0 0 1 1 0 2 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vectors_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "collected-shuttle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert (sum_onehots == vectors_bow).all()   # 要素すべてが一致しているはず、ということの表明（満たされなかったらエラー）\n",
    "sum_onehots == vectors_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-siemens",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "electrical-gnome",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-49a4998cd6ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_idently\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectors_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvectors_bow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_docs' is not defined"
     ]
    }
   ],
   "source": [
    "bow = CountVectorizer(lowercase=False, tokenizer=tokenize_idently)\n",
    "vectors_bow = bow.fit_transform(parsed_docs).toarray()\n",
    "vectors_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "opposite-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 21)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "exempt-american",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42857143, 0.76190476, 0.33333333])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vectors_bow == 0).sum(axis=1) / vectors_bow.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-correction",
   "metadata": {},
   "source": [
    "### 3-5-2-2 TF-IDF (Term Frequency Inverse Document Frequency)\n",
    "BoWを拡張した文書のベクトル化手法です。  \n",
    "BoWベクトルを正規化したTFに、各単語を含む文書数の逆数に相当するIDFを掛けることで、  \n",
    "どの文章にも高頻度で現れる女子や句読点などの単語に低い重みをつけてエンコードし、  \n",
    "逆に特定の文書にしか表れない単語を重要語として高い重みをつけてエンコードします。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "geological-vertex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40055242, 0.15553234, 0.        , 0.        , 0.26333915,\n",
       "        0.26333915, 0.        , 0.40055242, 0.26333915, 0.26333915,\n",
       "        0.20027621, 0.        , 0.        , 0.40055242, 0.        ,\n",
       "        0.        , 0.20027621, 0.20027621, 0.31106469, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.30714405, 0.        , 0.52004008, 0.        ,\n",
       "        0.        , 0.52004008, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.52004008, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30714405, 0.        ,\n",
       "        0.        ],\n",
       "       [0.38793189, 0.15063186, 0.25504191, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.38793189, 0.        , 0.        ,\n",
       "        0.19396595, 0.25504191, 0.        , 0.38793189, 0.25504191,\n",
       "        0.25504191, 0.19396595, 0.19396595, 0.15063186, 0.25504191,\n",
       "        0.25504191]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = toydocs\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenize_idently)\n",
    "vectors_tfidf = tfidf.fit_transform(corpus).toarray()\n",
    "vectors_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "infrared-short",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 0.9999999999999999, 0.9999999999999999]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[numpy.linalg.norm(docvec) for docvec in vectors_tfidf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "pretty-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = toydocs\n",
    "\n",
    "# BoW ベクトルに変換\n",
    "bow = CountVectorizer(lowercase=False, tokenizer=tokenize_idently)\n",
    "vectors_bow = bow.fit_transform(corpus).toarray()\n",
    "\n",
    "# tf を計算 (BoW を正規化)\n",
    "# # BoW ベクトル(各単語の各文書における出現頻度)を各文書の単語数で正規化\n",
    "vectors_tf = vectors_bow / vectors_bow.sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "# 各文書が単語を含む(1)か否(0)かをベクトル化\n",
    "vectors_td = (vectors_bow > 0).astype(numpy.float32)\n",
    "\n",
    "# idf の計算 / 単語を含む文書の発生確率に対する情報量を計算\n",
    "all_df = len(vectors_td)\n",
    "vectors_df = vectors_td.sum(axis=0) \n",
    "vectors_idf = numpy.log( (all_df + 1) / (vectors_df + 1) ) + 1\n",
    "\n",
    "# tfidf を計算し、L2ノルムで正規化\n",
    "vectors_tfidf_custom = vectors_tf * vectors_idf\n",
    "vectors_tfidf_custom /= numpy.linalg.norm(vectors_tfidf_custom, axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "temporal-diagram",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40055242, 0.15553235, 0.        , 0.        , 0.26333915,\n",
       "        0.26333915, 0.        , 0.40055242, 0.26333915, 0.26333915,\n",
       "        0.20027621, 0.        , 0.        , 0.40055242, 0.        ,\n",
       "        0.        , 0.20027621, 0.20027621, 0.31106469, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.30714405, 0.        , 0.52004008, 0.        ,\n",
       "        0.        , 0.52004008, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.52004008, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30714405, 0.        ,\n",
       "        0.        ],\n",
       "       [0.38793189, 0.15063186, 0.25504191, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.38793189, 0.        , 0.        ,\n",
       "        0.19396595, 0.25504191, 0.        , 0.38793189, 0.25504191,\n",
       "        0.25504191, 0.19396595, 0.19396595, 0.15063186, 0.25504191,\n",
       "        0.25504191]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_tfidf_custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "neural-julian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40055242, 0.15553234, 0.        , 0.        , 0.26333915,\n",
       "        0.26333915, 0.        , 0.40055242, 0.26333915, 0.26333915,\n",
       "        0.20027621, 0.        , 0.        , 0.40055242, 0.        ,\n",
       "        0.        , 0.20027621, 0.20027621, 0.31106469, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.30714405, 0.        , 0.52004008, 0.        ,\n",
       "        0.        , 0.52004008, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.52004008, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.30714405, 0.        ,\n",
       "        0.        ],\n",
       "       [0.38793189, 0.15063186, 0.25504191, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.38793189, 0.        , 0.        ,\n",
       "        0.19396595, 0.25504191, 0.        , 0.38793189, 0.25504191,\n",
       "        0.25504191, 0.19396595, 0.19396595, 0.15063186, 0.25504191,\n",
       "        0.25504191]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "devoted-crown",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.allclose(vectors_tfidf_custom, vectors_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "motivated-madagascar",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-bfba38b12daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenize_idently\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectors_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vectors_tfidf:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectors_tfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_docs' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = parsed_docs\n",
    "tfidf = TfidfVectorizer(lowercase=False, tokenizer=tokenize_idently)\n",
    "vectors_tfidf = tfidf.fit_transform(corpus).toarray()\n",
    "print(\"vectors_tfidf:\", vectors_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "gentle-harrison",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 21)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "rural-headline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42857143, 0.76190476, 0.33333333])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vectors_tfidf == 0).sum(axis=1) / vectors_tfidf.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-difficulty",
   "metadata": {},
   "source": [
    "### 3-5-2-3 Doc2vec\n",
    "word2vecの文章ベクトル版です。  \n",
    "word2vecによるベクトル化に加え、各文章に対応するParagraphidをベクトル化した結果のベクトルを使って  \n",
    "入力した単語から対象の単語を推定できるように学習します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "overhead-disorder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['今日', 'は', '、', '晴れ', 'です', 'が', '、', '明日', 'は', '晴れ', 'そう', 'に', 'ない', 'です', 'ね', '。'], ['晴れ', 'たら', 'いい', 'のに', '。'], ['今日', 'は', '、', 'あいにく', 'の', '雨', 'です', 'ね', '。', '明日', 'は', '、', '晴れ', 'らしい', 'です', 'よ', '！']]\n"
     ]
    }
   ],
   "source": [
    "corpus = toydocs\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ignored-decimal",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-92b1bfa66567>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Doc2Vec のパラメータを設定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m params = dict(\n\u001b[1;32m      5\u001b[0m     \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Doc2Vec のパラメータを設定\n",
    "params = dict(\n",
    "    vector_size=7,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=2,\n",
    "    seed=777,\n",
    ")\n",
    "\n",
    "# doc2vec に入力するためにTaggedDocument のリストを生成\n",
    "tagged_docs = [TaggedDocument(doc, [idx, f\"doc-{idx:02d}\"]) for idx, doc in enumerate(corpus)]\n",
    "\n",
    "# doc2vec の学習を実行\n",
    "model = Doc2Vec(tagged_docs, **params)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "sudden-cream",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-6d6178409ee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"晴れ\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{w}:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "w = \"晴れ\"\n",
    "print(f\"{w}:\", model.wv[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "metallic-student",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph: ['今日', 'は', '、', '晴れ', 'です', 'が', '、', '明日', 'は', '晴れ', 'そう', 'に', 'ない', 'です', 'ね', '。']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-0ce691b110c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paragraph:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paragraph vector (trained):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"doc-00\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paragraph vector (infer) 1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paragraph vector (infer) 2:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "paragraph = corpus[0]\n",
    "print(\"paragraph:\", paragraph)\n",
    "print(\"paragraph vector (trained):\", model[\"doc-00\"])\n",
    "print(\"paragraph vector (infer) 1:\", model.infer_vector(paragraph))\n",
    "print(\"paragraph vector (infer) 2:\", model.infer_vector(paragraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "micro-dividend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph: ['今日', 'は']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-0807629ebc0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paragraph:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "paragraph = [\"今日\", \"は\"]\n",
    "print(\"paragraph:\", paragraph)\n",
    "\n",
    "pv1 = model.infer_vector(paragraph)\n",
    "pv2 = model.infer_vector(paragraph)\n",
    "\n",
    "print(\"paragraph vector 1:\", pv1)\n",
    "print(\"paragraph vector 2:\", pv2)\n",
    "print(\"pv1 == pv2:\", pv1==pv2)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cs = cosine_similarity([pv1], [pv2])\n",
    "print(\"cosine similarity:\", cs)\n",
    "print(\"allclose:\", numpy.allclose(cs, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "acute-writing",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-08736f4179b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TaggedDocument のリストを生成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtagged_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"doc-{idx:02d}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Doc2Vec のパラメータを設定\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_docs' is not defined"
     ]
    }
   ],
   "source": [
    "# TaggedDocument のリストを生成\n",
    "corpus = parsed_docs\n",
    "tagged_docs = [TaggedDocument(doc, [idx, f\"doc-{idx:02d}\"]) for idx, doc in enumerate(corpus)]\n",
    "\n",
    "# Doc2Vec のパラメータを設定\n",
    "params = dict(\n",
    "    vector_size=128,  # 十分な次元数に変更\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=2,\n",
    "    epochs=7,  # 学習回数を7 に変更\n",
    ")\n",
    "\n",
    "# 学習を実行\n",
    "model = Doc2Vec(tagged_docs, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "crucial-artist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 1e+03 ns, total: 11 µs\n",
      "Wall time: 24.1 µs\n",
      "paragraph: ['今日', 'は', '、', '晴れ', 'です', 'が', '、', '明日', 'は', '晴れ', 'そう', 'に', 'ない', 'です', 'ね', '。']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-3d45c03ec3ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paragraph:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "%time\n",
    "paragraph = corpus[0]\n",
    "print(\"paragraph:\", paragraph)\n",
    "\n",
    "pv1 = model.infer_vector(paragraph)\n",
    "pv2 = model.infer_vector(paragraph)\n",
    "\n",
    "print(\"paragraph vector:\", pv1[:3], \"...\")\n",
    "print(\"paragraph vector:\", pv2[:3], \"...\")\n",
    "\n",
    "cs = cosine_similarity([pv1], [pv2])\n",
    "print(\"cosine similarity:\", cs)\n",
    "print(\"allclose:\", numpy.allclose(cs, 1))\n",
    "print(\"mse:\", ((pv1-pv2)**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "alternate-tracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 0 ns, total: 9 µs\n",
      "Wall time: 14.8 µs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-d3c960e8cc04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paragraph vector:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpv1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"paragraph vector:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpv2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "%time\n",
    "pv1 = model.infer_vector(paragraph, epochs=500)\n",
    "pv2 = model.infer_vector(paragraph, epochs=500)\n",
    "print(\"paragraph vector:\", pv1[:3], \"...\")\n",
    "print(\"paragraph vector:\", pv2[:3], \"...\")\n",
    "print(\"cosine similarity:\", cosine_similarity([pv1], [pv2]))\n",
    "print(\"allclose:\", numpy.allclose(cs, 1))\n",
    "print(\"mse:\", ((pv1-pv2)**2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-consequence",
   "metadata": {},
   "source": [
    "# Chapter3-6 オーグメンテーション（データの増幅)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-venture",
   "metadata": {},
   "source": [
    "純粋に精度を上げるという効果がある訳ではなく、学習データに類似するデータに対する頑健性を高める効果があり、  \n",
    "結果として検証データ、テストデータに対して副次的に精度が向上する事がある。  \n",
    "そのため、学習データに対して類似しないデータに対しての精度向上には繋がらない。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-tutorial",
   "metadata": {},
   "source": [
    "## 3-6-1 データ収集時のオーグメンテーション\n",
    "最も効果が見込める方法は人手によるオーグメンテーションです。  \n",
    "企業固有の表現や言い回しがあるため、導入先の企業社員の方やビジネスサイドの社員の方に協力を仰ぎ、増幅に協力してもらう事が有効です。\n",
    "\n",
    "導入先企業での作業が難しい場合  \n",
    "- AI/自然言語処理の品質を落としてもよい → 分析する企業やAIベンダ、専門業者に頼みましょう。\n",
    "- 人手を裂けない、多少変な日本語でも良い → 機械翻訳（google翻訳など） → 無料は学習１回しかできない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "annoying-edwards",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parsed_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-4ea59f2744ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparsed_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'parsed_docs' is not defined"
     ]
    }
   ],
   "source": [
    "parsed_docs[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fewer-madrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-translate\n",
      "  Downloading google_cloud_translate-3.1.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 1.5 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.3.0\n",
      "  Downloading google_cloud_core-1.6.0-py2.py3-none-any.whl (28 kB)\n",
      "Collecting proto-plus>=0.4.0\n",
      "  Downloading proto_plus-1.18.1-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 232 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting google-api-core[grpc]<2.0.0dev,>=1.22.2\n",
      "  Downloading google_api_core-1.26.3-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[K     |████████████████████████████████| 93 kB 203 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-translate) (2.25.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-translate) (3.14.0)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-translate) (1.15.0)\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz in /opt/conda/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-translate) (2021.1)\n",
      "Collecting google-auth<2.0dev,>=1.21.1\n",
      "  Downloading google_auth-1.28.1-py2.py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-translate) (54.1.1)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.8/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-translate) (20.9)\n",
      "Collecting grpcio<2.0dev,>=1.29.0\n",
      "  Downloading grpcio-1.37.0-cp38-cp38-manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 16.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-translate) (2.4.7)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 1.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-translate) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-translate) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-translate) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.22.2->google-cloud-translate) (2.10)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, cachetools, googleapis-common-protos, google-auth, grpcio, google-api-core, proto-plus, google-cloud-core, google-cloud-translate\n",
      "Successfully installed cachetools-4.2.1 google-api-core-1.26.3 google-auth-1.28.1 google-cloud-core-1.6.0 google-cloud-translate-3.1.0 googleapis-common-protos-1.53.0 grpcio-1.37.0 proto-plus-1.18.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-cloud-translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "above-rugby",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "entire-harmony",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "basic-marijuana",
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "File gdrive/My Drive/secret/key.json was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-1551a32c0bda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Client インスタンスを生成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtranslate_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 指定された行(lines)毎に、日->英->日の翻訳で文章を作成する関数を定義\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/google/cloud/translate_v2/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, target_language, credentials, _http, client_info, client_options)\u001b[0m\n\u001b[1;32m     80\u001b[0m     ):\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_language\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_http\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_http\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mkw_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"client_info\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclient_info\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/google/cloud/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, credentials, _http, client_options)\u001b[0m\n\u001b[1;32m    151\u001b[0m                     client_options.credentials_file, scopes=scopes)\n\u001b[1;32m    152\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscopes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscopes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         self._credentials = google.auth.credentials.with_scopes_if_required(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/google/auth/_default.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchecker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheckers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mcredentials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             credentials = with_scopes_if_required(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/google/auth/_default.py\u001b[0m in \u001b[0;36m_get_explicit_environ_credentials\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexplicit_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         credentials, project_id = load_credentials_from_file(\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menvironment_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCREDENTIALS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/google/auth/_default.py\u001b[0m in \u001b[0;36mload_credentials_from_file\u001b[0;34m(filename, scopes, default_scopes, quota_project_id, request)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         raise exceptions.DefaultCredentialsError(\n\u001b[0m\u001b[1;32m    108\u001b[0m             \u001b[0;34m\"File {} was not found.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         )\n",
      "\u001b[0;31mDefaultCredentialsError\u001b[0m: File gdrive/My Drive/secret/key.json was not found."
     ]
    }
   ],
   "source": [
    "# 環境変数に、秘密鍵ファイル(JSON)を指定\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"gdrive/My Drive/secret/key.json\"\n",
    "\n",
    "# Translate APIv2 をインポート\n",
    "from google.cloud import translate_v2 as translate\n",
    "\n",
    "# Client インスタンスを生成\n",
    "translate_client = translate.Client()\n",
    "\n",
    "# 指定された行(lines)毎に、日->英->日の翻訳で文章を作成する関数を定義\n",
    "def augment_by_translation(lines: list, do_print=True):\n",
    "    augmented = []\n",
    "    for text in lines:\n",
    "        # 翻訳(日->英)\n",
    "        translation = translate_client.translate(\n",
    "            text, target_language='en')\n",
    "        translated_en = translation['translatedText']\n",
    "        \n",
    "        # 再翻訳(英->日)\n",
    "        translation = translate_client.translate(\n",
    "            translated_en, target_language='ja')\n",
    "        restored = translation['translatedText']\n",
    "\n",
    "        # 増幅データとして保持\n",
    "        augmented.append(restored)\n",
    "\n",
    "        # テキストを表示\n",
    "        if do_print:\n",
    "            print(\"-\" * 80)\n",
    "            print(f'Original: {text}')\n",
    "            print(f'English: {translated_en}')\n",
    "            print(f'Restored: {restored}')\n",
    "    return augmented\n",
    "\n",
    "augmented = augment_by_translation(prepped_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "theoretical-invite",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'augment_by_translation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-12e7db0b0cf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maugmented2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_by_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmented\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'augment_by_translation' is not defined"
     ]
    }
   ],
   "source": [
    "augmented2 = augment_by_translation(augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-small",
   "metadata": {},
   "source": [
    "## 3-6-2 形態素解析後のオーグメンテーション"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-african",
   "metadata": {},
   "source": [
    "形態素解析後では、別の類義語に置き換えることで、データ増幅をする事ができます。  \n",
    "- Sudachiの類義語辞書らを選択する方法\n",
    "- Wikipediaのword2vecから類義語を選択する方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "burning-intro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer\t\t\t   preprocess_knock_Python.ipynb\n",
      "data\t\t\t   preprocess_knock_R.ipynb\n",
      "Entity_Relationship.ipynb  preprocess_knock_SQL.ipynb\n",
      "genbapro_chapter3.ipynb    wikipedia.json\n",
      "--2021-04-10 08:03:16--  https://github.com/singletongue/WikiEntVec/releases/download/20190520/jawiki.all_vectors.100d.txt.bz2\n",
      "Resolving github.com (github.com)... 52.192.72.89\n",
      "Connecting to github.com (github.com)|52.192.72.89|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-releases.githubusercontent.com/141127256/0f0df900-8e07-11e9-8309-25da3d3b21b1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210410%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210410T080316Z&X-Amz-Expires=300&X-Amz-Signature=ba411812fd883ad11b4b903933ddc0e71bfd5864455024dcce4644d37d68fc2f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=141127256&response-content-disposition=attachment%3B%20filename%3Djawiki.all_vectors.100d.txt.bz2&response-content-type=application%2Foctet-stream [following]\n",
      "--2021-04-10 08:03:16--  https://github-releases.githubusercontent.com/141127256/0f0df900-8e07-11e9-8309-25da3d3b21b1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210410%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210410T080316Z&X-Amz-Expires=300&X-Amz-Signature=ba411812fd883ad11b4b903933ddc0e71bfd5864455024dcce4644d37d68fc2f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=141127256&response-content-disposition=attachment%3B%20filename%3Djawiki.all_vectors.100d.txt.bz2&response-content-type=application%2Foctet-stream\n",
      "Resolving github-releases.githubusercontent.com (github-releases.githubusercontent.com)... 185.199.109.154, 185.199.110.154, 185.199.111.154, ...\n",
      "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 627240510 (598M) [application/octet-stream]\n",
      "Saving to: ‘jawiki-model.txt.bz2’\n",
      "\n",
      "jawiki-model.txt.bz   3%[                    ]  19.10M  6.84MB/s    in 2.8s    \n",
      "\n",
      "2021-04-10 08:03:19 (6.84 MB/s) - Read error at byte 20025964/627240510 (Connection reset by peer). Retrying.\n",
      "\n",
      "--2021-04-10 08:03:20--  (try: 2)  https://github-releases.githubusercontent.com/141127256/0f0df900-8e07-11e9-8309-25da3d3b21b1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210410%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210410T080316Z&X-Amz-Expires=300&X-Amz-Signature=ba411812fd883ad11b4b903933ddc0e71bfd5864455024dcce4644d37d68fc2f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=141127256&response-content-disposition=attachment%3B%20filename%3Djawiki.all_vectors.100d.txt.bz2&response-content-type=application%2Foctet-stream\n",
      "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 627240510 (598M), 607214546 (579M) remaining [application/octet-stream]\n",
      "Saving to: ‘jawiki-model.txt.bz2’\n",
      "\n",
      "jawiki-model.txt.bz  18%[==>                 ] 109.75M  7.12MB/s    in 13s     \n",
      "\n",
      "2021-04-10 08:03:33 (7.13 MB/s) - Read error at byte 115077232/627240510 (Connection reset by peer). Retrying.\n",
      "\n",
      "--2021-04-10 08:03:35--  (try: 3)  https://github-releases.githubusercontent.com/141127256/0f0df900-8e07-11e9-8309-25da3d3b21b1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210410%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210410T080316Z&X-Amz-Expires=300&X-Amz-Signature=ba411812fd883ad11b4b903933ddc0e71bfd5864455024dcce4644d37d68fc2f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=141127256&response-content-disposition=attachment%3B%20filename%3Djawiki.all_vectors.100d.txt.bz2&response-content-type=application%2Foctet-stream\n",
      "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 627240510 (598M), 512163278 (488M) remaining [application/octet-stream]\n",
      "Saving to: ‘jawiki-model.txt.bz2’\n",
      "\n",
      "jawiki-model.txt.bz  38%[+++===>             ] 232.28M  7.13MB/s    in 18s     \n",
      "\n",
      "2021-04-10 08:03:53 (7.00 MB/s) - Read error at byte 243560560/627240510 (Connection reset by peer). Retrying.\n",
      "\n",
      "--2021-04-10 08:03:56--  (try: 4)  https://github-releases.githubusercontent.com/141127256/0f0df900-8e07-11e9-8309-25da3d3b21b1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20210410%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20210410T080316Z&X-Amz-Expires=300&X-Amz-Signature=ba411812fd883ad11b4b903933ddc0e71bfd5864455024dcce4644d37d68fc2f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=141127256&response-content-disposition=attachment%3B%20filename%3Djawiki.all_vectors.100d.txt.bz2&response-content-type=application%2Foctet-stream\n",
      "Connecting to github-releases.githubusercontent.com (github-releases.githubusercontent.com)|185.199.109.154|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 627240510 (598M), 383679950 (366M) remaining [application/octet-stream]\n",
      "Saving to: ‘jawiki-model.txt.bz2’\n",
      "\n",
      "jawiki-model.txt.bz 100%[+++++++============>] 598.18M  7.15MB/s    in 51s     \n",
      "\n",
      "2021-04-10 08:04:47 (7.13 MB/s) - ‘jawiki-model.txt.bz2’ saved [627240510/627240510]\n",
      "\n",
      "answer\t\t\t   preprocess_knock_Python.ipynb\n",
      "data\t\t\t   preprocess_knock_R.ipynb\n",
      "Entity_Relationship.ipynb  preprocess_knock_SQL.ipynb\n",
      "genbapro_chapter3.ipynb    wikipedia.json\n",
      "jawiki-model.txt\n"
     ]
    }
   ],
   "source": [
    "# Wikipediaの方法\n",
    "!ls\n",
    "!wget https://github.com/singletongue/WikiEntVec/releases/download/20190520/jawiki.all_vectors.100d.txt.bz2 -O jawiki-model.txt.bz2\n",
    "!bzip2 -d jawiki-model.txt.bz2\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "configured-shark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 1748273390 Jun 13  2019 jawiki-model.txt\n",
      "Sat 10 Apr 2021 08:10:28 AM UTC\n"
     ]
    }
   ],
   "source": [
    "!ls -l jawiki-model.txt\n",
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "juvenile-february",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('jawiki-model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "tropical-windsor",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-b0ba5306d873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"増幅\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{word}:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"neighbour of {word}:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"増幅\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "word = \"増幅\"\n",
    "print(f\"{word}:\", model.wv[word][:5])\n",
    "print(f\"neighbour of {word}:\", model.similar_by_vector(\"増幅\", topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定されたドキュメントからランダムにインデックスを取得\n",
    "def random_index(doc):\n",
    "    for _ in doc:\n",
    "        idx = numpy.random.randint(0, len(doc))\n",
    "        word = doc[idx]\n",
    "        if word in model.wv:\n",
    "            return idx\n",
    "    return -1   # means Not Found\n",
    "\n",
    "# Wikipedia を使ったオーグメンテーション\n",
    "def do_augmentation_by_wikipedia(docs, replace_rate=0.2):\n",
    "    # replace_rate = 0.2   # 元文書からの変換割合\n",
    "\n",
    "    # 文書毎に処理\n",
    "    augmented = []\n",
    "    replaced = []\n",
    "    for doc in docs:\n",
    "        # 類義語に変換する回数\n",
    "        max_replaces = int(len(doc) * replace_rate)\n",
    "\n",
    "        # 変換数分ループ処理\n",
    "        doc_new = doc.copy()\n",
    "        replaced_pairs = []\n",
    "        for _ in range(max_replaces):\n",
    "            # 変換する場所・インデックスをランダムに取得\n",
    "            idx = random_index(doc)\n",
    "            if idx < 0:     # Not Found\n",
    "                break\n",
    "            word = doc_new[idx]\n",
    "            # 類義語(一番近い単語)を取得\n",
    "            word_similar = model.similar_by_word(word, topn=1)[0]\n",
    "            # 取得した類義語で更新\n",
    "            doc_new[idx] = word_similar[0]\n",
    "            # 変換前と後の単語を記録用に保持\n",
    "            replaced_pairs.append((idx, word, word_similar[0]))\n",
    "            # print(f\"{_}:\", word, \"->\", word_similar[0], \"at\", idx)   # デバッグ用\n",
    "        augmented.append(doc_new)\n",
    "        replaced.append(replaced_pairs)\n",
    "    \n",
    "    return augmented, replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-academy",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# around 5 min\n",
    "augmented, replaced = do_augmentation_by_wikipedia(parsed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-collector",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = numpy.random.randint(0, len(parsed_docs))\n",
    "print(f\"org[{doc_idx}]:\", \"\".join(parsed_docs[doc_idx]))\n",
    "print(f\"aug[{doc_idx}]:\", \"\".join(augmented[doc_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(replaced[doc_idx])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# around 5 min\n",
    "augmented2, replaced2 = do_augmentation_by_wikipedia(augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-giant",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idx = numpy.random.randint(0, len(parsed_docs))\n",
    "print(f\"org[{doc_idx}]:\", \"\".join(parsed_docs[doc_idx]))\n",
    "print(f\"aug[{doc_idx}]:\", \"\".join(augmented2[doc_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-eating",
   "metadata": {},
   "source": [
    "# 3-6-3 ベクトル化後のオーグメンテーション"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-claim",
   "metadata": {},
   "source": [
    "ベクトル化を行った後に実行可能なオーグメンテーション。  \n",
    "One-Hotベクトルに対するノイズ付与と、Word2vecで埋め込んだベクトルに対するノイズ付与があります。  \n",
    "※Doc2vecに対してもWord2vecと同じ方法で、正規乱数を付与してのオーグメンテーションができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-vampire",
   "metadata": {},
   "source": [
    "## 3-6-3-1 One-Hot ベクトルのオーグメンテーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder に入力するために変換\n",
    "words = [[word] for doc in parsed_docs for word in doc]\n",
    "X = words\n",
    "\n",
    "# One-Hot ベクトルに変換\n",
    "onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "vectors_onehot = onehot.fit_transform(X).toarray().astype(numpy.int)\n",
    "\n",
    "# One-Hot であることを確認\n",
    "# # 各ベクトル（各行）の要素の最小値が0であること\n",
    "min_equals_0 = all(vectors_onehot.min(axis=1) == 0)\n",
    "\n",
    "# # 各ベクトル（各行）の要素の最大値が1であること\n",
    "max_equals_1 = all(vectors_onehot.max(axis=1) == 1)\n",
    "\n",
    "# # 各ベクトル（各行）の要素の合計値が1であること\n",
    "sum_equals_1 = all(vectors_onehot.sum(axis=1) == 1)\n",
    "\n",
    "is_one_hot = all([min_equals_0, max_equals_1, sum_equals_1])\n",
    "is_one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 桁数を使って、ベルヌーイ分布の確率を設定\n",
    "## ノイズフラグ=1になる個数を5個以下(ノイズとみなしても良さそうな個数)に抑えるための工夫\n",
    "pw = numpy.ceil(numpy.log10(vectors_onehot.shape[1]))\n",
    "noise_prob = 0.5 * 10**(-pw)\n",
    "\n",
    "# ベルヌーイ分布から乱数ベクトル(ノイズベクトル)を取得\n",
    "noise_vectors = numpy.ranacdom.binomial(1, noise_prob, vectors_onehot.shape)\n",
    "set(noise_vectors.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本統計量を表示\n",
    "nvsum = pandas.Series(noise_vectors.sum(axis=1))\n",
    "nvsum.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "apt-get install -y fonts-ipafont-gothic > /dev/null\n",
    "cachedir=$(python -c 'import matplotlib as m; print(m.get_cachedir())')\n",
    "rm -f $cachedir/fontlist-v300.json\n",
    "pip install japanize-matplotlib > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-cooling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from matplotlib import pyplot\n",
    "import japanize_matplotlib\n",
    "\n",
    "matplotlib.rc('font', family=\"IPAexGothic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ヒストグラムを表示\n",
    "bins = nvsum.max()\n",
    "pyplot.clf()\n",
    "pyplot.xlabel(\"ノイズベクトルのノイズ次元の個数\")\n",
    "pyplot.ylabel(\"個数に対する頻度\")\n",
    "nvsum.hist(bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (参考) 各値の個数を表示\n",
    "for n in range(nvsum.max() + 1):\n",
    "    print(f\"{n}:\", (nvsum == n).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_rate = 0.2\n",
    "vectors_augmented = vectors_onehot + noise_rate * noise_vectors\n",
    "vaug = pandas.Series(vectors_augmented.sum(axis=1))\n",
    "pyplot.clf()\n",
    "pyplot.xlabel(\"ベクトル毎の全次元の合計値\")\n",
    "pyplot.ylabel(\"合計値に対する頻度\")\n",
    "vaug.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# コピーを生成\n",
    "v = vectors_augmented.copy()\n",
    "\n",
    "# 配列の型(shape)を確認\n",
    "print(\"v.shape:\", v.shape)\n",
    "print(\"v.sum(axis=1).shape:\", v.sum(axis=1).shape)\n",
    "\n",
    "# 各要素(次元)の値が1になるように正規化\n",
    "v = v / v.sum(axis=1).reshape(-1, 1)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agreed-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.allclose(v.sum(axis=1), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-arlington",
   "metadata": {},
   "source": [
    "## 3-6-3-2 Word2vec ベクトルのオーグメンテーション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-optics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "vector_size = 128\n",
    "corpus = parsed_docs\n",
    "model = Word2Vec(corpus, size=vector_size, min_count=1, window=5)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "word = random.sample(model.wv.vocab.keys(), 1)\n",
    "print(\"word:\", word)\n",
    "\n",
    "v = model.wv[word]\n",
    "sv = pandas.Series(v.ravel())\n",
    "sv.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_rate = 0.1\n",
    "noise_vector = numpy.random.normal(0.0, scale= scale_rate * sv.std(), size=v.shape)\n",
    "snv = pandas.Series(noise_vector.ravel())\n",
    "snv.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.clf()\n",
    "pyplot.xlabel(\"ノイズベクトルの各次元の値\")\n",
    "pyplot.ylabel(\"各次元の値に対する頻度\")\n",
    "snv.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_by_normal_noise(word, scale_rate=0.1):\n",
    "    v = model.wv[word]\n",
    "    noise_vector = numpy.random.normal(0.0, scale= scale_rate* v.std(), size=v.shape)\n",
    "    return v + noise_vector\n",
    "\n",
    "word = random.sample(model.wv.vocab.keys(), 1)\n",
    "print(\"word:\", word)\n",
    "aug_vector1 = augment_by_normal_noise(word)\n",
    "aug_vector2 = augment_by_normal_noise(word)\n",
    "print(\"aug_vector1:\", aug_vector1.ravel()[:5])\n",
    "print(\"aug_vector2:\", aug_vector2.ravel()[:5])\n",
    "(aug_vector1 == aug_vector2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_word1 = model.similar_by_vector(aug_vector1.ravel(), topn=1)\n",
    "restored_word2 = model.similar_by_vector(aug_vector2.ravel(), topn=1)\n",
    "print(\"original word:\", word)\n",
    "print(\"restored_word1:\", restored_word1)\n",
    "print(\"restored_word2:\", restored_word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "worse-understanding",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'augment_by_normal_noise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-ad843bd5b588>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maug_vector1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_by_normal_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maug_vector2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_by_normal_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrestored_word1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_vector1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrestored_word2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilar_by_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_vector2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'augment_by_normal_noise' is not defined"
     ]
    }
   ],
   "source": [
    "aug_vector1 = augment_by_normal_noise(word, scale_rate=7.0)\n",
    "aug_vector2 = augment_by_normal_noise(word, scale_rate=7.0)\n",
    "\n",
    "restored_word1 = model.similar_by_vector(aug_vector1.ravel(), topn=1)\n",
    "restored_word2 = model.similar_by_vector(aug_vector2.ravel(), topn=1)\n",
    "\n",
    "print(\"original word:\", word)\n",
    "print(\"restored_word1:\", restored_word1)\n",
    "print(\"restored_word2:\", restored_word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "emerging-berkeley",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat 10 Apr 2021 08:10:42 AM UTC\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-rwanda",
   "metadata": {},
   "source": [
    "# その他の方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-excerpt",
   "metadata": {},
   "source": [
    "テキスト生成モデル(GAN)を使ってのオーグメンテーションすることもできます。  \n",
    "現在は学習データの分布から生成するため、学習データの分布と異なる実データでの精度には限界があると言われていますが、  \n",
    "画像データでは学習なしの画像生成も実現しているので、今後に注目したい技術です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-footage",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
