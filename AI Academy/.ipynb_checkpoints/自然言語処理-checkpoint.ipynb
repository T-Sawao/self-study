{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"自然言語処理","private_outputs":true,"provenance":[{"file_id":"1ADrrCWWUCEQcD87BVsHzwCFb3V76hzFn","timestamp":1609913709110}],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1gj3BfLop2-HzSacj34UzaiUldBnXRPO2","authorship_tag":"ABX9TyPAXeD/TsSGdQktLY02u7O6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"53DTi1_oAZ0q"},"source":["# 自然言語処理の前処理における一連の流れ\n","\n","①クリーニング処理\n","\n","②形態素解析\n","\n","③正規化\n","\n","④ストップワード除去\n","\n","⑤単語のベクトル表現\n"]},{"cell_type":"markdown","metadata":{"id":"JIkaofxuAvpu"},"source":["## ①クリーニング処理\n","\n","<font color=\"Crimson\">クリーニング処理ではノイズとなるものを除去します。</font>  \n","ノイズとは、例えばインターネット上から拾ってくる文書を前処理する場合、htmlタグなどが含まれていますが、このようなタスクの結果に悪影響を及ぼすようなものをノイズといいます。"]},{"cell_type":"code","metadata":{"id":"jVvJ-Kr7AUJK"},"source":["import urllib.request\n","from bs4 import BeautifulSoup\n","\n","url = \"https://news.livedoor.com/topics/category/main/\"\n","html = urllib.request.urlopen(url)\n","\n","soup = BeautifulSoup(html, \"lxml\")\n","\n","titles = soup.find_all(\"h3\", class_=\"articleListTtl\")\n","print(titles)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YysV-OlEBOu_"},"source":["# stringで文字列のみ抜き出す。\n","for title in titles:\n","    print(title.string)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4d98RjDaCEd0"},"source":["## ②形態素解析\n","<font color=\"Crimson\">形態素解析とは文章を形態素に分けることです。  \n","形態素とは意味を持つ言葉の最小単位を指します。</font>"]},{"cell_type":"code","metadata":{"id":"2hCqjDz-CVzz"},"source":["!pip install mecab-python3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYasYPFPIEoH"},"source":["!pip install unidic-lite"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tUYcH9eAIJVW"},"source":["!pip install --no-binary :all: mecab-python3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6hF56WSCAqL"},"source":["import MeCab\n","m = MeCab.Tagger (\" \")\n","# 形態素解析した解析結果をparse()によって取得\n","sentence = '鬼滅の刃の映画を見に行った。'\n","\n","text = m.parse(sentence)\n","print(text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mI-5dfGuLDdV"},"source":["他にも形態素解析ツールには、janomeやjuman++,cabocha, nagisaなどがあります。  \n","MeCabは形態素解析ツールの中でも知名度が高く、実行速度が早いため、よく利用されます。"]},{"cell_type":"markdown","metadata":{"id":"0EvSxInTLd5r"},"source":["## ③正規化\n","形態素解析した文字には同じ意味でも異なる表記をしているものが存在します。  \n","例えば、全角の「クルマ」と半角の「ｸﾙﾏ」は別の単語として分類されてしまいます。  \n","上記のように、<font color=\"Crimson\">これらを同じ単語として処理させるために正規化を行います。</font>"]},{"cell_type":"code","metadata":{"id":"GKgI-6PuL74M"},"source":["!pip install neologdn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0nuzmmZCQos"},"source":["import neologdn\n","\n","# 半角を全角に統一\n","full_letter = neologdn.normalize(\"ｸﾙﾏ\")\n","print(full_letter) # クルマ\n","\n","# 不要なスペース削除\n","cut_space = neologdn.normalize(\"こ　ん　に　ち　は　\")\n","print(cut_space) # こんにちは\n","\n","# 似た文字の統一\n","unification = neologdn.normalize(\"-⁃֊⁻₋−‑˗‒–\")\n","print(unification) # -\n","\n","# 伸ばし棒を一つにする\n","shorter = neologdn.normalize(\"とてもきれいだーーーーーー\")\n","print(shorter) # とてもきれいだー\n","\n","# 繰り返しの制限\n","cut_repeat = neologdn.normalize(\"あああいたたたいなぁ\", repeat=1)\n","print(cut_repeat) # あいたいなぁ"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3dglxAu4McA0"},"source":["他には数字の置換えがあります。  \n","<font color=\"Crimson\">数字の置換えは、日時や値段に適用させます。基本的にこれらは特徴量として重要ではないので同じ文字に変換します。</font>  \n","数字そのものをヒットさせるためには正規表現を利用します。数字(正確には整数)は正規表現を用いると[0-9]または\\dと表現できます。  \n","正規表現を使って置き換えるにはre.subメソッドを使います。  "]},{"cell_type":"code","metadata":{"id":"d2cILDU3L5nm"},"source":["import re\n","t = \"12月のクリスマスに100万円のダイヤモンドをプレゼントする\"\n","# re.sub(\"ヒットさせたいもの(正規表現)\",\"置換する文字列\",\"置換したい文章\" )\n","print(re.sub(\"[0-9]+\",\"0\", t)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iQIKPihEMhLC"},"source":["１つの文書中に1回または2回しかでてこない単語は、その文書の意味を捉える特徴量にはなりにくいです。  \n","そのため、このような単語はリスト化して取り除くか、まとめて1つの特徴量として扱うべきです。  \n","こうすることによって<font color=\"Crimson\">精度向上と、学習時間の削減を実現できます。</font>"]},{"cell_type":"markdown","metadata":{"id":"zWx65-zvN1Oe"},"source":["## ④ストップワード除去\n","<font color=\"Crimson\">ストップワードは自然言語処理をするにおいて、頻出過ぎて特徴量として成立しないような単語を指します。</font>  \n","例えば、助詞の「は」は、主語の次につくことが多いです。**そのため頻出過ぎて文章の特徴をつかむ単語にはなり得ないのです。**  \n","ストップワードの取得方法は<font color=\"Crimson\">大体3種類あり、①辞書、②出現頻度、③レアな単語</font>から取得できます。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mvDuJ1kQQ-_6"},"source":["### ①辞書からストップワードを取得する方法"]},{"cell_type":"code","metadata":{"id":"7aWR3G49M90-"},"source":["# 辞書を使ったストップワード\n","import os\n","def download(path):\n","    # SlothLibと呼ばれるストップワード辞書を読み込み\n","    url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n","    if os.path.exists(path):\n","        print('File already exists.')\n","    else:\n","        print('Downloading...')\n","        urllib.request.urlretrieve(url, path)\n","        print(\"Finish!\")\n","\n","download(\"stopwords\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2G6rS6tmOUex"},"source":["with open(\"./stopwords\", \"r\", encoding=\"utf-8\") as f:\n","    lines = f.readlines()\n","\n","stopword_list = [line.split(\"\\n\")[0] for line in lines if line.split(\"\\n\")[0]]\n","# 形態素解析した表層形のリスト\n","text = [\"あそこ\",\"に\",\"は\",\"、\", \"多く\", \"の\", \"りんご\", \"が\", \"あり\", \"ます\", \"。\"]\n","\n","result = [word for word in text if word not in stopword_list]\n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8lQ_H4HlQXhf"},"source":["**この方法は自分で独自の辞書を作成することができますが、時間的コストがかかることが欠点です。**"]},{"cell_type":"markdown","metadata":{"id":"ZOTA1ninQtWu"},"source":["英語のストップワードは、NLTKという自然言語処理のライブラリをインストールすることで取得することが可能です。"]},{"cell_type":"code","metadata":{"id":"17tsnQNQPWfr"},"source":["import nltk\n","nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words(\"english\")\n","print(stop_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ugTnEGOrRMcN"},"source":["### ②出現頻度からストップワードを取得する方法\n","\n","この方法は①の辞書と元となる考え方は同じで、高頻出な単語は、特徴量としての重要度が低いという考えです。　　\n","\n","また、単語の希少性も加味することが可能です。①とは違ってコーパスごとに特有のストップワードを定めることが出来ます。\n"]},{"cell_type":"code","metadata":{"id":"UyavDgKvRP5o"},"source":["# 吾輩は猫であるのデータセット  \n","!wget https://aiacademy.jp/dataset/neko.txt -O neko.txt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pi3WjwiiR6SP"},"source":["MeCabにより出現回数の多い単語を数え、その単語をストップワードにする方法を学びます。\n","出現回数を数えるにはCounterメソッドを使います。<font color=\"Crimson\">頻度の高いものはmost_common()を使うことで引数に指定した数だけ上位から取り出すことが可能です。</font>"]},{"cell_type":"code","metadata":{"id":"79UvBPAsRdvj"},"source":["import MeCab\n","from collections import Counter\n","import re\n","\n","# ファイルの読み込み\n","with open(\"neko.txt\", \"r\", encoding=\"utf-8\") as f:\n","    file = f.readlines()\n","\n","# 形態素解析した単語を辞書型に書き換える\n","r = []\n","for line in file:\n","    if len(line.split(\"\\t\")) == 6:\n","        if len(line.split(\"\\t\")[3].split(\"-\")) > 1:\n","            r.append({\"surface\":line.split(\"\\t\")[0],\n","                     \"base\":line.split(\"\\t\")[2],\n","                     \"pos\":line.split(\"\\t\")[3].split(\"-\")[0],\n","                     \"pos1\":line.split(\"\\t\")[3].split(\"-\")[1]})\n","        else:\n","            r.append({\"surface\":line.split(\"\\t\")[0],\n","            \"base\":line.split(\"\\t\")[2],\n","            \"pos\":line.split(\"\\t\")[3].split(\"-\")[0],\n","            \"pos1\":\"\"})\n","surface = []\n","for dc in r:\n","    surface.append(dc[\"surface\"])\n","\n","count = Counter()\n","for word in surface: #数え上げる\n","    count[word] += 1\n","#上位10だけ取り出す\n","stopwords = [word[0] for word in count.most_common(10)]\n","print(\"most_common(10):\", count.most_common(10))\n","print(stopwords)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lZjqgIXiYBix"},"source":["## ⑤単語のベクトル表現\n","<font color=\"Crimson\">単語をベクトル化することで単語の類似度を測ることが可能になるなど扱いやすくなります。</font>  \n","\n","### one-hot表現\n","one-hot表現はある要素だけが1で、他の要素を0で表現する方法です。この方法を行うことで、単語を固定長のベクトルに変換できます。例えば、この単語ベクトルを入力とすればニューラルネットワークを固定したニューロンの数で構築することが可能です。  \n","\n","しかしながら、この表現はベクトル間の演算では何も意味のない結果を得ることの方が多いことが欠点です。つまり無駄が多いのです。また、1つの単語につき1次元使うので、単語の数が多くなるにつれて高次元になってしまいます。\n","\n","### 単語の分散表現  \n","分散表現とは単語を高次元の実数ベクトルで表現することです。  \n","<font color=\"Crimson\">one-hot表現とは異なり、要素ごとでベクトル化するのではなく、単語ごとでベクトル化します。</font>  \n","この時、意味が近いほど距離が近くなるように実数ベクトルが与えられます。(つまり似た単語には似たようなベクトルが与えられるということになります。)  \n","例えば、「クロワッサン」と「ドーナツ」をハイブリットさせた「クロワッサンドーナツ」というものがあります。この「クロワッサンドーナツ」を表現したい場合は「クロワッサン」と「ドーナツ」の分散表現を組み合わせるだけで表現することが出来ます。\n","単語の分散表現を獲得する方法の１つに**word2vec**というものがあります。\n","word2vecを使えば有名な例ですが、\n","「King」-「Man」＋「Woman」＝「Queen」という出力をさせることが可能です。\n","word2vecのText8Corpusでコーパス化するファイルresult.txtを用意します。これは先ほど形態素解析したものを辞書型に変えた処理を用いることで簡単に行うことが可能です。"]},{"cell_type":"code","metadata":{"id":"H806EZYIRm-L"},"source":["r = []\n","for line in file:\n","    if len(line.split(\"\\t\")) == 6:\n","        if len(line.split(\"\\t\")[3].split(\"-\")) > 1:\n","            r.append({\"surface\":line.split(\"\\t\")[0],\n","                     \"base\":line.split(\"\\t\")[2],\n","                     \"pos\":line.split(\"\\t\")[3].split(\"-\")[0],\n","                     \"pos1\":line.split(\"\\t\")[3].split(\"-\")[1]})\n","        else:\n","            r.append({\"surface\":line.split(\"\\t\")[0],\n","            \"base\":line.split(\"\\t\")[2],\n","            \"pos\":line.split(\"\\t\")[3].split(\"-\")[0],\n","            \"pos1\":\"\"})\n","for dc in r:\n","    if dc[\"surface\"] not in stopwords:\n","        result.append(dc[\"surface\"])\n","\n","with open(\"result.txt\", \"w\", encoding=\"utf-8\") as f:\n","    for word in result:\n","        f.write(word + \"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1oIq0yUTuoT"},"source":["from gensim.models import word2vec\n","\n","data = word2vec.Text8Corpus('result.txt') # 形態素解析して表層形のみ書いてあるファイル\n","\n","# size:圧縮したい次元数\n","# min_count:最低出現数\n","# window:ある単語の前後でみる単語数\n","# iter:反復数\n","model = word2vec.Word2Vec(data, size=100, min_count=5, window=5, iter=100)\n","# word2vecで表現された猫の単語ベクトル\n","print(model[\"猫\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fy9e5CQT9sG4"},"source":["# 形態素解析とは\n","形態素解析（morphological analysis）とは、検索エンジンにも用いられている自然言語処理の手法の一つで、ある文章・フレーズを「意味を持つ最小限の単位（＝単語）」に分解して、文章やフレーズの内容を判断するために用いられます。\n","例えば、以下のような文章があった場合それを形態素解析してみます。"]},{"cell_type":"markdown","metadata":{"id":"Id0IWzaJ98f5"},"source":["##MeCabとJanome\n","Pythonで形態素解析を行うにはMeCabとJanomeというのがあります。\n","MeCabは形態素解析ツールの中でも知名度が高く、実行速度が早いという特徴があります。\n","しかし、インストールが環境によって少々時間がかかってしまいます。\n","Janomeはインストールが比較的簡単に出来ますが、MeCabに比べ速度が遅かったりします。\n","\n","### Janomeのインストール  \n","http://mocobeta.github.io/janome/"]},{"cell_type":"code","metadata":{"id":"RDl613lQ-IR8"},"source":["!pip install janome"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-pjSX9JJ98db"},"source":["##Janomeで形態素解析  \n","形態素解析をするには、tokenize()を利用します。"]},{"cell_type":"code","metadata":{"id":"25VPabyp-cmQ"},"source":["from janome.tokenizer import Tokenizer\n","t = Tokenizer()\n","for token in t.tokenize('すもももももももものうち'):\n","    print(token)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iBcAYxkb98bK"},"source":["### ユーザー定義辞書の作成方法\n","単語を独自に追加することも可能です。  \n","その場合、作成したユーザー定義辞書をToknizer()の引数に指定します。  \n","辞書ファイルはカンマ区切りで作成します。  \n","CSVデータのフォーマットですが、左から次のようになります。"]},{"cell_type":"code","metadata":{"id":"DZtp-mXO9wtd"},"source":["#表層形,左文脈ID,右文脈ID,コスト,品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWGJNF4W_MdU"},"source":["となります。\n","このとき、「ヘッダーなし」かつ「CSV形式」で作成する事に注意です。  \n","左文脈IDや右文脈IDなどの詳細はこちらを参考にしてみてください。  \n","http://taku910.github.io/mecab/dic.html  \n","\n","＊表層形とは、活用や表記揺れなどを考慮した、本文中において、文字列として実際に出現する形式のことです。  \n","※これ以降に作成する全てのcsvファイルの保存先は、これから作成及び実行するPythonプログラムと同じ場所に配置してください。  \n","【参考リンク】  \n","usedic.csv内に独自の単語を定義しています。  \n","https://mocobeta.github.io/janome/\n"]},{"cell_type":"code","metadata":{"id":"RxcoFNge_MMa"},"source":["cd /content/drive/MyDrive/Colab Notebooks/AI Academy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IddYc4zZM3BE"},"source":["#pandasでの辞書の作成\n","import pandas as pd\n","\n","note = [[\"東京スカイツリー\",1288,1288,4569,\"名詞\",\"固有名詞\",\"一般\",\"*\",\"*\",\"*\",\"東京スカイツリー\",\"トウキョウスカイツリー\",\"トウキョウスカイツリー\"],\n","        [\"東武スカイツリーライン\",1288,1288,4700,\"名詞\",\"固有名詞\",\"一般\",\"*\",\"*\",\"*\",\"東武スカイツリーライン\",\"トウブスカイツリーライン\",\"トウブスカイツリーライン\"],\n","        [\"とうきょうスカイツリー駅\",1288,1288,4143,\"名詞\",\"固有名詞\",\"一般\",\"*\",\"*\",\"*\",\"とうきょうスカイツリー駅\",\"トウキョウスカイツリーエキ\",\"トウキョウスカイツリーエキ\"]]\n","\n","df2 = pd.DataFrame(note)\n","df2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDlOERHA-sPv"},"source":["#csvファイルとして保存\n","df2.to_csv(\"sample.csv\", index=False, header=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jhk8_fjtF8QH"},"source":["#辞書を使った形態素解析\n","from janome.tokenizer import Tokenizer\n","t = Tokenizer(\"sample.csv\", udic_enc=\"utf8\")\n","for token in t.tokenize(u'東京スカイツリーへのお越しは、東武スカイツリーライン「とうきょうスカイツリー駅」が便利です。'):\n","    print(token)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKZdu3nHFzTp"},"source":["Tokenizerの第一引数に辞書を指定することで、  \n","形態素解析する際に、固有名詞が細かく分解（形態素）されるの防ぎます。"]},{"cell_type":"markdown","metadata":{"id":"EJAN6PXSQukt"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"WIV11Gt7Re2j"},"source":["### 情報を抽出する\n","ここでは、janomeを用いて、品詞や表層形、活用形等の情報をそれぞれ取り出してみます。  \n","表層形とは、活用や表記揺れなどを考慮した、本文中において、文字列として実際に出現する形式のことです。"]},{"cell_type":"code","metadata":{"id":"ZwFj8gf5E3wB"},"source":["from janome.tokenizer import Tokenizer\n","\n","t = Tokenizer()\n","tokens = t.tokenize('すもももももももものうち')\n","\n","for token in tokens:\n","     print(token)\n","\n","\"\"\"\n","すもも   名詞,一般,*,*,*,*,すもも,スモモ,スモモ\n","も 助詞,係助詞,*,*,*,*,も,モ,モ\n","もも  名詞,一般,*,*,*,*,もも,モモ,モモ\n","も 助詞,係助詞,*,*,*,*,も,モ,モ\n","もも  名詞,一般,*,*,*,*,もも,モモ,モモ\n","の 助詞,連体化,*,*,*,*,の,ノ,ノ\n","うち  名詞,非自立,副詞可能,*,*,*,うち,ウチ,ウチ\n","\"\"\"\n","\n","for token in tokens:\n","    print(token.surface, '\\t')                  # 表層形\n","    print(token.part_of_speech.split(',')[0] )  # 品詞\n","    print(token.part_of_speech.split(',')[1] )  # 品詞細分類1\n","    print(token.part_of_speech.split(',')[2] )  # 品詞細分類2\n","    print(token.part_of_speech.split(',')[3] )  # 品詞細分類3\n","    print(token.infl_type)                      # 活用型\n","    print(token.infl_form)                      # 活用形\n","    print(token.base_form)                      # 原形\n","    print(token.reading)                        # 読み\n","    print(token.phonetic)                       # 発音\n","    print(token.node_type)                      # node_type"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xrj_8bFYVdQF"},"source":["###品詞判定\n","次に品詞判定を行います。\n","品詞の抽出には、part_of_speech.split()を利用し、if文を用いて判定を行います。\n"]},{"cell_type":"code","metadata":{"id":"qBnFGCP2VeXN"},"source":["from janome.tokenizer import Tokenizer\n","\n","t = Tokenizer()\n","tokens = t.tokenize('すもももももももものうち')\n","for token in tokens:\n","    # 品詞を取り出す。\n","    part_of_speech = token.part_of_speech.split(',')[0]\n","\n","    if part_of_speech == '名詞':\n","        print(token.surface)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j1hRQ43GYfgo"},"source":["### 読み込んだデータをリスト化する"]},{"cell_type":"code","metadata":{"id":"lNZ4BHjiVsvx"},"source":["from janome.tokenizer import Tokenizer\n","# from gensim import corpora, matutils\n","# from gensim import corpora\n","import pandas as pd\n","\n","t = Tokenizer()\n","data = list(pd.read_csv('sample.csv')['東京スカイツリー'].values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UX75i480Yiub"},"source":["data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RxSXta3gYl4i"},"source":["### 名詞と動詞と助動詞のみを抽出する"]},{"cell_type":"code","metadata":{"id":"vGH__xm5Yp0Z"},"source":["all_dictionary_norm, all_dictionary_verb , all_dictionary_averb = [], [], []\n","\n","for i in range(len(data)):\n","    dictionary_norm, dictionary_verb, dictionary_averb= [], [], []\n","\n","    try:\n","        tokendata = t.tokenize(data[i])\n","    except:\n","        continue\n","\n","    for token in tokendata:\n","        base, part = token.base_form, token.part_of_speech\n","\n","        if '動詞' in part:\n","            dictionary_verb += [base]\n","        elif '名詞' in part:\n","            dictionary_norm += [base]\n","        elif '助動詞' in part:\n","            dictionary_norm += [base]\n","\n","    all_dictionary_norm += [dictionary_norm]\n","    all_dictionary_verb += [dictionary_verb]\n","    all_dictionary_averb+= [dictionary_averb]\n","\n","print(\"名詞: \", all_dictionary_norm )\n","print(\"動詞: \", all_dictionary_verb)\n","print(\"助動詞:\", all_dictionary_averb)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GPcwwT3wY-QO"},"source":["### csvを形態素解析"]},{"cell_type":"code","metadata":{"id":"LMQj_NVnYuse"},"source":["note1 =[[\"トマト 袋 スタンドポリ\",\"398円\",\"429円\"],\n","        [\"ミニトマト 1個\",\"158円\",\"170円\"],\n","        [\"トマト袋 無選別 500g\",\"398円\",\"429円\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LerTpViWZYzG"},"source":["note2 =[[\"トマト\",\"カスタム名詞\",\"トマト\"],\n","        [\"ルネッサンストマト\",\"カスタム名詞\",\"ルネッサンストマト\"],\n","        [\"ミニトマト\",\"カスタム名詞\",\"ミニトマト\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKoby6_NZsr1"},"source":["df3 = pd.DataFrame(note1)\n","display(df3)\n","df4 = pd.DataFrame(note2)\n","display(df4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNKksB2OZ4k8"},"source":["df3.to_csv(\"food.csv\", index=False, header=None)\n","df4.to_csv(\"userdic2.csv\", index=False, header=None)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBIu6BQwaTRH"},"source":["import csv\n","from janome.tokenizer import Tokenizer\n","t = Tokenizer(\"userdic2.csv\", udic_type=\"simpledic\", udic_enc=\"utf8\")\n","\n","with open('food.csv', encoding='utf-8') as f:\n","    reader = csv.reader(f)\n","    next(reader)\n","    for columns in reader:\n","        for i in t.tokenize(columns[0]):\n","            print(i)\n","        print()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pKb8Iyp3afdO"},"source":["### 文字をカウントする\n","今回は、著作権の切れた小説文のデータを青空文庫からデータを使って文字をカウントしていきます。  \n","http://www.aozora.gr.jp/index_pages/person_a.html"]},{"cell_type":"code","metadata":{"id":"Xwk6fVuNlyy9"},"source":["# zipファイルのnameを確認する\n","import zipfile\n","with zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/AI Academy/52409_ruby_51058.zip') as zf:\n","    for info in zf.infolist():\n","        print(info.filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dW1z5-KKahA1"},"source":["from janome.tokenizer import Tokenizer\n","import os.path\n","import urllib.request as request\n","\n","target_url = \"http://www.aozora.gr.jp/cards/000879/files/52409_ruby_51058.zip\"\n","\n","download_zip = \"52409_ruby_51058.zip\" # ダウンロードしたzip\n","\n","if not os.path.exists(download_zip):\n","    print(\"downloading...\")\n","    request.urlretrieve(target_url, download_zip)\n","    print(\"done!\")\n","\n","zip_file = zipfile.ZipFile(download_zip, \"r\")\n","f = zip_file.open(\"01jo.txt\", \"r\") # withを使ってopenでも良いです。\n","data = f.read()\n","f.close()\n","\n","text_data = data.decode(\"shift_jis\") # 文字化け対策\n","\n","# 形態素解析処理\n","t = Tokenizer()\n","\n","word_dic = {}\n","lines = text_data.split(\"\\r\\n\") # 改行コードの分割\n","\n","for line in lines:\n","    li = t.tokenize(line)\n","    for w in li:\n","        word = w.surface\n","        part = w.part_of_speech\n","\n","        if part.find(\"名詞\") < 0: # 漢字の変換ミスに注意\n","            continue\n","        if not word in word_dic:\n","            word_dic[word] = 0\n","        word_dic[word] += 1\n","\n","keys = sorted(word_dic.items(), key = lambda x:x[1], reverse=True)\n","for word, cnt in keys[:100]:\n","    print(word, cnt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R7x68WkMJq-0"},"source":["# 係り受け解析入門\n","\n","係り受け解析とは文節間の修飾する（係る）、修飾される（受ける）の関係を調べる事です。"]},{"cell_type":"markdown","metadata":{"id":"hDQxWGdzJ2ZO"},"source":[""]},{"cell_type":"code","metadata":{"id":"uE3FqySJfcLw"},"source":[""],"execution_count":null,"outputs":[]}]}