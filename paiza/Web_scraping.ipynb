{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jn7aMn8A-Spw"
   },
   "source": [
    "# Webスクレイピング  \n",
    "<font color=\"Crimson\">**Webスクレイピング(Scraping)とは、Webサイトから任意の情報を抽出、整形、解析する技術のことです。**</font>  \n",
    "Pythonにはスクレイピングをするためのライブラリがいくつかありますが、\n",
    "ここでは「BeautifulSoup」（ビューティフル・スープ）を使って、スクレイピングします。\n",
    "Beautiful Soupを使うことで、手軽にHTMLやXMLから情報の抽出が可能です。\n",
    "Beautiful SoupはHTMLなどの解析するためのライブラリですので、データのダウンロードを行う場合は、urllibもしくはrequestsなどを使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ny6NuGWE-SnV"
   },
   "source": [
    "## クローリング\n",
    "<font color=\"Crimson\">**クローリング(Crawling)とは、プログラムが「Webサイトを定期的に巡回し、情報を取得する技術です。**</font>  \n",
    "クローリングを行うプログラムをクローラー（crawler)やスパイダー(Spider)とも呼びます。\n",
    "集めたデータを機械学習で利用しやすいようにデータフォーマットとして、CSV形式で保存したりします。\n",
    "他にもPythonのライブラリなどを活用することで、JSON形式で保存したり、データベースなどに保存することも可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaS9ECyZ-Skv"
   },
   "source": [
    "## Webスクレイピングの活用事例\n",
    "Webスクレイピングの活用事例として大きく3つあげられます。\n",
    "\n",
    "1. マーケティング\n",
    "2. 業務効率化\n",
    "3. サービス開発\n",
    "\n",
    "まずマーケティングですが、Webスクレイピングを活用することで、毎日定期的に確認する株価の変動やニュースメディアからの最新記事などの情報を自動で収集することができます。\n",
    "次に、業務効率化を実現可能です。\n",
    "例えば、Twitterの特定の検索条件のツイートを数百件自動収集したりすることが出来ます。\n",
    "もちろん、手動でツイートを集めることも出来ますがかなりの労力がかかってしまいます。\n",
    "最後に、スクレイピングを利用したWebやアプリケーション開発が可能となります。\n",
    "Googleのような検索エンジンや定期的に収集したデータをサービス開発に役立てることも可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRDmmVOW-SiH"
   },
   "source": [
    "## Webスクレイピングを行うにあたっての注意事項\n",
    "スクレイピングを行う前に、確認するべき点や、作業中に気を付ける必要がある点がいくつかかありますので説明します。\n",
    " \n",
    "１）APIが存在するかどうか\n",
    " \n",
    "APIを提供しているサービスがあればそちらを使い、データを取得しましょう。  \n",
    "それでも、十分なデータが得られない等の問題があるのであればスクレイピングを検討します。   \n",
    "\n",
    "２）取得後のデータの用途に関して  \n",
    "\n",
    "取得後のデータを使う場合には、注意が必要です。  \n",
    "\n",
    "取得先のデータは自分以外の著作物にあたり、著作権法に抵触しないように考慮する必要があるためです。  \n",
    "\n",
    "- 私的利用のための複製(第30条)  \n",
    "\n",
    "- 情報解析のための複製等(第47条の7)  \n",
    "\n",
    "※２０１９年１月１日の改正著作権法により、この４７条の７が廃止され、新しい条文である新３０条の４や新４７条の５が発効します。詳しくは下記記事をご確認ください。  \n",
    "\n",
    "進化する機械学習パラダイス ～改正著作権法が日本のAI開発をさらに加速する～  \n",
    "https://storialaw.jp/blog/4936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg7da4kh-Sfe"
   },
   "source": [
    "### 複製権\n",
    "1) 複製権  \n",
    "複製権は、著作権に含まれる権利のひとつで、著作権法第21条で規定されています。  \n",
    "（第21条「著作者は、その著作物を複製する権利を専有する。」）  \n",
    "複製とは、作品を複写したり、録画・録音したり、印刷や写真にしたり、模写（書き写し）したりすること、そしてスキャナーなどにより電子的に読み取ること、また保管することなどを言います。  \n",
    "引用先: https://www.jrrc.or.jp/guide/outline.html  \n",
    "\n",
    "### 翻案権\n",
    "2) 翻案権  \n",
    "翻訳権・翻案権は、著作権法第二十七条に規定されている著作財産権です。  \n",
    "第二十七条では｢著作者は、その著作物を翻訳し、編曲し、若しくは変形し、又は脚色し、映画化し、その他翻案する権利を専有する｣（『社団法人著作権情報センター』より）と明記されています。  \n",
    "反対に見ると、これらを著作者の許諾なしに行うと、著作権の侵害になるということです。  \n",
    "引用元:　http://www.iprchitekizaisan.com/chosakuken/zaisan/honyaku_honan.html  \n",
    "\n",
    "### 公衆送信権\n",
    "3) 公衆送信権  \n",
    "公衆送信権は、著作権法第二十三条において規定される著作財産権です。  \n",
    "この第二十三条では「著作者は、その著作物について、公衆送信（自動公衆送信の場合にあっては、送信可能化を含む。）を行う権利を占有する。」  \n",
    "「著作者は、公衆送信されるその著作物を受信装置を用いて公に伝達する権利を占有する。」と明記されています。  \n",
    "引用元: http://www.iprchitekizaisan.com/chosakuken/zaisan/kousyusoushin.html  \n",
    " \n",
    "また上記を注意しながら、実際にスクレイピングを行う際に、書いたコードによってサーバに負荷がかからないようにしましょう。  \n",
    "過度なアクセスはサーバに負担をかけてしまい攻撃だとみなされてしまい、最悪の場合一定期間サービスを利用できなくなってしまう可能性があります。  \n",
    "さらには、システムにアクセス障害が発生し、利用者の一人が逮捕された事件もありますので、常識の範囲内での使用してください。  \n",
    "岡崎市立中央図書館事件 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ok9R0HWx-Sc7"
   },
   "source": [
    "## Webスクレイピングの手順と必要な知識・ライブラリ  \n",
    "\n",
    "### スクレイピングの基本的な手順\n",
    "大きく分けると3つ手順を行う必要があります。  \n",
    "\n",
    "1.  Webページを取得する\n",
    "2.  スクレイピングする\n",
    "3.  抽出したデータを保存する  \n",
    "\n",
    "この3つの手順を行うことで、データを収集することが可能になります。  \n",
    "\n",
    "### スクレイピングをする上で必要なライブラリ\n",
    "Pythonをベースに説明します。  \n",
    "- 「urllib」もしくは「Requests」 HTTPライブラリ\n",
    "- BeautifulSoup HTMLの構造を解析するライブラリ（Webスクレイピング用のライブラリ）\n",
    "- Scrapy Webスクレイピングフレームワーク  \n",
    "場合によっては、PhantomJSもしくはSeleniumを使う場合もあります。  \n",
    "\n",
    "\n",
    "PhantomJS  \n",
    "http://phantomjs.org/  \n",
    "Selenium  \n",
    "http://www.seleniumhq.org/  \n",
    "\n",
    "### HTMLとCSS\n",
    "スクレイピングをするにあたって、HTMLとCSSに関しての基本を知っておくと便利です。  \n",
    "HTMLとはHyperText Markup Languageの略で、Webページを作成するためのマーク アップ言語の1つです。  \n",
    "CSS(Cascading Style Sheets)とは、HTML文書の装飾やレイアウトをつくるための言語です。  \n",
    "Webページは「文章の構造を定義するHTML」と「デザインを指定するCSS」のセット」で作られています。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l24o7GTW-SYA"
   },
   "source": [
    "# Webサイトからデータを取得しよう\n",
    "まずは、スクレイピングの基本的な手順の1つ目にあたる、『1.Webページを取得する』をやっていきます。  \n",
    "Pythonを使いWebサイトからデータを取得する方法を紹介します。   \n",
    "## Requests\n",
    "Pythonには他にも先ほど紹介したHTTPライブラリのurllib以外にもRequestsというライブラリもあります。  \n",
    "公式ドキュメント  \n",
    "http://requests-docs-ja.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0W3aBrJDGdK"
   },
   "outputs": [],
   "source": [
    "# 指定したURLのコードを取得\n",
    "import requests\n",
    "\n",
    "res = requests.get(\"https://www.anicom-sompo.co.jp/nekonoshiori/3079.html\")\n",
    "# テキスト形式で出力\n",
    "print(res.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63qQPiU3-SVm"
   },
   "source": [
    "## BeautifulSoupでスクレイピングする\n",
    "Beautiful Soupはインストールは下記コマンドでインストール可能です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrRikylBFGn4"
   },
   "outputs": [],
   "source": [
    "pip install bs4 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxBxzU98-SS6"
   },
   "source": [
    "### find()メソッド\n",
    "find()メソッドを利用することで、任意のid属性を指定し要素を見つけることが出来ます。  \n",
    "Beautiful Soup日本語ドキュメント  \n",
    "http://kondou.com/BS4/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orvMPPfAFYho"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 指定したURLを指定したパーサーに変換\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# 解析したHTMLから任意の部分のみを抽出（ここではtitleとtd）\n",
    "title = soup.find(\"title\")\n",
    "td = soup.find(\"td\")\n",
    "\n",
    "print(\"title: \" + title.text)\n",
    "print(\"td: \" + td.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExzlJGgU-SQW"
   },
   "source": [
    "### find_all()メソッド\n",
    "find_all()を使うと、複数の要素（タグ）を一気に取得することが出来ます。  \n",
    "また、find_all()にはキーワード引数を指定することが可能です。  \n",
    "キーワード引数にidに値を渡すとタグの’id’属性に対して取得可能です。  \n",
    "soup.find_all(id='ID名')  \n",
    "soup.find_all('タグ', class_=\"クラス名\")でクラスの特定のタグに対して取得できます。  \n",
    "find_all()ドキュメント  \n",
    "http://kondou.com/BS4/#find-all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b4xCkoAF7kQ"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 指定したURLを指定したパーサーに変換\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# 解析したHTMLから任意の部分のみを抽出（ここではtitleとtd）\n",
    "tds = soup.find_all(\"td\")\n",
    "for td in tds:\n",
    "    print(\"td: \" + td.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nMrEzFx6-SN0"
   },
   "source": [
    "### CSSセレクタ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAf4npym-SLY"
   },
   "source": [
    "セレクタとは一言でいうと、スタイルを適用する対象のことです。  \n",
    "セレクタには、「HTMLの要素」、「class名（プログラミングのクラスではありません）」、「ID名」の3つがセレクタの基本セレクタです。  \n",
    "他にもセレクタの種類はいくつかあります。  \n",
    "CSSセレクタ  \n",
    "https://developer.mozilla.org/ja/docs/Learn/CSS/Introduction_to_CSS/Selectors\n",
    " \n",
    "CSSセレクタ  \n",
    "スクレイピングをする際に、CSSのセレクタを指定して、任意の要素を抽出することが出来ます。  \n",
    "\n",
    " \n",
    "メソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iS5nj0Q0kOxc"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 指定したURLを指定したパーサーに変換\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# 1つのセレクタを指定\n",
    "title = soup.select_one(\"div#entry-title-wrapper > h1\").string\n",
    "print(\"title: \",title)\n",
    "\n",
    "li_list = soup.select(\"td\")\n",
    "print(li_list)\n",
    "for li in li_list:\n",
    "    print(\"li: \", li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ON19EcpvGlSE"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen(\"https://aiacademy.jp/assets/scraping/sample3.html\")\n",
    "data = html.read()\n",
    "html = data.decode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "#main > h1\n",
    "h1 = soup.select_one(\"div#main > h1\").string\n",
    "print(\"h1: \",h1)\n",
    "\n",
    "li_list = soup.select(\"div#main > ul.items > li\")\n",
    "for li in li_list:\n",
    "    print(\"li: \", li.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hnqxg7A-SD1"
   },
   "source": [
    "次の6つのSTEPでCSSセレクタを取得可能です。\n",
    "1. STEP1 開発者ツールで検証を開く。\n",
    "2. STEP2 開発者ツール矢印ボタンを押して青色にする。\n",
    "3. STEP3 青色にしたら、取得したい部分を選択しクリックする。\n",
    "4. STEP4 青色ラインが引かれるので、左の白の・・・を選択する。\n",
    "5. STEP5 copy > Copy selectorを押すとクリップボードにコピーされる。\n",
    "6. STEP6 貼り付けする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FlBxrylLkVT"
   },
   "source": [
    "### CSSセレクタに関して補足\n",
    "cssセレクタをコピーした際に、nth-child(2)といったような書式が出てくる場合があります。  \n",
    "その場合、BeautifulSoupはnth-childの記述方法に対応していないため、nth-of-typeを使う必要があります。  \n",
    "nth-child(n)というのは、n番目の子となる要素を意味しております。  \n",
    "nth-of-type(n)はn番目のその種類の要素という意味でこちらを使えば取得可能です。  \n",
    " \n",
    "**セレクタをそのままとってもうまくとれない場合がありますが、セレクタを多少修正する必要があるとはいえ、目視でセレクタを取得するよりChromeの検証ツールを使うことで簡単に必要なセレクタを取得できます。**  \n",
    "（実際に検証ツールでそのまま取得したセレクタだとデータが取得できない場合はよくありますので、その都度取得したセレクタを調整したりします。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr2LhBZiMKtk"
   },
   "source": [
    "# Selenium入門"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp6W4Y38MKrX"
   },
   "source": [
    "## 実行環境に関して\n",
    "本テキストでは、Google Colabではなく、手元のPCのJupyter Notebook等で実行を進めます。\n",
    "ですので、Google Colabではなく、手元のPython実行環境で動作させてください。\n",
    "その際に、ブラウザはGoogle Chromeを前提に進めていきますので、まだインストールされていない方はあらかじめインストールしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4rKevkYMKob"
   },
   "source": [
    "## SeleniumとChromeDriver\n",
    "SeleniumはWebブラウザで行うクリック操作やキーボード入力などをプログラム上から自動で操作できるようにしたライブラリでChromeDriverはChromeブラウザをプログラムで動かす為のドライバーです。  \n",
    "この2つを使うことで、SeleniumでChromeブラウザを操作してログインすることが可能です。  \n",
    "またこの2つを組み合わせて使うことで、次のことが可能になります。\n",
    " \n",
    "- Webスクレイピング\n",
    "- ブラウザの自動操作(次へボタンや購入ボタンなどを自動で押すなど）\n",
    "- システムの自動テスト\n",
    "- 非同期サイトのスクレイピング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAzgraeyMKlq"
   },
   "source": [
    "### Seleinumを動作させるための手順\n",
    "Seleinumを動作させるためには、主に２つの手順を行います。  \n",
    "①  Seleniumのインストール  \n",
    "②  Chrome Driverのインストール  \n",
    "それでは順に確認していきましょう。  \n",
    "Seleniumのインストール  \n",
    "まずはSeleniumのインストールです。  \n",
    "Seleniumのインストールは下記のコマンドを、ターミナルもしくはコマンドプロントで打ち込み実行することで、インストール出来ます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stZMV0GMNgUk"
   },
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwBl2CQZMKjL"
   },
   "source": [
    "### Chrome Driverのインストール\n",
    "次にChrome Driverのインストールです。  \n",
    "Chrome Driverのインストールは下記リンクから飛べます。  \n",
    "Chrome Driver インストールリンク  \n",
    "http://chromedriver.chromium.org/downloads  \n",
    "\n",
    "ここで注意ですが、お使いのChromeのバージョンに適したdriverを入れる必要があります。  \n",
    "\n",
    "適切なChrome Driverをインストールするにあたり、Chromeブラウザのバージョンは下記の方法で確認出来ます。  \n",
    "メニューの『chrome』から『Google Chromeについて』で確認出来ます。  \n",
    "\n",
    "バージョンの書かれた部分のリンクをクリックすると以下のような画面に飛びますので、お使いのOSにあわせてZIPファイルを解凍し、chromediverを次のセクションにて任意の場所に配置していきます。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXt1T1DYMKga"
   },
   "source": [
    "### Seleniumの動作確認\n",
    "まず初めに、最新のGoogle Chromeと、最新のChrome Driverをインストールしていることを確認してください。  \n",
    "※片方が最新でも、Seleniumが動作しない場合があります。  \n",
    "その後、次のプログラムを任意のテキストエディタやJupyter Notebookに打ち込んで実行してください。  \n",
    "気をつける部分としては、インストールしたChrome Driverの配置場所です。  \n",
    "下記プログラムの例では、Windowsの場合、Chrome DriverをC直下に配置しています。  \n",
    "Macの場合は、下記プログラムが保存されている場所に置いてあることを想定しています。  \n",
    "executable_path=''の''内に任意のパスを指定できるので、Desktopなりご自由に指定していただいて構いません。  \n",
    "<font color=\"Crimson\">**また、よくあるエラーとして、chromedriverというファイル名を間違えて記述している場合もエラーになりますので、動作確認の場合は上記内容をよく確認してから実行してください。**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tD4WFFXeMKeX"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys as keys\n",
    "\n",
    "\"\"\"\n",
    "※注意※\n",
    "下記コードにて、MacとWindows両方のpathを設定しているので、\n",
    "お使いのPCに合わせて不要なコードをコメントしてください。(コメントアウト)\n",
    "\"\"\"\n",
    "\n",
    "# Macの場合 (Chromedriveがこのプログラムを実行している同じ場所にある前提)\n",
    "driver = webdriver.Chrome(executable_path=\"./chromedriver\") # Windowsの方はこの行をコメントアウト    \n",
    "\n",
    "\n",
    "# windowsの場合 (Cドライブ直下にchromedriver.exeがある前提)\n",
    "driver = webdriver.Chrome(executable_path=\"C:\\chromedriver.exe\") # Macの方はこの行をコメントアウト\n",
    "# PATHの指定の仕方として、windows環境にてエラーが出る場合、 \\\\２つ利用することで回避できます。\n",
    "# driver = webdriver.Chrome(executable_path=\"C:\\\\chromedriver.exe\")\n",
    "\n",
    "### 1.Webサイトにアクセスする\n",
    "driver.get(\"https://aiacademy.jp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrgQuJGKMKcY"
   },
   "source": [
    "上記のプログラムを動作させると、ブラウザが立ちあがり、AI Academyのサイトを自動で開いてくれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dV_4BfJ_MKaV"
   },
   "source": [
    "### 簡単にSeleniumを動作させる方法(Macの場合）\n",
    "Mac OSの場合、brewコマンドでchromedriverをインストールする事で、chromedriverをWebサイトからダウンロードしなくても利用可能になります。  \n",
    "Mac OSの方は、こちらの利用方法がオススメですので、是非brewコマンドでchromedriverをインストールすることをオススメ致します。  \n",
    "まずは、下記のbrewコマンドで、chromedriverをインストールします。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uz1504brSG72"
   },
   "outputs": [],
   "source": [
    "brew install chromedriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PECBmz6oMKYS"
   },
   "source": [
    "その後、Mac OSの方は下記の２行をJupyter Notebook等で記述し実行する事で、簡単にChromeブラウザをSeleniumにより自動で制御することができるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTNFB_tXLjsw"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kOdUWW3KGdS"
   },
   "source": [
    "### ヘッドレスモード\n",
    "ヘッドレスモードとは、Chromeブラウザの画面を介さずに、CUIのみで処理を完結させることのできるモードです。  \n",
    "画面を起動し処理させなくても良いケース場面ではよく利用します。  \n",
    "例えば、下記のプログラムではAI AcademyのWebサイトをChromeブラウザを立ち上げることなしに、Seleniumを起動し、ページのスースコードを取得するプログラムになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oC5cAp_hscZ_"
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "options = Options()\n",
    "options.add_argument('--headless') # ヘッドレスモード\n",
    "\n",
    "# ※『簡単にSeleniumを動作させる方法(Macの場合）』セクションのコードを前提に記載しています。\n",
    "# Windows OSの方は、Chrome()の引数に、chromdriver.exeのパスを指定してください。\n",
    "driver = webdriver.Chrome(options=options)\n",
    "url = 'https://aiacademy.jp/'\n",
    "driver.get(url)\n",
    "# ページのソースを取得する\n",
    "print(driver.page_source)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5THHxzCFSkpF"
   },
   "source": [
    "1行目のselenium.webdriver.chrome.optionsからOptionsを読み込ませます。\n",
    "次に、8行目のwebdriver.Chrome()の引数に、options=optionsと3,4行目で定義したヘッドレスモードのOptionsを渡すことで、ヘッドレスモードでSeleniumを起動できます"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8_gL5wmXc_5"
   },
   "source": [
    "## Seleniumよく使う操作メソッド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjqqePouSjOm"
   },
   "outputs": [],
   "source": [
    "# 全てのウィンドウを閉じます\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJwXa1kQXjw1"
   },
   "outputs": [],
   "source": [
    "# テキストを入力する\n",
    "driver.find_element_by_id(\"ID\").send_keys(\"strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2pO4r1LXjoN"
   },
   "outputs": [],
   "source": [
    "# テキストを取得する\n",
    "driver.find_element_by_id(\"ID\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYxBgi3vXjl3"
   },
   "outputs": [],
   "source": [
    "# 属性を取得する\n",
    "driver.find_element_by_id(\"ID\").get_attribute(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hnuaf-3iXji4"
   },
   "outputs": [],
   "source": [
    "# タイトルを取得する\n",
    "driver.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmcJ6QYEXjhn"
   },
   "outputs": [],
   "source": [
    "# ページのソースを取得する\n",
    "driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80xdrJFNXjgD"
   },
   "outputs": [],
   "source": [
    "# 待ち時間（秒）を設定することで、指定したドライバの要素が見つかるまで待機\n",
    "driver.implicitly_wait(待ち時間秒)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1zxqDfNY0Y2"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "find_element_by_xpath()を使用する際に、XPathの取得方法ですが、\n",
    "Chromeであれば、右クリックを押して\n",
    "検証 → Elementsの任意のhtmlタグを選択し、右クリックすると、\n",
    "Copyというのがあるので、Copy▶ Copy selectorや︎Copy XPathが出てきますので、\n",
    "Copy XPathをクリックすると、選択したタグのXPathの値が取得がコピーできます。\n",
    "\"\"\"\n",
    "\n",
    "# 要素を取得する\n",
    "driver.find_element_by_class_name(\"classname\") # classでの指定\n",
    "driver.find_element_by_id(\"id\") # idでの指定\n",
    "driver.find_element_by_xpath(\"xpath\") # xpathでの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQw0uhdpY0Wp"
   },
   "outputs": [],
   "source": [
    "# ある要素をクリックする\n",
    "driver.find_element_by_xpath(\"XPATH\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJEdQ3CxY0Uw"
   },
   "outputs": [],
   "source": [
    "# ある要素までスクロールする\n",
    "element = driver.find_element_by_id(\"ID\") # 適切なIDに変更してください。\n",
    "actions = new Actions(driver)\n",
    "actions.move_to_element(element)\n",
    "actions.perform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxPIyn2kY0Sq"
   },
   "outputs": [],
   "source": [
    "# ドロップダウンを選択したいとき\n",
    "element = driver.find_element_by_xpath(\"xpath\")\n",
    "Select(element).select_by_index(indexnum) # indexで選択\n",
    "Select(element).select_by_value(\"value\") # valueの値\n",
    "Select(element).select_by_text(\"text\") # 表示テキスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXt5MHIMZS5D"
   },
   "outputs": [],
   "source": [
    "# 特定のURLでブラウザを起動する\n",
    "driver.get(\"URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nS9aFMLPZS3K"
   },
   "outputs": [],
   "source": [
    "# ブラウザを更新する\n",
    "driver.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2yxnqwkagpi"
   },
   "source": [
    "# Twitterに自動でログインしてみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJpehaknagmQ"
   },
   "source": [
    "今回はtwitterを対象とします。  \n",
    "事前にアカウント登録が必要なので、まだ登録が終わっていない方は、登録お願い致します。  \n",
    "では早速、Twitterに自動でログインしてしてみましょう。  \n",
    "ログインする機能として、実際にプログラムを組む前に私たち人間はどのようにログインするか考えてみましょう。  \n",
    "\n",
    "1. Webサイトのログインページにアクセスする\n",
    "2. 入力フォームにIDとパスワードを入力する\n",
    "3. エンターを押して（ログインボタンを押して）ログインをする  \n",
    "\n",
    "この手順をseleniumにもわかるように書いていきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KelkoLzrbglk"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys as keys\n",
    "\n",
    "# Chromeを動かすドライバを読み込み\n",
    "# お使いの環境に合わせてパスを変更\n",
    "driver = webdriver.Chrome(\"./chromedriver\")\n",
    "\n",
    "# URLの指定\n",
    "url = \"https://twitter.com/login\"\n",
    "# TwitterのURLにアクセス\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(2) \n",
    "\n",
    "\n",
    "# 入力フォームにIDとパスワードを入力する\n",
    "\"\"\"\n",
    "毎回css_selectorが変わっている可能性があるため、\n",
    "Google Chromeの検証ツールの右クリックから、取得したいセレクターを指定し、\n",
    "Copy > Copy selctorをし貼り付けてください。\n",
    "\"\"\"\n",
    "# 例：#react-root > div > div > div.css-1dbjc4n.r-1pi2tsx.r-13qz1uu.r-417010 > main > div > div > form > div > div:nth-child(6) > label > div.css-1dbjc4n.r-18u37iz.r-16y2uox.r-1wbh5a2.r-1udh08x > div > input'\n",
    "username = driver.find_element_by_css_selector('ここに貼り付ける')\n",
    "\n",
    "# 例：#react-root > div > div > div.css-1dbjc4n.r-1pi2tsx.r-13qz1uu.r-417010 > main > div > div > form > div > div:nth-child(7) > label > div > div.css-1dbjc4n.r-18u37iz.r-16y2uox.r-1wbh5a2.r-1udh08x > div > input\n",
    "password = driver.find_element_by_css_selector('ここに貼り付ける')\n",
    "\n",
    "# send_keysで文字を入力\n",
    "username.send_keys(\"Twitter ID\")\n",
    "password.send_keys(\"パスワード\")\n",
    "\n",
    "# 暗黙的待機と言って、要素が出現するまで２秒待機するという設定\n",
    "# JavaScriptなどで書かれた最近のWebサイトの場合、上記の記述がないとスクレイピング出来ない事が多くなっています。\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "# ENTERキーを押す\n",
    "password.send_keys(keys.ENTER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe0Xs0Q9sy8j"
   },
   "source": [
    "# 食べログの操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAO91EgPtAGE"
   },
   "source": [
    "### 食べログ\bの任意のページを指定した形式で開く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQUgmokBtIvl"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys as keys\n",
    "\n",
    "# googleドライバを起動\n",
    "driver = webdriver.Chrome(\"./chromedriver\")\n",
    "url = \"https://tabelog.com/\"\n",
    "\n",
    "# 指定したURLに移動\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(2) \n",
    "\n",
    "# 指定した入力欄にキーワードを入力\n",
    "username = driver.find_element_by_css_selector('#sk')\n",
    "username.send_keys(\"らーめん\")\n",
    "\n",
    "# 検索ボタンに移動して、押す\n",
    "search_btn = driver.find_element_by_css_selector('#js-global-search-btn > i')\n",
    "search_btn.click()\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "# ランキングを押す\n",
    "ranking_btn = driver.find_element_by_css_selector('#rstlst-access > ul > li.navi-rstlst__tab.navi-rstlst__tab--rank > a > span')\n",
    "ranking_btn.click()\n",
    "driver.implicitly_wait(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8itVHh3twGN"
   },
   "source": [
    "### 食べログのランキングを取得し保存\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZcS5QwhDtvu_"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# 取得するデータのあるURLページを指定\n",
    "url = \"https://tabelog.com/rstLst/ramen/?SrtT=rt&sk=%E3%82%89%E3%83%BC%E3%82%81%E3%82%93&svd=20201227&svt=1900&svps=2&Srt=D&sort_mode=1\"\n",
    "res = requests.get(url)\n",
    "\n",
    "# 指定したパーサーでコードを取得\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "data = []\n",
    "# １つの店舗データの塊を取得\n",
    "spots = soup.find_all(\"div\", attrs={\"class\":\"list-rst__wrap js-open-new-window\"})\n",
    "for spot in spots:\n",
    "    # 1つの店舗データ内から取得するデータを抽出する\n",
    "    spot_name = spot.find_all(\"a\", attrs={\"class\":\"list-rst__rst-name-target cpy-rst-name js-ranking-num\"})[0]\n",
    "    name = spot_name.text\n",
    "    rst_rate= spot.find_all(\"span\", attrs={\"class\":\"c-rating__val c-rating__val--strong list-rst__rating-val\"})[0]\n",
    "    rate = float(rst_rate.text)\n",
    "    rvw_count= spot.find_all(\"em\", attrs={\"class\":\"list-rst__rvw-count-num cpy-review-count\"})[0]\n",
    "    count = int(rvw_count.text)\n",
    "    spot_url = spot.find(\"a\")\n",
    "\n",
    "    # 抽出したデータを辞書型で保存\n",
    "    shop = {\"店名\":name, \"星\":rate, \"レビュー数\":count, \"URL\":spot_url}\n",
    "    data.append(shop)\n",
    "\n",
    "# pandasのデータフレームに格納\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# CSVファイルに保存\n",
    "df.to_csv(\"ラーメンリスト.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HionAEDWwEgn"
   },
   "source": [
    "## 画像データの取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kg_GsMKvr08"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# 取得するデータのあるURLページを指定\n",
    "url = \"https://ailovei.com/?p=10515\"\n",
    "res = requests.get(url)\n",
    "\n",
    "# 指定したパーサーでコードを取得\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "# １つの画像データを取得\n",
    "cats = soup.find_all(\"img\")\n",
    "for i, cat in enumerate(cats):\n",
    "    # srcに不足しているURLをroot_urlに記載\n",
    "    root_url =  \"https://ailovei.com\"\n",
    "    img_url = root_url + cat[\"src\"]\n",
    "\n",
    "    # 習得したURLデータを画像ファイルとして読み込み\n",
    "    img = Image.open(io.BytesIO(requests.get(img_url).content))\n",
    "    # 保存する\n",
    "    img.save(f\"img/{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bs1zBLgtagi3"
   },
   "source": [
    "## Webスクレイピング応用(1)\n",
    "\n",
    "### ログ出力とログ管理\n",
    "実際にクローラーを運用する際に、クロールに想定以上の時間がかかってしまったり、  \n",
    "動作しているはずのクローラーにバグがあり、データ収集ができていなかったといったことが  \n",
    "開発している途中や実際に運用を始めてから気づくケースがあります。  \n",
    "基本的に何度も動作させていく過程で、バグの発見や想定していなかったことの発見が開発過程の中で出てきます。  \n",
    "その中で開発や運用フェーズにて役立つ方法をいくつか解説していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNHx8GvSjo9D"
   },
   "outputs": [],
   "source": [
    "# ループの処理時間を計測\n",
    "import time\n",
    "start = time.time()\n",
    "for i in range(500000):\n",
    "    print(\"iの数\", i)\n",
    "elapsed_time = time.time() - start\n",
    "print (\"ループにかかった時間 :{0}\".format(elapsed_time) + \"秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh_VKsPwagKn"
   },
   "source": [
    "上記プログラムですと、print(\"iの数\", i)や最後の行の、\n",
    "print (\"ループにかかった時間 :{0}\".format(elapsed_time) + \"秒\")がログにあたります。  \n",
    "実際にクローラーのクロールの状況でログを吐き出す場合には例えば、urllibやrequestsなどのHTTPライブラリを使ってクロールしたページのURLやリクエストにかかった時間や\n",
    "randomモジュールで何秒ランダムにsleepしたかなどを吐き出すことがあります。\n",
    "また先ほどのプログラムにopen()及びwith文を使ってファイルとしてログファイルにログを残したりすることも多くあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUNIu2qCkLPl"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for i in range(500000):\n",
    "    print(\"iの数\", i)\n",
    "elapsed_time = time.time() - start\n",
    "print (\"ループにかかった時間 :{0}\".format(elapsed_time) + \"秒\")\n",
    "\n",
    "with open(\"time.log\", \"a\") as log_file:\n",
    "    log_file.write(\"経過時間:\" + str(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpOSTIWnagHr"
   },
   "source": [
    "上記プログラムを実行すると、time.logファイルが保存され、次のような内容が保存されているかと思います\n",
    "経過時間:5.122201919555664\n",
    "with文を使っているのはclose()の記述忘れを防ぐためです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf-QrebhagEm"
   },
   "source": [
    "### ログ管理\n",
    "ここではloggingに関して説明します。  \n",
    "loggingはログを出力するためのライブラリで、print()を使うより柔軟にログ出力が可能になります。  \n",
    "具体的には、出力先をログの緊急度合いに応じて変えることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57IYx4I2kldv"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "# ログメッセージ(例えば、infoは画面表示用)\n",
    "logging.info(\"sample message write here!\")\n",
    "# ログのレベルは幾つかの緊急度合いに応じて出力方法を変更可能です\n",
    "# 例えば、errorはファイル(.log)などの保存するなど使い分けたりしてみてください。\n",
    "logging.error(\"Error message write here!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5xdJeK-agBx"
   },
   "source": [
    "他にもハンドラ(handler)というクラスもあり、ハンドラを使うことでで出力先を変更可能です。  \n",
    "StreamHandler  \n",
    "https://docs.python.jp/3/library/logging.handlers.html  \n",
    "\n",
    "公式loggingチュートリアル  \n",
    "https://docs.python.jp/3/howto/logging.html\n",
    "\n",
    "使い方のわかりやすい記事　　\n",
    "\n",
    "https://qiita.com/knknkn1162/items/87b1153c212b27bd52b4\n",
    "\n",
    "公式loggingクックブック　　　\n",
    "\n",
    "https://docs.python.jp/3/howto/logging-cookbook.html　　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJhS7EP6af_J"
   },
   "source": [
    "## Cronによるクローラーの定期実行\n",
    "ここではCronというコマンドに関して学びます。  \n",
    "Cronを使うことでクローラーなどを定期的に実行することが可能になります。  \n",
    "まずはCronの基本内容として設定方法を見ていきましょう。  \n",
    "Cronは日時・曜日などを指定することでプログラムを実行することが可能になります。  \n",
    "※ちなみに、CronはmacやLinuxのみで使え、かつPC本体が起動中の場合にのみ動作します。  \n",
    "定期実行をパソコンを閉じた状態でもさせたい場合は、別途サーバー（例：さくらVPSなど）を借りる必要があります。  \n",
    "また、Windowsの場合はタスクスケジューラーを使う必要がありますのでCronは利用できませんので注意です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDxNKIhFln3H"
   },
   "outputs": [],
   "source": [
    "分 時 日 月 曜日 ユーザー コマンドのパス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UsFNGjJ2af8d"
   },
   "source": [
    "下記コマンドでCronを編集可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyDJOaiPl20a"
   },
   "outputs": [],
   "source": [
    "crontab -e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yky_Vm4gaf5p"
   },
   "source": [
    "下記コマンドでCronの設定を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4sjChttl3cB"
   },
   "outputs": [],
   "source": [
    "crontab -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKqA2AowlvBD"
   },
   "source": [
    "各要素の設定可能数値ですが、分は0〜59まで、時間は0〜23時まで、\n",
    "日は1〜31、月は1〜12、また曜日は0〜7で指定可能で、7が日曜日になります。  \n",
    "Cron参考記事  \n",
    "https://webkaru.net/linux/crontab-command/  \n",
    "下記は、毎朝10時に実行するCronです。  \n",
    "crontab -e で編集してみましょう。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QKAIFMIl2Bm"
   },
   "outputs": [],
   "source": [
    "0 10 * * * python /Users/aiacademy/Desktop/Crowler/sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "714N1g0-mBu1"
   },
   "source": [
    "## スクレイピングと相性の良いライブラリ\n",
    "スクレイピングをする上で使いこなせると良いライブラリをいくつか紹介いたします。\n",
    "\n",
    "1.  正規表現  \n",
    "2.  requestsモジュール  \n",
    "3.  lxml  \n",
    "\n",
    "\n",
    "ここではlxmlの基本的な使い方を説明します。  \n",
    "lxmlを使うためには、C言語の拡張ライブラリである、libxml2とlibxsltが必要になります。それらをインストールします。  \n",
    "下記はMac OSの場合brewでインストール可能です。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fl3imqFRmTtK"
   },
   "outputs": [],
   "source": [
    "brew install libxml2 libxslt\n",
    "pip install lxml cssselect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFQCe0Z0mMwM"
   },
   "outputs": [],
   "source": [
    "import lxml.html\n",
    "target_url = \"https://aiacademy.jp\"\n",
    "lx = lxml.html.parse(target_url) # <lxml.etree._ElementTree object at 0x101420dc8>\n",
    "html = lx.getroot()\n",
    "for a in html.cssselect(\"a\"):\n",
    "    print(target_url + a.get(\"href\"))\n",
    "    with open(\"get_url.txt\", \"a\") as f:\n",
    "        f.write(target_url + a.get(\"href\"))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6hEFJHjRzWd"
   },
   "source": [
    "## Webスクレイピング応用(2)  \n",
    "\n",
    "### 例外処理\n",
    "try-exceptを用いて例外処理をスクレイピングするプログラムの中に記述する方法を学びます。  \n",
    "なぜスクレピングのプログラムに例外処理を入れるかですが、  \n",
    "例えば、例外ごとに別のログを出力したり、定期実行時に何らかの例外でデータ取得できていなかったというようなことを避けるためにも例外があったほうが便利です。  \n",
    "\n",
    "ここではrequestsモジュールを例に説明していきます。  \n",
    "\n",
    "例外処理を行う上で、下記内容を最低限考慮しておけば良いです。  \n",
    "\n",
    "- ConnectionError ネットワークの問題が起こった場合\n",
    "- TooManyRedirects リクエストが設定されたリダイレクトの最大数超えた場合\n",
    "- HTTPError 不正なHTTPレスポンスがあった場合\n",
    "- Timeout リクエストがタイムアウトした場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9TNRaYNSH2H"
   },
   "outputs": [],
   "source": [
    "try\n",
    "    # スクレイピング処理\n",
    "except ConnectionError:\n",
    "    print(\"ConnectionError\")\n",
    "    raise\n",
    "except TooManyRedirects:\n",
    "    print(\"TooManyRedirects\")\n",
    "    raise\n",
    "except HTTPError:\n",
    "    print(\"HTTPError\")\n",
    "    raise\n",
    "except:\n",
    "    print(\"Response not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZHsNFqYRzTy"
   },
   "source": [
    "### retry\n",
    "**retryを用いることで、関数内で例外が発生した時に関数を指定の回数再施行することができます。**  \n",
    "スクレイピング中、ネットワークの環境などにより、サーバへのリクエストが失敗する可能性がありますが、そのような時にretryデコレータが有効です。\n",
    "インストールは下記で可能です。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XkUQnEiTSbKY"
   },
   "outputs": [],
   "source": [
    "pip install retry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3cgw0ZWRzRL"
   },
   "source": [
    "例えばプログラムは次のようになります。\n",
    "失敗しそうな関数（処理）の上にretryデコレータを記述します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V75ikbWqSfgf"
   },
   "outputs": [],
   "source": [
    "from retry import retry\n",
    "\n",
    "# 施行回数　間隔(秒数) 間隔の指数の3つを指定します。\n",
    "@retry(tries=3, delay= 2, backoff=2)\n",
    "def get():\n",
    "    print(\"get\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vhf0H0k2RzOn"
   },
   "source": [
    "# Scrapy入門(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDxRHgN-RzLd"
   },
   "source": [
    "### Scrapyとは\n",
    "Scrapy（スクレピー、スクラパイ、スクレパイ）とはPython用スクレイピングフレームワークです。  \n",
    "Scrapyを使うことで、サイトのクロール及びスクレイピングする処理を簡潔に記述することが可能です。  \n",
    " \n",
    "scrapy公式  \n",
    "https://scrapy.org/  \n",
    "scrapy公式ドキュメンテーション  \n",
    "https://doc.scrapy.org/en/1.1/index.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3GcqxUZTUEG"
   },
   "source": [
    "インストール（Mac）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9id2wUeHTXdB"
   },
   "outputs": [],
   "source": [
    "pip install scrapy\n",
    "#又は\n",
    "conda install scrapy\n",
    "# バージョン確認\n",
    "scrapy version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUbUcznGTUBk"
   },
   "source": [
    "### サンプルSpiderを実行する\n",
    "さてまずは、Scrapinghub社のブログをクロールするプログラムを実行してみましょう。  \n",
    "scrapy.orgのプログラムと全く同じものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eMWPSTPTTr2Y"
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name = 'blogspider'\n",
    "    start_urls = ['https://blog.scrapinghub.com']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for title in response.css('h2.entry-title'):\n",
    "            yield {'title': title.css('a ::text').extract_first()}\n",
    "\n",
    "        for next_page in response.css('div.prev-post > a'):\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D10G6e5MUCNR"
   },
   "outputs": [],
   "source": [
    "!scrapy runspider sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b852uKnfTT_J"
   },
   "source": [
    "# icrawler入門\n",
    "\n",
    "## icrawlerとは\n",
    "icrawlerとはウェブクローラのミニフレームワークです。\n",
    "\n",
    "It supports media data like images and videos very well, and can also be applied to texts and other type of files.\n",
    "\n",
    "上記記述が公式サイトに記載があるように、画像や動画などのメディアデータをサポートしており、テキストやその他の種類のファイルにも適用可能です。\n",
    "\n",
    "公式マニュアル  \n",
    "https://icrawler.readthedocs.io/en/latest/\n",
    "\n",
    "インストール  \n",
    "Macならターミナル、Windowsならコマンドプロンプトを開き、次のコマンドを実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TNcWliECUoOy"
   },
   "outputs": [],
   "source": [
    "!pip install icrawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VX3qSl3GTTs5"
   },
   "source": [
    "## 犬と猫の画像をダウンロードする\n",
    "今回はBing検索エンジンから、犬と猫の画像をそれぞれ10枚ずつダウンロードしてきましょう。  \n",
    "まずは次のプログラムを実行してみます。  \n",
    "実行した場所に、dogsというフォルダが作成され、犬の画像が10枚ダウンロードされます。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3mNL4THUy3u"
   },
   "outputs": [],
   "source": [
    "from icrawler.builtin import BingImageCrawler\n",
    "crawler = BingImageCrawler(storage={\"root_dir\": \"dogs\"})\n",
    "crawler.crawl(keyword=\"犬\", max_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-Pj4hyTU-4t"
   },
   "outputs": [],
   "source": [
    "from icrawler.builtin import BingImageCrawler\n",
    "\n",
    "crawler = BingImageCrawler(storage={\"root_dir\": \"cats\"})\n",
    "crawler.crawl(keyword=\"猫\", max_num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5Bxf1E5TTqF"
   },
   "source": [
    "keywordには取得したい画像の名前を渡します。  \n",
    "max_numにはダウンロードしたい画像の枚数を渡します。  \n",
    "（最大1000枚まで指定可能です。）  \n",
    "\n",
    "今回は、ダウンロード先のサイトをBingから取得しましたが、他にもBaidu、Googleからも取得できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBuKK8TqTTmT"
   },
   "source": [
    "# Webスクレイピングしたデータを保存する\n",
    "\n",
    "## データをスクレイピングしてテキストファイル保存する\n",
    "このテキストでは、スクレイピング（抽出）したデータをファイルなどに保存する方法を学んでいきます。  \n",
    "よく保存先に使われるフォーマットとしてCSV形式やJSON形式が多いため、ここではCSVに保存する方法を学びます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gC9qSRgDWpv4"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"https://aiacademy.jp/assets/scraping/sample4.html\")\n",
    "data = html.read()\n",
    "html = data.decode('utf-8')\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "for a in links:\n",
    "    href = a.attrs['href']\n",
    "    # text = a.string\n",
    "    f = open(\"sample.txt\",\"a\")\n",
    "    f.write(href)\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "    print(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQG9bJPzTTjZ"
   },
   "source": [
    "## データをスクレイピングしてデータベースに保存する\n",
    "先ほどは、テキストファイル(.txtや.csv)にスクレイピングしたデータを保存しました。  \n",
    "ちなみに、Pythonで扱えるデータベースはqlite3やPostgreSQLなどいくつかありますが、  \n",
    "ここではスクレイピングしたデータをデータベース(MySQL)に保存する方法を解説していきます。  \n",
    "さて、まずはテキストファイルよりデータベースに保存する方が良いということに関して説明します。  \n",
    "今後、スクレイピングしたデータは後から解析したり、またWebアプリケーションとして保存したデータを利用したりすることが実用上多くあります。  \n",
    "また、ファイルに保存している場合よりも、データベースに保存していることで処理速度の面やデータの扱いやすさ的な面からいくつかのメリットが多くあります。  \n",
    "デメリットとすれば、データベースを扱うための言語であるSQL(MySQLなど)の基本構文を扱える必要だったり、\n",
    "PythonからMysqlを連携するためのライブラリ(mysqlclient)などを学ぶための学習コストがかかったりすることがあります。  \n",
    "ですが、今後の運用などを行うというのを想定した場合、メリットの方が大きいですので、ここではPythonからMysqlに接続し、データを保存する方法を解説していきます。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQ1yMBh8TTgy"
   },
   "source": [
    "## MySQLの準備は別紙参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjHTbv1lhDk5"
   },
   "outputs": [],
   "source": [
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_QiWSuygObA"
   },
   "outputs": [],
   "source": [
    "import MySQLdb\n",
    "connect = MySQLdb.connect(\n",
    "    user=\"Host\",\n",
    "    passwd=\"sawao117\",\n",
    "    host=\"localhost\",\n",
    "    db=\"test_db1\",\n",
    "    charset=\"utf8\"\n",
    ")\n",
    "cursor = connect.cursor() # カーソルの取得\n",
    "\n",
    "# テーブルの作成\n",
    "\"\"\"\n",
    "Cursorクラスのexecute()によりSQLの実行が可能です。\n",
    "\"\"\"\n",
    "\n",
    "# webからuser_nameとemailの情報をスクレイピングしたとする\n",
    "# それらをテーブルに入れる。\n",
    "cursor.execute(\"create table users (user_id, user_name, email\")\n",
    "connect.commit()\n",
    "\n",
    "# 作成したテーブルにデータを保存する\n",
    "cursor.execute(\"insert into users values(%s, %s, %s)\", (1, \"ai_academy\", \"aiacademy@cyberbra.in\"))\n",
    "connect.commit()\n",
    "\n",
    "# 接続を閉じる\n",
    "connect.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjbQEtt6Xvpm"
   },
   "source": [
    "## 画像データをスクレイピングする\n",
    "次に、画像データを画像系サービスのAPIを使って収集して行きます。  \n",
    "画像の収集方法として、検索エンジンや、写真共有系サービスのFlickrやフォト蔵があります。  \n",
    "以下の写真共有サイトは写真検索のためのAPIが用意されてあります。  \n",
    "- Flickr  \n",
    "https://www.flickr.com/  \n",
    "search API 3600クエリー/時間  \n",
    "\n",
    "- フォト蔵  \n",
    "http://photozou.jp/basic/api_method_search_public\n",
    "search_public 「1リクエスト/秒を目安」  \n",
    "今回はフォト蔵のAPIを使い、画像を収集します。  \n",
    "URLにパラメータを指定することで、様々なキーワードで画像をダウンロードすることができます。  \n",
    "猫の画像を取得してみましょう。  \n",
    "以下のページにアクセスしてみてください。  \n",
    "https://api.photozou.jp/rest/search_public.json?keyword=猫  \n",
    "これはJSONという形式になっています。  \n",
    "また、original_image_urlというのが元の画像で、image_urlは、リサイズされてる画像です。  \n",
    "まずは、画像のURLを抽出してみましょう。  \n",
    "次のプログラムで、一覧を取得できます。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFGCSp2eltH7"
   },
   "outputs": [],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NaId_Y5llyiZ"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "url = \"https://api.photozou.jp/rest/search_public.xml?keyword=%E7%8C%AB\"\n",
    "response = urllib.request.urlopen(url)\n",
    "rss = response.read().decode(\"utf-8\")\n",
    "\n",
    "soup = BeautifulSoup(rss, \"xml\")\n",
    "\n",
    "for s in soup.find_all(\"photo\"):\n",
    "    print(s.find_all(\"image_url\")[0].string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lraugpcPmFo4"
   },
   "source": [
    "これは、100件の画像URLを取得しています。  \n",
    "次に、パラメータを加えて、1000件のURLを抽出してみましょう。  \n",
    "また、1秒ごとに、画像のURLを抽出してみます。  \n",
    "これを1000件分出力するプログラムです。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nllv3Afel0_t"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "url = \"https://api.photozou.jp/rest/search_public.xml?keyword=%E7%8C%AB&amp;limit=1000\"\n",
    "response = urllib.request.urlopen(url)\n",
    "rss = response.read().decode(\"utf-8\")\n",
    "\n",
    "soup = BeautifulSoup(rss, \"xml\")\n",
    "\n",
    "for s in soup.find_all(\"photo\"):\n",
    "    time.sleep(1) # 1秒スリープ\n",
    "    print(s.find_all(\"image_url\")[0].string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kydbb5wUmgRf"
   },
   "source": [
    "では、次に画像をダウンロードするdownload_image関数を定義します。  \n",
    "先ほどでURLを取り出せたので、そのURLの画像を保存する処理を書きます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSHSZdvkmQO8"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "\n",
    "def download_image(url,name):\n",
    "    path = \"./scrape_image/\"\n",
    "    imagename = str(name) + \".jpg\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    print(\"download...\", url)\n",
    "    print(path)\n",
    "    urllib.request.urlretrieve(url, path+imagename)\n",
    "    time.sleep(1) # 1秒スリープ\n",
    "\n",
    "\n",
    "url = \"https://api.photozou.jp/rest/search_public.xml?keyword=%E7%8C%AB&amp;limit=1000\"\n",
    "response = urllib.request.urlopen(url)\n",
    "rss = response.read().decode(\"utf-8\")\n",
    "\n",
    "soup = BeautifulSoup(rss, \"xml\")\n",
    "\n",
    "name = 0\n",
    "for s in soup.find_all(\"photo\"):\n",
    "    url = s.find_all(\"image_url\")[0].string\n",
    "    name+=1\n",
    "    download_image(url,name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3QAix0cmxVC"
   },
   "source": [
    "実行すると、特定の画像を1000枚取得できるプログラムが出来ました。  \n",
    "?keywordを自由な名前にすることで、好きな画像をダウンロードできます。  \n",
    "実行すると、1秒ごとに1枚の画像をダウンロードできます。  \n",
    "Ctrl + Cを押すことで、途中で止めることが出来ます。  \n",
    " \n",
    "また、APIを使わない方法として、icrawlerというのがあります。  \n",
    "こちらを使うことで、グーグルなどから画像を最大1000枚まで収集することができますので、合わせて読み進めてみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtnBQAUZmqhL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPSiNZJDVdEfv5xCXF7Z/cF",
   "collapsed_sections": [],
   "name": "Webスクレイピング.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
